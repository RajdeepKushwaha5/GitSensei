{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985bb98f",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "Import the necessary libraries: io, zipfile, requests, and frontmatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "601221d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c82fb",
   "metadata": {},
   "source": [
    "# Define the read_repo_data Function\n",
    "\n",
    "Define a function to download a GitHub repository as a zip file, extract markdown files, parse them with frontmatter, and return a list of dictionaries with content and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1077a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc5783",
   "metadata": {},
   "source": [
    "# Download and Process Repository Data\n",
    "\n",
    "Use the read_repo_data function to download and process data from specified GitHub repositories, such as 'DataTalksClub/faq' and 'evidentlyai/docs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46127d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For homework, select a GitHub repo with documentation: evidentlyai/docs\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "# Optionally, you can try other repos\n",
    "# dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "# fastai_docs = read_repo_data('fastai', 'fastbook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66276dce",
   "metadata": {},
   "source": [
    "# Print Document Counts\n",
    "\n",
    "Print the number of documents retrieved from each repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a5e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Engineering Zoomcamp documents: 95\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n",
    "\n",
    "# Uncomment to print others\n",
    "# print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "# print(f\"FastAI documents: {len(fastai_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc9f6d",
   "metadata": {},
   "source": [
    "# Day 2: Chunking and Intelligent Processing for Data\n",
    "\n",
    "Welcome to Day 2 of our 7-Day AI Agents Email Crash-Course.\n",
    "\n",
    "In the first part of the course, we focus on data preparation â€“ the process of properly preparing data before it can be used for AI agents.\n",
    "\n",
    "## Small and Large Documents\n",
    "\n",
    "Yesterday (Day 1), we downloaded the data from a GitHub repository and processed it. For some use cases, like the FAQ database, this is sufficient. The questions and answers are small enough. We can put them directly into the search engine.\n",
    "\n",
    "But it's different for the Evidently documentation. These documents are quite large. Let's take a look at this one: https://github.com/evidentlyai/docs/blob/main/docs/library/descriptors.mdx.\n",
    "\n",
    "We could use it as is, but we risk overwhelming our LLMs.\n",
    "\n",
    "## Why We Need to Prepare Large Documents Before Using Them\n",
    "\n",
    "Large documents create several problems:\n",
    "\n",
    "- Token limits: Most LLMs have maximum input token limits\n",
    "- Cost: Longer prompts cost more money\n",
    "- Performance: LLMs perform worse with very long contexts\n",
    "- Relevance: Not all parts of a long document are relevant to a specific question\n",
    "\n",
    "So we need to split documents into smaller subdocuments. For AI applications like RAG (which we will discuss tomorrow), this process is referred to as \"chunking.\"\n",
    "\n",
    "Today, we will cover multiple ways of chunking data:\n",
    "\n",
    "1. Simple character-based chunking\n",
    "2. Paragraph and section-based chunking\n",
    "3. Intelligent chunking with LLM\n",
    "\n",
    "Just so you know, for the last section, you will need a Gemini API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0165d1",
   "metadata": {},
   "source": [
    "## 1. Simple Chunking\n",
    "\n",
    "Let's start with simple chunking. This will be sufficient for most cases.\n",
    "\n",
    "We can continue with the notebook from Day 1. We already downloaded the data from Evidently docs. We put them into the evidently_docs list.\n",
    "\n",
    "This is how the document at index 45 looks like:\n",
    "\n",
    "{'title': 'LLM regression testing',\n",
    " 'description': 'How to run regression testing for LLM outputs.',\n",
    " 'content': 'In this tutorial, you will learn...'\n",
    "}\n",
    "\n",
    "The content field is 21,712 characters long. The simplest thing we can do is cut it into pieces of equal length. For example, for size of 2000 characters, we will have:\n",
    "\n",
    "Chunk 1: 0..2000\n",
    "Chunk 2: 2000..4000\n",
    "Chunk 3: 4000..6000\n",
    "\n",
    "And so on.\n",
    "\n",
    "However, this approach has disadvantages:\n",
    "\n",
    "- Context loss: Important information might be split in the middle\n",
    "- Incomplete sentences: Chunks might end mid-sentence\n",
    "- Missing connections: Related information might end up in different chunks\n",
    "\n",
    "That's why, in practice, we usually make sure there's overlap between chunks. For size 2000 and overlap 1000, we will have:\n",
    "\n",
    "Chunk 1: 0..2000\n",
    "Chunk 2: 1000..3000\n",
    "Chunk 3: 2000..4000\n",
    "...\n",
    "\n",
    "This is better for AI because:\n",
    "\n",
    "- Continuity: Important information isn't lost at chunk boundaries\n",
    "- Context preservation: Related sentences stay together in at least one chunk\n",
    "- Better search: Queries can match information even if it spans chunk boundaries\n",
    "\n",
    "This approach is known as the \"sliding window\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a60c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply it for document 45. This gives us 21 chunks:\n",
    "# 0..2000, 1000..3000, ..., 19000..21000, 20000..21712\n",
    "\n",
    "if len(evidently_docs) > 45:\n",
    "    doc_45_content = evidently_docs[45]['content']\n",
    "    chunks_45 = sliding_window(doc_45_content, 2000, 1000)\n",
    "    print(f\"Document 45 has {len(chunks_45)} chunks\")\n",
    "else:\n",
    "    print(\"Document 45 not available\")\n",
    "\n",
    "# Let's process all the documents:\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total chunks created: {len(evidently_chunks)} from {len(evidently_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc16533",
   "metadata": {},
   "source": [
    "## 2. Splitting by Paragraphs and Sections\n",
    "\n",
    "Splitting by paragraphs is relatively easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc694a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "if len(evidently_docs) > 45:\n",
    "    text = evidently_docs[45]['content']\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "    print(f\"Document 45 has {len(paragraphs)} paragraphs\")\n",
    "    print(f\"First paragraph: {paragraphs[0][:200]}...\")\n",
    "else:\n",
    "    print(\"Document 45 not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ef858",
   "metadata": {},
   "source": [
    "Let's now look at section splitting. Here, we take advantage of the documents' structure. Markdown documents have this structure:\n",
    "\n",
    "# Heading 1\n",
    "## Heading 2  \n",
    "### Heading 3\n",
    "\n",
    "What we can do is split by headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e51045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f13ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to split by second-level headers, that's what we do:\n",
    "\n",
    "if len(evidently_docs) > 45:\n",
    "    text = evidently_docs[45]['content']\n",
    "    sections = split_markdown_by_level(text, level=2)\n",
    "    print(f\"Document 45 has {len(sections)} sections\")\n",
    "    if sections:\n",
    "        print(f\"First section: {sections[0][:200]}...\")\n",
    "else:\n",
    "    print(\"Document 45 not available\")\n",
    "\n",
    "# Now we iterate over all the docs to create the final result:\n",
    "\n",
    "evidently_chunks_sections = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks_sections.append(section_doc)\n",
    "\n",
    "print(f\"Total sections created: {len(evidently_chunks_sections)} from {len(evidently_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b278b0",
   "metadata": {},
   "source": [
    "## 3. Intelligent Chunking with LLM\n",
    "\n",
    "In some cases, we want to be more intelligent with chunking. Instead of doing simple splits, we delegate this work to AI.\n",
    "\n",
    "This makes sense when:\n",
    "\n",
    "- Complex structure: Documents have complex, non-standard structure\n",
    "- Semantic coherence: You want chunks that are semantically meaningful\n",
    "- Custom logic: You need domain-specific splitting rules\n",
    "- Quality over cost: You prioritize quality over processing cost\n",
    "\n",
    "This costs money. In most cases, we don't need intelligent chunking.\n",
    "\n",
    "Simple approaches are sufficient. Use intelligent chunking only when\n",
    "\n",
    "- You already evaluated simpler methods and you can confirm that they produce poor results\n",
    "- You have complex, unstructured documents\n",
    "- Quality is more important than cost\n",
    "- You have the budget for LLM processing\n",
    "\n",
    "Let's create a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Set up Gemini API\n",
    "# You need to set your API key here\n",
    "# genai.configure(api_key=\"YOUR_GEMINI_API_KEY\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n",
    "\n",
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    \n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    sections = response.text.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce91ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we apply this to every document:\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks_intelligent = []\n",
    "\n",
    "# Uncomment the next line and set your API key\n",
    "# genai.configure(api_key=\"YOUR_GEMINI_API_KEY\")\n",
    "\n",
    "for doc in tqdm(evidently_docs[:5]):  # Process only first 5 docs for demo (costs money)\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks_intelligent.append(section_doc)\n",
    "\n",
    "print(f\"Total intelligent sections created: {len(evidently_chunks_intelligent)} from 5 documents\")\n",
    "\n",
    "# Note: This process requires time and incurs costs. As mentioned before, use this only when really necessary.\n",
    "# For most applications, you don't need intelligent chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef29ff",
   "metadata": {},
   "source": [
    "## Bonus: Processing Code in Your GitHub Repository\n",
    "\n",
    "You can use this approach for processing the code in your GitHub repository. You can use a variation of the following prompt:\n",
    "\n",
    "\"Summarize the code in plain English. Briefly describe each class and function/method (their purpose and role), then give a short overall summary of how they work together. Avoid low-level details.\"\n",
    "\n",
    "Then add both the source code and the summary to your documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
