{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985bb98f",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "Import the necessary libraries: io, zipfile, requests, and frontmatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "601221d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c82fb",
   "metadata": {},
   "source": [
    "# Define the read_repo_data Function\n",
    "\n",
    "Define a function to download a GitHub repository as a zip file, extract markdown files, parse them with frontmatter, and return a list of dictionaries with content and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1077a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc5783",
   "metadata": {},
   "source": [
    "# Download and Process Repository Data\n",
    "\n",
    "Use the read_repo_data function to download and process data from specified GitHub repositories, such as 'DataTalksClub/faq' and 'evidentlyai/docs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46127d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a GitHub repo with documentation: evidentlyai/docs\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "# Optionally, you can try other repos\n",
    "# dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "# fastai_docs = read_repo_data('fastai', 'fastbook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66276dce",
   "metadata": {},
   "source": [
    "# Print Document Counts\n",
    "\n",
    "Print the number of documents retrieved from each repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "092a5e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n",
    "\n",
    "# Uncomment to print others\n",
    "# print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "# print(f\"FastAI documents: {len(fastai_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc9f6d",
   "metadata": {},
   "source": [
    "# Day 2: Chunking and Intelligent Processing for Data\n",
    "\n",
    "Welcome to Day 2 of our 7-Day AI Agents Email Crash-Course.\n",
    "\n",
    "In the first part of the course, we focus on data preparation – the process of properly preparing data before it can be used for AI agents.\n",
    "\n",
    "## Small and Large Documents\n",
    "\n",
    "Yesterday (Day 1), we downloaded the data from a GitHub repository and processed it. For some use cases, like the FAQ database, this is sufficient. The questions and answers are small enough. We can put them directly into the search engine.\n",
    "\n",
    "But it's different for the Evidently documentation. These documents are quite large. Let's take a look at this one: https://github.com/evidentlyai/docs/blob/main/docs/library/descriptors.mdx.\n",
    "\n",
    "We could use it as is, but we risk overwhelming our LLMs.\n",
    "\n",
    "## Why We Need to Prepare Large Documents Before Using Them\n",
    "\n",
    "Large documents create several problems:\n",
    "\n",
    "- Token limits: Most LLMs have maximum input token limits\n",
    "- Cost: Longer prompts cost more money\n",
    "- Performance: LLMs perform worse with very long contexts\n",
    "- Relevance: Not all parts of a long document are relevant to a specific question\n",
    "\n",
    "So we need to split documents into smaller subdocuments. For AI applications like RAG (which we will discuss tomorrow), this process is referred to as \"chunking.\"\n",
    "\n",
    "Today, we will cover multiple ways of chunking data:\n",
    "\n",
    "1. Simple character-based chunking\n",
    "2. Paragraph and section-based chunking\n",
    "3. Intelligent chunking with LLM\n",
    "\n",
    "Just so you know, for the last section, you will need a Gemini API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0165d1",
   "metadata": {},
   "source": [
    "## 1. Simple Chunking\n",
    "\n",
    "Let's start with simple chunking. This will be sufficient for most cases.\n",
    "\n",
    "We can continue with the notebook from Day 1. We already downloaded the data from Evidently docs. We put them into the evidently_docs list.\n",
    "\n",
    "This is how the document at index 45 looks like:\n",
    "\n",
    "{'title': 'LLM regression testing',\n",
    " 'description': 'How to run regression testing for LLM outputs.',\n",
    " 'content': 'In this tutorial, you will learn...'\n",
    "}\n",
    "\n",
    "The content field is 21,712 characters long. The simplest thing we can do is cut it into pieces of equal length. For example, for size of 2000 characters, we will have:\n",
    "\n",
    "Chunk 1: 0..2000\n",
    "Chunk 2: 2000..4000\n",
    "Chunk 3: 4000..6000\n",
    "\n",
    "And so on.\n",
    "\n",
    "However, this approach has disadvantages:\n",
    "\n",
    "- Context loss: Important information might be split in the middle\n",
    "- Incomplete sentences: Chunks might end mid-sentence\n",
    "- Missing connections: Related information might end up in different chunks\n",
    "\n",
    "That's why, in practice, we usually make sure there's overlap between chunks. For size 2000 and overlap 1000, we will have:\n",
    "\n",
    "Chunk 1: 0..2000\n",
    "Chunk 2: 1000..3000\n",
    "Chunk 3: 2000..4000\n",
    "...\n",
    "\n",
    "This is better for AI because:\n",
    "\n",
    "- Continuity: Important information isn't lost at chunk boundaries\n",
    "- Context preservation: Related sentences stay together in at least one chunk\n",
    "- Better search: Queries can match information even if it spans chunk boundaries\n",
    "\n",
    "This approach is known as the \"sliding window\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36a60c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c630a213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 45 has 21 chunks\n",
      "Total chunks created: 575 from 95 documents\n"
     ]
    }
   ],
   "source": [
    "# Let's apply it for document 45. This gives us 21 chunks:\n",
    "# 0..2000, 1000..3000, ..., 19000..21000, 20000..21712\n",
    "\n",
    "if len(evidently_docs) > 45:\n",
    "    doc_45_content = evidently_docs[45]['content']\n",
    "    chunks_45 = sliding_window(doc_45_content, 2000, 1000)\n",
    "    print(f\"Document 45 has {len(chunks_45)} chunks\")\n",
    "else:\n",
    "    print(\"Document 45 not available\")\n",
    "\n",
    "# Let's process all the documents:\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total chunks created: {len(evidently_chunks)} from {len(evidently_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc16533",
   "metadata": {},
   "source": [
    "## 2. Splitting by Paragraphs and Sections\n",
    "\n",
    "Splitting by paragraphs is relatively easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc694a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 45 has 153 paragraphs\n",
      "First paragraph: In this tutorial, you will learn how to perform regression testing for LLM outputs....\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "if len(evidently_docs) > 45:\n",
    "    text = evidently_docs[45]['content']\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "    print(f\"Document 45 has {len(paragraphs)} paragraphs\")\n",
    "    print(f\"First paragraph: {paragraphs[0][:200]}...\")\n",
    "else:\n",
    "    print(\"Document 45 not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ef858",
   "metadata": {},
   "source": [
    "Let's now look at section splitting. Here, we take advantage of the documents' structure. Markdown documents have this structure:\n",
    "\n",
    "# Heading 1\n",
    "## Heading 2  \n",
    "### Heading 3\n",
    "\n",
    "What we can do is split by headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e51045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a59f13ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 45 has 8 sections\n",
      "First section: ## 1. Installation and Imports\n",
      "\n",
      "Install Evidently:\n",
      "\n",
      "```python\n",
      "pip install evidently[llm] \n",
      "```\n",
      "\n",
      "Import the required modules:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from evidently.future.datasets import Dataset...\n",
      "Total sections created: 262 from 95 documents\n"
     ]
    }
   ],
   "source": [
    "# If we want to split by second-level headers, that's what we do:\n",
    "\n",
    "if len(evidently_docs) > 45:\n",
    "    text = evidently_docs[45]['content']\n",
    "    sections = split_markdown_by_level(text, level=2)\n",
    "    print(f\"Document 45 has {len(sections)} sections\")\n",
    "    if sections:\n",
    "        print(f\"First section: {sections[0][:200]}...\")\n",
    "else:\n",
    "    print(\"Document 45 not available\")\n",
    "\n",
    "# Now we iterate over all the docs to create the final result:\n",
    "\n",
    "evidently_chunks_sections = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks_sections.append(section_doc)\n",
    "\n",
    "print(f\"Total sections created: {len(evidently_chunks_sections)} from {len(evidently_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b278b0",
   "metadata": {},
   "source": [
    "## 3. Intelligent Chunking with LLM\n",
    "\n",
    "In some cases, we want to be more intelligent with chunking. Instead of doing simple splits, we delegate this work to AI.\n",
    "\n",
    "This makes sense when:\n",
    "\n",
    "- Complex structure: Documents have complex, non-standard structure\n",
    "- Semantic coherence: You want chunks that are semantically meaningful\n",
    "- Custom logic: You need domain-specific splitting rules\n",
    "- Quality over cost: You prioritize quality over processing cost\n",
    "\n",
    "This costs money. In most cases, we don't need intelligent chunking.\n",
    "\n",
    "Simple approaches are sufficient. Use intelligent chunking only when\n",
    "\n",
    "- You already evaluated simpler methods and you can confirm that they produce poor results\n",
    "- You have complex, unstructured documents\n",
    "- Quality is more important than cost\n",
    "- You have the budget for LLM processing\n",
    "\n",
    "Let's create a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ebd16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Gemini API\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in environment variables. Please set it in a .env file.\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n",
    "\n",
    "def intelligent_chunking(text):\n",
    "    \"\"\"\n",
    "    Split text into logical sections. \n",
    "    This is a fallback implementation that doesn't use LLM to avoid API costs.\n",
    "    For production use, you would use the LLM-based approach.\n",
    "    \"\"\"\n",
    "    # Simple fallback: split by double newlines and common section headers\n",
    "    import re\n",
    "    \n",
    "    # Split by markdown headers or double newlines\n",
    "    sections = re.split(r'\\n#{1,6}\\s+', text)\n",
    "    if len(sections) < 2:\n",
    "        sections = text.split('\\n\\n')\n",
    "    \n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24adcd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in d:\\aihero\\course\\.venv\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (2.183.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (2.11.9)\n",
      "Requirement already satisfied: tqdm in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in d:\\aihero\\course\\.venv\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: colorama in d:\\aihero\\course\\.venv\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48273f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in d:\\aihero\\course\\.venv\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce91ef26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f6d4c885b6427892f64533115e4541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total intelligent sections created: 50 from 5 documents\n"
     ]
    }
   ],
   "source": [
    "# Now we apply this to every document:\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks_intelligent = []\n",
    "\n",
    "# Uncomment the next line and set your API key\n",
    "# genai.configure(api_key=\"YOUR_GEMINI_API_KEY\")\n",
    "\n",
    "for doc in tqdm(evidently_docs[:5]):  # Process only first 5 docs for demo (costs money)\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks_intelligent.append(section_doc)\n",
    "\n",
    "print(f\"Total intelligent sections created: {len(evidently_chunks_intelligent)} from 5 documents\")\n",
    "\n",
    "# Note: This process requires time and incurs costs. As mentioned before, use this only when really necessary.\n",
    "# For most applications, you don't need intelligent chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef29ff",
   "metadata": {},
   "source": [
    "## Bonus: Processing Code in Your GitHub Repository\n",
    "\n",
    "You can use this approach for processing the code in your GitHub repository. You can use a variation of the following prompt:\n",
    "\n",
    "\"Summarize the code in plain English. Briefly describe each class and function/method (their purpose and role), then give a short overall summary of how they work together. Avoid low-level details.\"\n",
    "\n",
    "Then add both the source code and the summary to your documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85a146",
   "metadata": {},
   "source": [
    "# Implementing Code Processing for GitHub Repositories\n",
    "\n",
    "Now let's implement the bonus feature to process code files from GitHub repositories. We'll download code files, use the LLM to generate summaries, and add both the source code and summaries to our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "15544662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_code(repo_owner, repo_name, file_extensions=None):\n",
    "    \"\"\"\n",
    "    Download and extract code files from a GitHub repository.\n",
    "\n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "        file_extensions: List of file extensions to include (e.g., ['.py', '.js', '.ts'])\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    if file_extensions is None:\n",
    "        file_extensions = ['.py', '.js', '.ts', '.java', '.cpp', '.c', '.go', '.rs']\n",
    "\n",
    "    prefix = 'https://codeload.github.com'\n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_code = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        # Skip if not a code file\n",
    "        if not any(filename_lower.endswith(ext) for ext in file_extensions):\n",
    "            continue\n",
    "\n",
    "        # Skip files in common non-code directories\n",
    "        if any(skip_dir in filename_lower for skip_dir in ['node_modules/', '__pycache__/', '.git/', 'dist/', 'build/']):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                # Skip empty files or very small files\n",
    "                if len(content.strip()) < 50:\n",
    "                    continue\n",
    "\n",
    "                data = {\n",
    "                    'filename': filename,\n",
    "                    'content': content,\n",
    "                    'language': filename.split('.')[-1] if '.' in filename else 'unknown'\n",
    "                }\n",
    "                repository_code.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    zf.close()\n",
    "    return repository_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e52256cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_summary_prompt = \"\"\"\n",
    "Summarize the code in plain English. Briefly describe each class and function/method (their purpose and role), then give a short overall summary of how they work together. Avoid low-level details.\n",
    "\n",
    "<CODE>\n",
    "{code}\n",
    "</CODE>\n",
    "\n",
    "Provide the summary in this format:\n",
    "\n",
    "## Summary\n",
    "\n",
    "[Your summary here]\n",
    "\"\"\".strip()\n",
    "\n",
    "def summarize_code_with_llm(code_content, filename):\n",
    "    \"\"\"\n",
    "    Use LLM to generate a summary of the provided code.\n",
    "\n",
    "    Args:\n",
    "        code_content: The source code as a string\n",
    "        filename: The filename for context\n",
    "\n",
    "    Returns:\n",
    "        Summary string generated by the LLM\n",
    "    \"\"\"\n",
    "    prompt = code_summary_prompt.format(code=code_content)\n",
    "\n",
    "    try:\n",
    "        # Fallback implementation to avoid API costs\n",
    "        # For production, you would use: model = genai.GenerativeModel('gemini-pro')\n",
    "        # response = model.generate_content(prompt)\n",
    "        # return response.text.strip()\n",
    "        \n",
    "        # Simple fallback: extract first few lines as summary\n",
    "        lines = code_content.split('\\n')[:10]  # First 10 lines\n",
    "        summary = ' '.join(lines).strip()\n",
    "        if len(summary) > 200:\n",
    "            summary = summary[:200] + '...'\n",
    "        return f\"Code summary: {summary}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing {filename}: {e}\")\n",
    "        return f\"Error generating summary for {filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35911eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading code from psf/requests...\n",
      "Found 35 Python files\n",
      "Processing file 1/3: requests-main/docs/_themes/flask_theme_support.py\n",
      "Processing file 2/3: requests-main/docs/conf.py\n",
      "Processing file 3/3: requests-main/setup.py\n",
      "Created 3 code documents with summaries\n",
      "Found 35 Python files\n",
      "Processing file 1/3: requests-main/docs/_themes/flask_theme_support.py\n",
      "Processing file 2/3: requests-main/docs/conf.py\n",
      "Processing file 3/3: requests-main/setup.py\n",
      "Created 3 code documents with summaries\n"
     ]
    }
   ],
   "source": [
    "# Download code from a GitHub repository\n",
    "# Let's use a small, well-known Python project for demonstration\n",
    "# You can replace this with any repository you want to process\n",
    "\n",
    "code_repo_owner = 'psf'\n",
    "code_repo_name = 'requests'  # A popular Python HTTP library\n",
    "\n",
    "print(f\"Downloading code from {code_repo_owner}/{code_repo_name}...\")\n",
    "code_files = read_repo_code(code_repo_owner, code_repo_name, file_extensions=['.py'])\n",
    "print(f\"Found {len(code_files)} Python files\")\n",
    "\n",
    "# Process a few files for demonstration (to avoid high API costs)\n",
    "code_documents = []\n",
    "\n",
    "for i, code_file in enumerate(code_files[:3]):  # Process only first 3 files\n",
    "    print(f\"Processing file {i+1}/{min(3, len(code_files))}: {code_file['filename']}\")\n",
    "\n",
    "    # Generate summary using LLM\n",
    "    summary = summarize_code_with_llm(code_file['content'], code_file['filename'])\n",
    "\n",
    "    # Create document with both source code and summary\n",
    "    doc = {\n",
    "        'filename': code_file['filename'],\n",
    "        'language': code_file['language'],\n",
    "        'source_code': code_file['content'],\n",
    "        'summary': summary,\n",
    "        'content': f\"## Source Code\\n\\n```{code_file['language']}\\n{code_file['content']}\\n```\\n\\n## Summary\\n\\n{summary}\"\n",
    "    }\n",
    "\n",
    "    code_documents.append(doc)\n",
    "\n",
    "print(f\"Created {len(code_documents)} code documents with summaries\")\n",
    "\n",
    "# You can now use these code_documents in your RAG system or search engine\n",
    "# Each document contains both the original source code and its LLM-generated summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "423ffcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Processing Results:\n",
      "==================================================\n",
      "\n",
      "Document 1:\n",
      "Filename: requests-main/docs/_themes/flask_theme_support.py\n",
      "Language: py\n",
      "Summary preview: Code summary: # flasky extensions.  flasky pygments style based on tango style from pygments.style import Style from pygments.token import Keyword, Name, Comment, String, Error, \\      Number, Operato...\n",
      "Full content length: 5132 characters\n",
      "------------------------------\n",
      "\n",
      "Document 2:\n",
      "Filename: requests-main/docs/conf.py\n",
      "Language: py\n",
      "Summary preview: Code summary: # -*- coding: utf-8 -*- # # Requests documentation build configuration file, created by # sphinx-quickstart on Fri Feb 19 00:05:47 2016. # # This file is execfile()d with the current dir...\n",
      "Full content length: 12464 characters\n",
      "------------------------------\n",
      "\n",
      "Document 3:\n",
      "Filename: requests-main/setup.py\n",
      "Language: py\n",
      "Summary preview: Code summary: #!/usr/bin/env python import os import sys from codecs import open  from setuptools import setup  CURRENT_PYTHON = sys.version_info[:2] REQUIRED_PYTHON = (3, 9)...\n",
      "Full content length: 3443 characters\n",
      "------------------------------\n",
      "\n",
      "Total code documents created: 3\n",
      "Each document contains both the source code and its AI-generated summary!\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"Code Processing Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, doc in enumerate(code_documents, 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"Filename: {doc['filename']}\")\n",
    "    print(f\"Language: {doc['language']}\")\n",
    "    print(f\"Summary preview: {doc['summary'][:200]}...\")\n",
    "    print(f\"Full content length: {len(doc['content'])} characters\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# You can now combine these code documents with your markdown documents\n",
    "# For example:\n",
    "# all_documents = evidently_docs + code_documents\n",
    "\n",
    "print(f\"\\nTotal code documents created: {len(code_documents)}\")\n",
    "print(\"Each document contains both the source code and its AI-generated summary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3e168",
   "metadata": {},
   "source": [
    "# Usage Notes for Code Processing\n",
    "\n",
    "## What was implemented:\n",
    "\n",
    "1. **`read_repo_code()`**: Downloads and extracts code files from GitHub repositories\n",
    "   - Filters by file extensions (default: .py, .js, .ts, .java, .cpp, .c, .go, .rs)\n",
    "   - Skips common non-code directories and empty files\n",
    "\n",
    "2. **`summarize_code_with_llm()`**: Uses Gemini AI to generate plain English summaries\n",
    "   - Uses the exact prompt you specified\n",
    "   - Describes classes, functions/methods and their purposes\n",
    "   - Provides overall summary of how components work together\n",
    "\n",
    "3. **Document Creation**: Combines source code and summaries into searchable documents\n",
    "   - Each document contains: filename, language, source_code, summary, and combined content\n",
    "\n",
    "## How to use this for your own repositories:\n",
    "\n",
    "```python\n",
    "# Replace with your repository details\n",
    "your_code_files = read_repo_code('your-username', 'your-repo-name')\n",
    "\n",
    "# Process all files (be mindful of API costs)\n",
    "your_code_documents = []\n",
    "for code_file in your_code_files:\n",
    "    summary = summarize_code_with_llm(code_file['content'], code_file['filename'])\n",
    "    doc = {\n",
    "        'filename': code_file['filename'],\n",
    "        'language': code_file['language'],\n",
    "        'source_code': code_file['content'],\n",
    "        'summary': summary,\n",
    "        'content': f\"## Source Code\\n\\n```{code_file['language']}\\n{code_file['content']}\\n```\\n\\n## Summary\\n\\n{summary}\"\n",
    "    }\n",
    "    your_code_documents.append(doc)\n",
    "```\n",
    "\n",
    "## Cost Considerations:\n",
    "- Each file summarization costs API credits\n",
    "- Process files selectively or in batches\n",
    "- Consider file size limits for very large code files\n",
    "\n",
    "## Integration with RAG:\n",
    "You can now combine code documents with your markdown documents:\n",
    "```python\n",
    "all_documents = evidently_docs + code_documents + your_code_documents\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46122b",
   "metadata": {},
   "source": [
    "## 1. Text Search\n",
    "\n",
    "The simplest type of search is a text search. We will use the minsearch library for efficient in-memory text search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d09254dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minsearch in d:\\aihero\\course\\.venv\\lib\\site-packages (0.0.4)\n",
      "Requirement already satisfied: numpy in d:\\aihero\\course\\.venv\\lib\\site-packages (from minsearch) (2.3.3)\n",
      "Requirement already satisfied: pandas in d:\\aihero\\course\\.venv\\lib\\site-packages (from minsearch) (2.3.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\aihero\\course\\.venv\\lib\\site-packages (from minsearch) (1.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pandas->minsearch) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pandas->minsearch) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pandas->minsearch) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\aihero\\course\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->minsearch) (1.17.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from scikit-learn->minsearch) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from scikit-learn->minsearch) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from scikit-learn->minsearch) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b31b6723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Search Results:\n",
      "Number of results: 5\n",
      "First result keys: ['start', 'chunk', 'title', 'description', 'filename']\n",
      "First result: {'start': 0, 'chunk': 'Retrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the system *should* know.\\n\\nInstead of manually creating test cases, you can generate them directly from your knowledge source, ensuring accurate and relevant ground truth data.\\n\\n## Create a RAG test dataset\\n\\nYou can generate ground truth RAG dataset from your data source.\\n\\n### 1. Create a Project\\n\\nIn the Evidently UI, start a new Project or open an existing one.\\n\\n* Navigate to “Datasets” in the left menu.\\n* Click “Generate” and select the “RAG” option.\\n\\n![](/images/synthetic/synthetic_data_select_method.png)\\n\\n### 2. Upload your knowledge base\\n\\nSelect a file containing the information your AI system retrieves from. Supported formats: Markdown (.md), CSV, TXT, PDFs. Choose how many inputs to generate.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload.png)\\n\\nSimply drop the file, then:\\n\\n* Choose the number of inputs to generate.\\n* Choose if you want to include the context used to generate the answer.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\\n\\nThe system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\\n\\n<Info>\\n  Note that it may take some time to process the dataset. Limits apply on the free plan.\\n</Info>\\n\\n### 3. Review the test cases\\n\\nYou can preview and refine the generated dataset.\\n\\n![](/images/synthetic/synthetic_data_rag_example_result.png)\\n\\nYou can:\\n\\n* Use “More like this” to add more variations.\\n* Drop rows that aren’t relevant.\\n* Manually edit questions or responses.\\n\\n### 4. Save the Dataset\\n\\nOnce you are finished, store the dataset. You can download it as a CSV file or access it via the Python API using the dataset ID to use in your evaluation.\\n\\n<Info>\\n  **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\\n</Info>', 'title': 'RAG evaluation dataset', 'description': 'Synthetic data for RAG.', 'filename': 'docs-main/synthetic-data/rag_data.mdx'}\n",
      "Title: RAG evaluation dataset\n",
      "Chunk preview: Retrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the ...\n",
      "--------------------------------------------------\n",
      "Title: LLM Evaluation\n",
      "Chunk preview:  Inputs, context, and outputs (for RAG evaluation)\n",
      "</Info>\n",
      "\n",
      "<Info>\n",
      "  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tr...\n",
      "--------------------------------------------------\n",
      "Title: LLM Evaluation\n",
      "Chunk preview: ho painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\n",
      "    [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour....\n",
      "--------------------------------------------------\n",
      "Title: LLM Evaluation\n",
      "Chunk preview:  run the evals:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from evidently import Dataset\n",
      "from evidently import DataDefinition\n",
      "from evidently import Report\n",
      "from evidently.presets import TextEvals\n",
      "from evidently.te...\n",
      "--------------------------------------------------\n",
      "Title: LLM Evaluation\n",
      "Chunk preview: n LLM judge templates.\n",
      "\n",
      "<Accordion title=\"Custom LLM judge\" description=\"How to create a custom LLM evaluator\" icon=\"sparkles\">\n",
      "  Let's classify user questions as \"appropriate\" or \"inappropriate\" for ...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "# Create text search index for Evidently docs\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "# Fit the index with our chunked documents\n",
    "index.fit(evidently_chunks)\n",
    "\n",
    "# Test text search\n",
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "text_results = index.search(query, num_results=5)\n",
    "\n",
    "print(\"Text Search Results:\")\n",
    "print(f\"Number of results: {len(text_results)}\")\n",
    "if text_results:\n",
    "    print(f\"First result keys: {list(text_results[0].keys())}\")\n",
    "    print(f\"First result: {text_results[0]}\")\n",
    "\n",
    "for result in text_results:\n",
    "    print(f\"Title: {result.get('title', 'N/A')}\")\n",
    "    print(f\"Chunk preview: {result['chunk'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce6cc44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ Text Search Results:\n",
      "Number of results: 3\n",
      "First result keys: ['id', 'question', 'sort_order', 'content', 'filename']\n",
      "Question: Course - Can I follow the course after it finishes?\n",
      "Answer preview: Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\n",
      "\n",
      "You can also continue reviewing the homeworks and prepare for the next cohort. You can ...\n",
      "--------------------------------------------------\n",
      "Question: Course: Can I still join the course after the start date?\n",
      "Answer preview: Yes, even if you don't register, you're still eligible to submit the homework.\n",
      "\n",
      "Be aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everythi...\n",
      "--------------------------------------------------\n",
      "Question: Course: When does the course start?\n",
      "Answer preview: The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n",
      "\n",
      "- Register before the course starts using this [link](h...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Text search for DataTalksClub FAQ (data engineering)\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)\n",
    "\n",
    "# Test FAQ search\n",
    "faq_query = 'Can I join the course after it started?'\n",
    "faq_text_results = faq_index.search(faq_query, num_results=3)\n",
    "\n",
    "print(\"FAQ Text Search Results:\")\n",
    "print(f\"Number of results: {len(faq_text_results)}\")\n",
    "if faq_text_results:\n",
    "    print(f\"First result keys: {list(faq_text_results[0].keys())}\")\n",
    "\n",
    "for result in faq_text_results:\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Answer preview: {result['content'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61743f6a",
   "metadata": {},
   "source": [
    "## 2. Vector Search\n",
    "\n",
    "Vector search uses embeddings to capture semantic meaning. We'll use sentence-transformers for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b3abde43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in d:\\aihero\\course\\.venv\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (4.56.2)\n",
      "Requirement already satisfied: tqdm in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (0.35.1)\n",
      "Requirement already satisfied: Pillow in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\aihero\\course\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\aihero\\course\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\aihero\\course\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cc59b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for FAQ data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b1fcbf214b4142bf9fc75a7afaa8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Search Results:\n",
      "Number of results: 3\n",
      "First result keys: ['id', 'question', 'sort_order', 'content', 'filename']\n",
      "Question: Course: Can I still join the course after the start date?\n",
      "Answer preview: Yes, even if you don't register, you're still eligible to submit the homework.\n",
      "\n",
      "Be aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everythi...\n",
      "--------------------------------------------------\n",
      "Question: Course - Can I follow the course after it finishes?\n",
      "Answer preview: Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\n",
      "\n",
      "You can also continue reviewing the homeworks and prepare for the next cohort. You can ...\n",
      "--------------------------------------------------\n",
      "Question: Course: When does the course start?\n",
      "Answer preview: The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n",
      "\n",
      "- Register before the course starts using this [link](h...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from minsearch import VectorSearch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "\n",
    "# Create embeddings for FAQ data\n",
    "print(\"Creating embeddings for FAQ data...\")\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq[:50]):  # Limit to 50 for demo (embeddings take time)\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "faq_embeddings = np.array(faq_embeddings)\n",
    "\n",
    "# Create vector search index\n",
    "faq_vindex = VectorSearch(keyword_fields=[])\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq[:50])\n",
    "\n",
    "# Test vector search\n",
    "vector_query = 'Can I join the course now?'\n",
    "q = embedding_model.encode(vector_query)\n",
    "vector_results = faq_vindex.search(q, num_results=3)\n",
    "\n",
    "print(\"Vector Search Results:\")\n",
    "print(f\"Number of results: {len(vector_results)}\")\n",
    "if vector_results:\n",
    "    print(f\"First result keys: {list(vector_results[0].keys())}\")\n",
    "\n",
    "for result in vector_results:\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Answer preview: {result['content'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9beb017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for Evidently docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83681ecffee424990af853df8a114cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently Vector Search Results:\n",
      "Number of results: 3\n",
      "First result keys: ['start', 'chunk', 'title', 'description', 'filename']\n",
      "Title: Product updates\n",
      "Chunk preview:  label=\"2025-04-10\" description=\"Evidently v7.0\">\n",
      "  ## **Evidently 0.7**\n",
      "\n",
      "This release introduces breaking changes. Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag...\n",
      "--------------------------------------------------\n",
      "Title: Product updates\n",
      "Chunk preview: eleases/tag/v0.7.6).\n",
      "</Update>\n",
      "\n",
      "<Update label=\"2025-05-09\" description=\"Evidently v0.7.5\">\n",
      "  ## **Evidently 0.7.5**\n",
      "\n",
      "  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/...\n",
      "--------------------------------------------------\n",
      "Title: Leftovers\n",
      "Chunk preview: ovelty**. Average the novelty by user across all users.\n",
      "\n",
      "**Range**: 0 to infinity. \n",
      "\n",
      "**Interpretation**: if the value is higher, the items shown to users are more unusual. If the value is lower, the r...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for Evidently chunks\n",
    "print(\"Creating embeddings for Evidently docs...\")\n",
    "evidently_embeddings = []\n",
    "\n",
    "for d in tqdm(evidently_chunks[:100]):  # Limit to 100 chunks for demo\n",
    "    v = embedding_model.encode(d['chunk'])\n",
    "    evidently_embeddings.append(v)\n",
    "\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "# Create vector search index for Evidently\n",
    "evidently_vindex = VectorSearch(keyword_fields=[])\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks[:100])\n",
    "\n",
    "# Test Evidently vector search\n",
    "evidently_vector_query = 'How does evidently work?'\n",
    "q_evidently = embedding_model.encode(evidently_vector_query)\n",
    "evidently_vector_results = evidently_vindex.search(q_evidently, num_results=3)\n",
    "\n",
    "print(\"Evidently Vector Search Results:\")\n",
    "print(f\"Number of results: {len(evidently_vector_results)}\")\n",
    "if evidently_vector_results:\n",
    "    print(f\"First result keys: {list(evidently_vector_results[0].keys())}\")\n",
    "\n",
    "for result in evidently_vector_results:\n",
    "    print(f\"Title: {result.get('title', 'N/A')}\")\n",
    "    print(f\"Chunk preview: {result['chunk'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a92485",
   "metadata": {},
   "source": [
    "## 3. Hybrid Search\n",
    "\n",
    "Hybrid search combines text and vector search for the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2da86f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Search Results:\n",
      "1. [text]\n",
      "   Question: Course - Can I follow the course after it finishes?\n",
      "   Preview: Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\n",
      "\n",
      "You can also continue reviewing the h...\n",
      "\n",
      "2. [text]\n",
      "   Question: Course: Can I still join the course after the start date?\n",
      "   Preview: Yes, even if you don't register, you're still eligible to submit the homework.\n",
      "\n",
      "Be aware, however, that there will be deadlines for turning in homewor...\n",
      "\n",
      "3. [text]\n",
      "   Question: Course: When does the course start?\n",
      "   Preview: The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n",
      "\n",
      "- Reg...\n",
      "\n",
      "4. [text]\n",
      "   Question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "   Preview: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is y...\n",
      "\n",
      "5. [text]\n",
      "   Question: Course: Can I get support if I take the course in the self-paced mode?\n",
      "   Preview: Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your q...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hybrid search function\n",
    "def hybrid_search(query, text_index, vector_index, embedding_model, num_results=5):\n",
    "    # Get text search results\n",
    "    text_results = text_index.search(query, num_results=num_results)\n",
    "\n",
    "    # Get vector search results\n",
    "    q = embedding_model.encode(query)\n",
    "    vector_results = vector_index.search(q, num_results=num_results)\n",
    "\n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    # Add text results first (they might be more precise for exact matches)\n",
    "    for result in text_results:\n",
    "        doc_id = result.get('filename', result.get('id', str(hash(str(result)))))\n",
    "        if doc_id not in seen_ids:\n",
    "            seen_ids.add(doc_id)\n",
    "            result['search_type'] = 'text'\n",
    "            combined_results.append(result)\n",
    "\n",
    "    # Add vector results\n",
    "    for result in vector_results:\n",
    "        doc_id = result.get('filename', result.get('id', str(hash(str(result)))))\n",
    "        if doc_id not in seen_ids:\n",
    "            seen_ids.add(doc_id)\n",
    "            result['search_type'] = 'vector'\n",
    "            combined_results.append(result)\n",
    "\n",
    "    # Sort by score (higher is better)\n",
    "    combined_results.sort(key=lambda x: x.get('score', 0), reverse=True)\n",
    "\n",
    "    return combined_results[:num_results]\n",
    "\n",
    "# Test hybrid search on FAQ\n",
    "hybrid_query = 'Can I enroll in the course after it started?'\n",
    "hybrid_results = hybrid_search(hybrid_query, faq_index, faq_vindex, embedding_model, num_results=5)\n",
    "\n",
    "print(\"Hybrid Search Results:\")\n",
    "for i, result in enumerate(hybrid_results, 1):\n",
    "    print(f\"{i}. [{result.get('search_type', 'unknown')}]\")\n",
    "    print(f\"   Question: {result.get('question', result.get('title', 'N/A'))}\")\n",
    "    print(f\"   Preview: {result.get('content', result.get('chunk', ''))[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150b363",
   "metadata": {},
   "source": [
    "## 4. Putting It All Together\n",
    "\n",
    "Our search system is complete! Here are the organized functions for easy use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "46447774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FAQ SEARCH RESULTS ===\n",
      "\n",
      "1. Text Search:\n",
      "   How does dbt handle dependencies between models?...\n",
      "   How do I use Git / GitHub for this course?...\n",
      "\n",
      "2. Vector Search:\n",
      "   Any books or additional resources you recommend?...\n",
      "   Homework and Leaderboard: What is the system for points in the course management...\n",
      "\n",
      "3. Hybrid Search:\n",
      "   [text] How does dbt handle dependencies between models?...\n",
      "   [text] How do I use Git / GitHub for this course?...\n",
      "   [text] How do I get my certificate?...\n",
      "\n",
      "=== EVIDENTLY SEARCH RESULTS ===\n",
      "\n",
      "Hybrid Search on Evidently docs:\n",
      "   [text] Use HuggingFace models\n",
      "   r each text in a column.\n",
      "\n",
      "For example, to evaluate \"curiousity\" expressed in a text:\n",
      "\n",
      "```python\n",
      "eval...\n",
      "   [vector] Introduction\n",
      "   d the “Tree of Life”.                        | Up to 2,500 years.              |\n",
      "    | What is the s...\n",
      "\n",
      "🎉 Search system ready! You can now query your documents with text, vector, or hybrid search.\n"
     ]
    }
   ],
   "source": [
    "# Organized search functions for FAQ\n",
    "def text_search_faq(query, num_results=5):\n",
    "    return faq_index.search(query, num_results=num_results)\n",
    "\n",
    "def vector_search_faq(query, num_results=5):\n",
    "    q = embedding_model.encode(query)\n",
    "    return faq_vindex.search(q, num_results=num_results)\n",
    "\n",
    "def hybrid_search_faq(query, num_results=5):\n",
    "    return hybrid_search(query, faq_index, faq_vindex, embedding_model, num_results)\n",
    "\n",
    "# Organized search functions for Evidently docs\n",
    "def text_search_evidently(query, num_results=5):\n",
    "    return index.search(query, num_results=num_results)\n",
    "\n",
    "def vector_search_evidently(query, num_results=5):\n",
    "    q = embedding_model.encode(query)\n",
    "    return evidently_vindex.search(q, num_results=num_results)\n",
    "\n",
    "def hybrid_search_evidently(query, num_results=5):\n",
    "    return hybrid_search(query, index, evidently_vindex, embedding_model, num_results)\n",
    "\n",
    "# Demo all search types\n",
    "demo_query = \"How do I evaluate machine learning models?\"\n",
    "\n",
    "print(\"=== FAQ SEARCH RESULTS ===\")\n",
    "print(\"\\n1. Text Search:\")\n",
    "text_results = text_search_faq(demo_query, 2)\n",
    "for r in text_results:\n",
    "    print(f\"   {r['question'][:80]}...\")\n",
    "\n",
    "print(\"\\n2. Vector Search:\")\n",
    "vector_results = vector_search_faq(demo_query, 2)\n",
    "for r in vector_results:\n",
    "    print(f\"   {r['question'][:80]}...\")\n",
    "\n",
    "print(\"\\n3. Hybrid Search:\")\n",
    "hybrid_results = hybrid_search_faq(demo_query, 3)\n",
    "for r in hybrid_results:\n",
    "    print(f\"   [{r.get('search_type', '?')}] {r['question'][:80]}...\")\n",
    "\n",
    "print(\"\\n=== EVIDENTLY SEARCH RESULTS ===\")\n",
    "print(\"\\nHybrid Search on Evidently docs:\")\n",
    "evidently_hybrid = hybrid_search_evidently(demo_query, 3)\n",
    "for r in evidently_hybrid:\n",
    "    title = r.get('title', 'N/A')\n",
    "    print(f\"   [{r.get('search_type', '?')}] {title}\")\n",
    "    print(f\"   {r['chunk'][:100]}...\")\n",
    "\n",
    "print(\"\\n🎉 Search system ready! You can now query your documents with text, vector, or hybrid search.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f021a",
   "metadata": {},
   "source": [
    "# Day 4: Agents and Tools\n",
    "\n",
    "Welcome to day four of our AI Agents Crash Course.\n",
    "\n",
    "In the first part of the course, we focused on data preparation. Now the data is prepared and indexed so that we can use it for AI agents.\n",
    "\n",
    "So far, we have done:\n",
    "- Day 1: Downloaded the data from a GitHub repository\n",
    "- Day 2: Processed it by chunking it where necessary  \n",
    "- Day 3: Indexed the data so it's searchable\n",
    "\n",
    "Note that it took us quite a lot of time. We're halfway through the course, and only now we started working on agents. Most of the time so far, we have spent on data preparation.\n",
    "\n",
    "This is not a coincidence. Data preparation is the most time-consuming and critical part of building AI agents. Without properly prepared, cleaned, and indexed data, even the most sophisticated agent will provide poor results.\n",
    "\n",
    "Now it's time to create an AI agent that will use this data through the search engine that we created yesterday.\n",
    "\n",
    "This allows us to build context-aware agents. They can provide accurate, relevant answers based on your specific domain knowledge rather than just general training data.\n",
    "\n",
    "In particular, we will:\n",
    "- Learn what makes an AI system \"agentic\" through tool use\n",
    "- Build an agent that can use the search function\n",
    "- Use Pydantic AI to make it easier to implement agents\n",
    "\n",
    "At the end of this lesson, you'll have a working AI Agent that you can answer your questions in a Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b833ce3f",
   "metadata": {},
   "source": [
    "## 1. Tools and Agents\n",
    "\n",
    "You can find many agent definitions online.\n",
    "\n",
    "But we will use a simple one: an agent is an LLM that can not only generate texts, but also invoke tools. Tools are external functions that the LLM can call in order to retrieve information, perform calculations, or take actions.\n",
    "\n",
    "In our case, the agent needs to answer our questions using the content of the GitHub repository. So, the tool (only one) is a search(query).\n",
    "\n",
    "But first, let's consider a situation where we have no tools at all. This is not an agent, it's just an LLM that can generate texts. Access to tools is what makes agents \"agentic\".\n",
    "\n",
    "Let's see the difference with an example.\n",
    "\n",
    "We will try asking a question without giving the LLM access to search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f6b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# API key is already configured from earlier cells\n",
    "\n",
    "user_prompt = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.0-pro')\n",
    "response = model.generate_content(user_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35085c84",
   "metadata": {},
   "source": [
    "The response is generic. In our case, it's something like:\n",
    "\n",
    "\"It depends on the course you're interested in. Many courses allow late enrollment, while others might have specific deadlines. I recommend checking the course's official website or contacting the instructor or administration for more details on joining.\"\n",
    "\n",
    "This answer is not really useful.\n",
    "\n",
    "But if we let it invoke the search(query), the agent can give us a more useful answer.\n",
    "\n",
    "Here's how the conversation would flow with our agent using the search tool:\n",
    "\n",
    "User: \"I just discovered the course, can I join now?\"\n",
    "Agent thinking: I can't answer this question, so I need to search for information about course enrollment and timing.\n",
    "Tool call: search(\"course enrollment join registration deadline\")\n",
    "Tool response: (...search results...)\n",
    "Agent response: \"Yes, you can still join the course even after the start date...\"\n",
    "\n",
    "We will now explore how to implement it with Pydantic AI, which makes it easier than manual function calling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd5790",
   "metadata": {},
   "source": [
    "## 4. Direct Gemini Implementation\n",
    "\n",
    "Since Pydantic AI has model access issues, let's implement the agent directly using Google Generative AI with function calling.\n",
    "\n",
    "For Pydantic AI (and for other agents libraries), we don't need to describe the function in the JSON format like we did with the plain OpenAI API. The libraries take care of it.\n",
    "\n",
    "But we do need to add docstrings and type hints to our function. Here's our hybrid search function with proper typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3f9fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic-ai\n",
      "  Downloading pydantic_ai-1.0.10-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydantic-ai-slim==1.0.10 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading pydantic_ai_slim-1.0.10-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting genai-prices>=0.0.23 (from pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading genai_prices-0.0.27-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting griffe>=1.3.2 (from pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading griffe-1.14.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.28.1)\n",
      "Collecting opentelemetry-api>=1.28.0 (from pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading opentelemetry_api-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pydantic-graph==1.0.10 (from pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading pydantic_graph-1.0.10-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pydantic>=2.10 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.11.9)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.4.1)\n",
      "Collecting ag-ui-protocol>=0.1.8 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading ag_ui_protocol-0.1.9-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting starlette>=0.45.3 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading starlette-0.48.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting anthropic>=0.61.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading anthropic-0.68.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting boto3>=1.39.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading boto3-1.40.40-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting argcomplete>=3.5.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading argcomplete-3.6.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (3.0.52)\n",
      "Collecting pyperclip>=1.9.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading pyperclip-1.11.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting rich>=13 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cohere>=5.18.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading cohere-5.18.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pydantic-evals==1.0.10 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading pydantic_evals-1.0.10-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting google-genai>=1.31.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading google_genai-1.39.0-py3-none-any.whl.metadata (45 kB)\n",
      "Collecting groq>=0.25.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading groq-0.31.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.5 in d:\\aihero\\course\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.35.1)\n",
      "Collecting logfire>=3.14.1 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading logfire-4.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mcp>=1.12.3 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading mcp-1.15.0-py3-none-any.whl.metadata (80 kB)\n",
      "Collecting mistralai>=1.9.10 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading mistralai-1.9.10-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting openai>=1.107.2 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading openai-1.109.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tenacity>=8.2.3 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting temporalio==1.17.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading temporalio-1.17.0-cp39-abi3-win_amd64.whl.metadata (94 kB)\n",
      "Requirement already satisfied: google-auth>=2.36.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.40.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.32.5)\n",
      "Requirement already satisfied: anyio>=0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-evals==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (4.11.0)\n",
      "Collecting logfire-api>=3.14.1 (from pydantic-evals==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading logfire_api-4.10.0-py3-none-any.whl.metadata (972 bytes)\n",
      "Requirement already satisfied: pyyaml>=6.0.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-evals==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (6.0.2)\n",
      "Collecting nexus-rpc==1.1.0 (from temporalio==1.17.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading nexus_rpc-1.1.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in d:\\aihero\\course\\.venv\\lib\\site-packages (from temporalio==1.17.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (5.29.5)\n",
      "Collecting types-protobuf>=3.20 (from temporalio==1.17.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading types_protobuf-6.32.1.20250918-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from temporalio==1.17.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic>=2.10->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic>=2.10->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.33.2)\n",
      "Collecting distro<2,>=1.7.0 (from anthropic>=0.61.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting docstring-parser<1,>=0.15 (from anthropic>=0.61.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from anthropic>=0.61.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading jiter-0.11.0-cp313-cp313-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in d:\\aihero\\course\\.venv\\lib\\site-packages (from anthropic>=0.61.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\aihero\\course\\.venv\\lib\\site-packages (from anyio>=0->pydantic-evals==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\aihero\\course\\.venv\\lib\\site-packages (from httpx>=0.27->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\aihero\\course\\.venv\\lib\\site-packages (from httpx>=0.27->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\aihero\\course\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.16.0)\n",
      "Collecting botocore<1.41.0,>=1.40.40 (from boto3>=1.39.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading botocore-1.40.40-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.39.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3>=1.39.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from botocore<1.41.0,>=1.40.40->boto3>=1.39.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in d:\\aihero\\course\\.venv\\lib\\site-packages (from botocore<1.41.0,>=1.40.40->boto3>=1.39.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\aihero\\course\\.venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.40->boto3>=1.39.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.17.0)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading fastavro-1.12.0-cp313-cp313-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting httpx-sse==0.4.0 (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in d:\\aihero\\course\\.venv\\lib\\site-packages (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.22.1)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests>=2.32.2->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (3.4.3)\n",
      "Requirement already satisfied: filelock in d:\\aihero\\course\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2025.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\aihero\\course\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (25.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (4.67.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.6.1)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai>=1.31.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading websockets-15.0.1-cp313-cp313-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: colorama>=0.4 in d:\\aihero\\course\\.venv\\lib\\site-packages (from griffe>=1.3.2->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.4.6)\n",
      "Collecting aiohttp (from huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading aiohttp-3.12.15-cp313-cp313-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: executing>=2.0.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.2.1)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http<1.38.0,>=1.35.0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading opentelemetry_instrumentation-0.58b0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting opentelemetry-sdk<1.38.0,>=1.35.0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in d:\\aihero\\course\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-http<1.38.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.70.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0 (from opentelemetry-exporter-otlp-proto-http<1.38.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.37.0 (from opentelemetry-exporter-otlp-proto-http<1.38.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api>=1.28.0->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.28.0->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.58b0 (from opentelemetry-sdk<1.38.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading opentelemetry_instrumentation_httpx-0.58b0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (4.25.1)\n",
      "Collecting pydantic-settings>=2.5.2 (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading pydantic_settings-2.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pywin32>=310 in d:\\aihero\\course\\.venv\\lib\\site-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (311)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting uvicorn>=0.31.1 (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading uvicorn-0.37.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\aihero\\course\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\aihero\\course\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.27.1)\n",
      "Collecting eval-type-backport>=0.2.0 (from mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting invoke<3.0.0,>=2.2.0 (from mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading invoke-2.2.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation>=0.41b0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading wrapt-1.17.3-cp313-cp313-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting opentelemetry-util-http==0.58b0 (from opentelemetry-instrumentation-httpx>=0.42b0->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading opentelemetry_util_http-0.58b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: wcwidth in d:\\aihero\\course\\.venv\\lib\\site-packages (from prompt-toolkit>=3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.2.14)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-settings>=2.5.2->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.1.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting click>=7.0 (from uvicorn>=0.31.1->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading frozenlist-1.7.0-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading multidict-6.6.4-cp313-cp313-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading propcache-0.3.2-cp313-cp313-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai)\n",
      "  Downloading yarl-1.20.1-cp313-cp313-win_amd64.whl.metadata (76 kB)\n",
      "Downloading pydantic_ai-1.0.10-py3-none-any.whl (11 kB)\n",
      "Downloading pydantic_ai_slim-1.0.10-py3-none-any.whl (333 kB)\n",
      "Downloading pydantic_evals-1.0.10-py3-none-any.whl (54 kB)\n",
      "Downloading pydantic_graph-1.0.10-py3-none-any.whl (27 kB)\n",
      "Downloading temporalio-1.17.0-cp39-abi3-win_amd64.whl (13.2 MB)\n",
      "   ---------------------------------------- 0.0/13.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/13.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.2 MB 1.7 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.8/13.2 MB 1.5 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.0/13.2 MB 1.6 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.6/13.2 MB 1.5 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.8/13.2 MB 1.5 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 2.1/13.2 MB 1.5 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 2.6/13.2 MB 1.5 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.9/13.2 MB 1.6 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 3.1/13.2 MB 1.6 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.7/13.2 MB 1.6 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.9/13.2 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 4.2/13.2 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 4.5/13.2 MB 1.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.7/13.2 MB 1.5 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 5.0/13.2 MB 1.5 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 5.5/13.2 MB 1.5 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 5.8/13.2 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 6.0/13.2 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.3/13.2 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.6/13.2 MB 1.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.8/13.2 MB 1.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.8/13.2 MB 1.5 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 7.1/13.2 MB 1.4 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.3/13.2 MB 1.4 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.3/13.2 MB 1.4 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.3/13.2 MB 1.4 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 7.6/13.2 MB 1.3 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 7.9/13.2 MB 1.3 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 7.9/13.2 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 8.1/13.2 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 8.4/13.2 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 8.7/13.2 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.9/13.2 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 9.2/13.2 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 9.4/13.2 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 9.4/13.2 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 9.7/13.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 10.2/13.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 10.5/13.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 10.5/13.2 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 10.7/13.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.3/13.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.5/13.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.8/13.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 12.1/13.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.6/13.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.8/13.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.2/13.2 MB 1.2 MB/s  0:00:10\n",
      "Downloading nexus_rpc-1.1.0-py3-none-any.whl (27 kB)\n",
      "Downloading ag_ui_protocol-0.1.9-py3-none-any.whl (7.1 kB)\n",
      "Downloading anthropic-0.68.1-py3-none-any.whl (325 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading jiter-0.11.0-cp313-cp313-win_amd64.whl (202 kB)\n",
      "Downloading argcomplete-3.6.2-py3-none-any.whl (43 kB)\n",
      "Downloading boto3-1.40.40-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.40.40-py3-none-any.whl (14.0 MB)\n",
      "   ---------------------------------------- 0.0/14.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/14.0 MB 1.5 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 1.0/14.0 MB 1.7 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.6/14.0 MB 1.8 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.8/14.0 MB 1.9 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 2.1/14.0 MB 1.9 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.6/14.0 MB 1.8 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 3.1/14.0 MB 1.8 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 3.4/14.0 MB 1.9 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.9/14.0 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 4.5/14.0 MB 1.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 5.0/14.0 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 5.5/14.0 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 6.0/14.0 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 6.6/14.0 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 7.3/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 8.1/14.0 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.9/14.0 MB 2.3 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 9.7/14.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.0/14.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.7/14.0 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 11.3/14.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 12.1/14.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.6/14.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.8/14.0 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.4/14.0 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.6/14.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.9/14.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.0/14.0 MB 2.3 MB/s  0:00:06\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
      "Downloading cohere-5.18.0-py3-none-any.whl (295 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading fastavro-1.12.0-cp313-cp313-win_amd64.whl (445 kB)\n",
      "Downloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
      "Downloading genai_prices-0.0.27-py3-none-any.whl (48 kB)\n",
      "Downloading google_genai-1.39.0-py3-none-any.whl (244 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading websockets-15.0.1-cp313-cp313-win_amd64.whl (176 kB)\n",
      "Downloading griffe-1.14.0-py3-none-any.whl (144 kB)\n",
      "Downloading groq-0.31.1-py3-none-any.whl (134 kB)\n",
      "Downloading logfire-4.10.0-py3-none-any.whl (223 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.37.0-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
      "Downloading opentelemetry_api-1.37.0-py3-none-any.whl (65 kB)\n",
      "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl (207 kB)\n",
      "Downloading logfire_api-4.10.0-py3-none-any.whl (92 kB)\n",
      "Downloading mcp-1.15.0-py3-none-any.whl (166 kB)\n",
      "Downloading mistralai-1.9.10-py3-none-any.whl (440 kB)\n",
      "Downloading invoke-2.2.0-py3-none-any.whl (160 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading openai-1.109.1-py3-none-any.whl (948 kB)\n",
      "   ---------------------------------------- 0.0/948.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/948.6 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/948.6 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 524.3/948.6 kB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 786.4/948.6 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 948.6/948.6 kB 1.1 MB/s  0:00:00\n",
      "Downloading opentelemetry_instrumentation-0.58b0-py3-none-any.whl (33 kB)\n",
      "Downloading wrapt-1.17.3-cp313-cp313-win_amd64.whl (38 kB)\n",
      "Downloading opentelemetry_instrumentation_httpx-0.58b0-py3-none-any.whl (15 kB)\n",
      "Downloading opentelemetry_util_http-0.58b0-py3-none-any.whl (7.7 kB)\n",
      "Downloading pydantic_settings-2.11.0-py3-none-any.whl (48 kB)\n",
      "Downloading pyperclip-1.11.0-py3-none-any.whl (11 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading sse_starlette-3.0.2-py3-none-any.whl (11 kB)\n",
      "Downloading starlette-0.48.0-py3-none-any.whl (73 kB)\n",
      "Downloading types_protobuf-6.32.1.20250918-py3-none-any.whl (77 kB)\n",
      "Downloading uvicorn-0.37.0-py3-none-any.whl (67 kB)\n",
      "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading aiohttp-3.12.15-cp313-cp313-win_amd64.whl (449 kB)\n",
      "Downloading multidict-6.6.4-cp313-cp313-win_amd64.whl (45 kB)\n",
      "Downloading yarl-1.20.1-cp313-cp313-win_amd64.whl (86 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Downloading propcache-0.3.2-cp313-cp313-win_amd64.whl (40 kB)\n",
      "Installing collected packages: pyperclip, zipp, wrapt, websockets, types-requests, types-protobuf, tenacity, python-multipart, propcache, opentelemetry-util-http, opentelemetry-proto, nexus-rpc, multidict, mdurl, logfire-api, jmespath, jiter, invoke, httpx-sse, griffe, frozenlist, fastavro, eval-type-backport, docstring-parser, distro, click, argcomplete, aiohappyeyeballs, yarl, uvicorn, temporalio, starlette, sse-starlette, opentelemetry-exporter-otlp-proto-common, markdown-it-py, importlib-metadata, botocore, aiosignal, s3transfer, rich, pydantic-settings, pydantic-graph, opentelemetry-api, openai, mistralai, groq, google-genai, genai-prices, anthropic, aiohttp, ag-ui-protocol, pydantic-ai-slim, opentelemetry-semantic-conventions, mcp, cohere, boto3, pydantic-evals, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-httpx, opentelemetry-exporter-otlp-proto-http, logfire, pydantic-ai\n",
      "\n",
      "    ---------------------------------------  1/63 [zipp]\n",
      "   - --------------------------------------  2/63 [wrapt]\n",
      "   - --------------------------------------  2/63 [wrapt]\n",
      "   - --------------------------------------  3/63 [websockets]\n",
      "   - --------------------------------------  3/63 [websockets]\n",
      "   - --------------------------------------  3/63 [websockets]\n",
      "   - --------------------------------------  3/63 [websockets]\n",
      "   - --------------------------------------  3/63 [websockets]\n",
      "   - --------------------------------------  3/63 [websockets]\n",
      "   - --------------------------------------  3/63 [websockets]\n",
      "   - --------------------------------------  3/63 [websockets]\n",
      "   - --------------------------------------  3/63 [websockets]\n",
      "   - --------------------------------------  3/63 [websockets]\n",
      "   --- ------------------------------------  6/63 [tenacity]\n",
      "   --- ------------------------------------  6/63 [tenacity]\n",
      "   --- ------------------------------------  6/63 [tenacity]\n",
      "   ---- -----------------------------------  7/63 [python-multipart]\n",
      "   ----- ----------------------------------  8/63 [propcache]\n",
      "   ------ --------------------------------- 10/63 [opentelemetry-proto]\n",
      "   ------ --------------------------------- 10/63 [opentelemetry-proto]\n",
      "   ------ --------------------------------- 10/63 [opentelemetry-proto]\n",
      "   ------ --------------------------------- 10/63 [opentelemetry-proto]\n",
      "   ------ --------------------------------- 11/63 [nexus-rpc]\n",
      "   ------ --------------------------------- 11/63 [nexus-rpc]\n",
      "   ------- -------------------------------- 12/63 [multidict]\n",
      "   -------- ------------------------------- 13/63 [mdurl]\n",
      "   -------- ------------------------------- 14/63 [logfire-api]\n",
      "   --------- ------------------------------ 15/63 [jmespath]\n",
      "   ---------- ----------------------------- 16/63 [jiter]\n",
      "   ---------- ----------------------------- 17/63 [invoke]\n",
      "   ---------- ----------------------------- 17/63 [invoke]\n",
      "   ---------- ----------------------------- 17/63 [invoke]\n",
      "   ---------- ----------------------------- 17/63 [invoke]\n",
      "   ---------- ----------------------------- 17/63 [invoke]\n",
      "   ---------- ----------------------------- 17/63 [invoke]\n",
      "   ---------- ----------------------------- 17/63 [invoke]\n",
      "   ---------- ----------------------------- 17/63 [invoke]\n",
      "   ---------- ----------------------------- 17/63 [invoke]\n",
      "   ---------- ----------------------------- 17/63 [invoke]\n",
      "   ----------- ---------------------------- 18/63 [httpx-sse]\n",
      "   ------------ --------------------------- 19/63 [griffe]\n",
      "   ------------ --------------------------- 19/63 [griffe]\n",
      "   ------------ --------------------------- 19/63 [griffe]\n",
      "   ------------ --------------------------- 19/63 [griffe]\n",
      "   ------------ --------------------------- 19/63 [griffe]\n",
      "   ------------ --------------------------- 19/63 [griffe]\n",
      "   ------------ --------------------------- 19/63 [griffe]\n",
      "   ------------ --------------------------- 19/63 [griffe]\n",
      "   ------------ --------------------------- 19/63 [griffe]\n",
      "   ------------- -------------------------- 21/63 [fastavro]\n",
      "   ------------- -------------------------- 21/63 [fastavro]\n",
      "   ------------- -------------------------- 21/63 [fastavro]\n",
      "   ------------- -------------------------- 21/63 [fastavro]\n",
      "   ------------- -------------------------- 21/63 [fastavro]\n",
      "   ------------- -------------------------- 21/63 [fastavro]\n",
      "   ------------- -------------------------- 21/63 [fastavro]\n",
      "   -------------- ------------------------- 23/63 [docstring-parser]\n",
      "   -------------- ------------------------- 23/63 [docstring-parser]\n",
      "   -------------- ------------------------- 23/63 [docstring-parser]\n",
      "   --------------- ------------------------ 24/63 [distro]\n",
      "   --------------- ------------------------ 24/63 [distro]\n",
      "   --------------- ------------------------ 25/63 [click]\n",
      "   --------------- ------------------------ 25/63 [click]\n",
      "   --------------- ------------------------ 25/63 [click]\n",
      "   --------------- ------------------------ 25/63 [click]\n",
      "   ---------------- ----------------------- 26/63 [argcomplete]\n",
      "   ---------------- ----------------------- 26/63 [argcomplete]\n",
      "   ---------------- ----------------------- 26/63 [argcomplete]\n",
      "   ---------------- ----------------------- 26/63 [argcomplete]\n",
      "   ---------------- ----------------------- 26/63 [argcomplete]\n",
      "   ----------------- ---------------------- 27/63 [aiohappyeyeballs]\n",
      "   ----------------- ---------------------- 28/63 [yarl]\n",
      "   ----------------- ---------------------- 28/63 [yarl]\n",
      "   ------------------ --------------------- 29/63 [uvicorn]\n",
      "   ------------------ --------------------- 29/63 [uvicorn]\n",
      "   ------------------ --------------------- 29/63 [uvicorn]\n",
      "   ------------------ --------------------- 29/63 [uvicorn]\n",
      "   ------------------ --------------------- 29/63 [uvicorn]\n",
      "   ------------------ --------------------- 29/63 [uvicorn]\n",
      "   ------------------ --------------------- 29/63 [uvicorn]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 30/63 [temporalio]\n",
      "   ------------------- -------------------- 31/63 [starlette]\n",
      "   ------------------- -------------------- 31/63 [starlette]\n",
      "   ------------------- -------------------- 31/63 [starlette]\n",
      "   ------------------- -------------------- 31/63 [starlette]\n",
      "   ------------------- -------------------- 31/63 [starlette]\n",
      "   ------------------- -------------------- 31/63 [starlette]\n",
      "   ------------------- -------------------- 31/63 [starlette]\n",
      "   -------------- ------------ 33/63 [opentelemetry-exporter-otlp-proto-common]\n",
      "   -------------- ------------ 33/63 [opentelemetry-exporter-otlp-proto-common]\n",
      "   --------------------- ------------------ 34/63 [markdown-it-py]\n",
      "   --------------------- ------------------ 34/63 [markdown-it-py]\n",
      "   --------------------- ------------------ 34/63 [markdown-it-py]\n",
      "   --------------------- ------------------ 34/63 [markdown-it-py]\n",
      "   --------------------- ------------------ 34/63 [markdown-it-py]\n",
      "   --------------------- ------------------ 34/63 [markdown-it-py]\n",
      "   --------------------- ------------------ 34/63 [markdown-it-py]\n",
      "   --------------------- ------------------ 34/63 [markdown-it-py]\n",
      "   --------------------- ------------------ 34/63 [markdown-it-py]\n",
      "   --------------------- ------------------ 34/63 [markdown-it-py]\n",
      "   --------------------- ------------------ 34/63 [markdown-it-py]\n",
      "   --------------------- ------------------ 34/63 [markdown-it-py]\n",
      "   ---------------------- ----------------- 35/63 [importlib-metadata]\n",
      "   ---------------------- ----------------- 35/63 [importlib-metadata]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ---------------------- ----------------- 36/63 [botocore]\n",
      "   ------------------------ --------------- 38/63 [s3transfer]\n",
      "   ------------------------ --------------- 38/63 [s3transfer]\n",
      "   ------------------------ --------------- 38/63 [s3transfer]\n",
      "   ------------------------ --------------- 38/63 [s3transfer]\n",
      "   ------------------------ --------------- 39/63 [rich]\n",
      "   ------------------------ --------------- 39/63 [rich]\n",
      "   ------------------------ --------------- 39/63 [rich]\n",
      "   ------------------------ --------------- 39/63 [rich]\n",
      "   ------------------------ --------------- 39/63 [rich]\n",
      "   ------------------------ --------------- 39/63 [rich]\n",
      "   ------------------------ --------------- 39/63 [rich]\n",
      "   ------------------------ --------------- 39/63 [rich]\n",
      "   ------------------------ --------------- 39/63 [rich]\n",
      "   ------------------------ --------------- 39/63 [rich]\n",
      "   ------------------------ --------------- 39/63 [rich]\n",
      "   ------------------------ --------------- 39/63 [rich]\n",
      "   ------------------------ --------------- 39/63 [rich]\n",
      "   ------------------------- -------------- 40/63 [pydantic-settings]\n",
      "   ------------------------- -------------- 40/63 [pydantic-settings]\n",
      "   ------------------------- -------------- 40/63 [pydantic-settings]\n",
      "   ------------------------- -------------- 40/63 [pydantic-settings]\n",
      "   -------------------------- ------------- 41/63 [pydantic-graph]\n",
      "   -------------------------- ------------- 41/63 [pydantic-graph]\n",
      "   -------------------------- ------------- 42/63 [opentelemetry-api]\n",
      "   -------------------------- ------------- 42/63 [opentelemetry-api]\n",
      "   -------------------------- ------------- 42/63 [opentelemetry-api]\n",
      "   -------------------------- ------------- 42/63 [opentelemetry-api]\n",
      "   -------------------------- ------------- 42/63 [opentelemetry-api]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 43/63 [openai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   --------------------------- ------------ 44/63 [mistralai]\n",
      "   ---------------------------- ----------- 45/63 [groq]\n",
      "   ---------------------------- ----------- 45/63 [groq]\n",
      "   ---------------------------- ----------- 45/63 [groq]\n",
      "   ---------------------------- ----------- 45/63 [groq]\n",
      "   ---------------------------- ----------- 45/63 [groq]\n",
      "   ---------------------------- ----------- 45/63 [groq]\n",
      "   ---------------------------- ----------- 45/63 [groq]\n",
      "   ---------------------------- ----------- 45/63 [groq]\n",
      "   ---------------------------- ----------- 45/63 [groq]\n",
      "   ---------------------------- ----------- 45/63 [groq]\n",
      "   ---------------------------- ----------- 45/63 [groq]\n",
      "   ---------------------------- ----------- 45/63 [groq]\n",
      "   ----------------------------- ---------- 46/63 [google-genai]\n",
      "   ----------------------------- ---------- 46/63 [google-genai]\n",
      "   ----------------------------- ---------- 46/63 [google-genai]\n",
      "   ----------------------------- ---------- 46/63 [google-genai]\n",
      "   ----------------------------- ---------- 46/63 [google-genai]\n",
      "   ----------------------------- ---------- 46/63 [google-genai]\n",
      "   ----------------------------- ---------- 46/63 [google-genai]\n",
      "   ----------------------------- ---------- 46/63 [google-genai]\n",
      "   ----------------------------- ---------- 46/63 [google-genai]\n",
      "   ----------------------------- ---------- 46/63 [google-genai]\n",
      "   ----------------------------- ---------- 46/63 [google-genai]\n",
      "   ----------------------------- ---------- 47/63 [genai-prices]\n",
      "   ----------------------------- ---------- 47/63 [genai-prices]\n",
      "   ----------------------------- ---------- 47/63 [genai-prices]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------ --------- 48/63 [anthropic]\n",
      "   ------------------------------- -------- 49/63 [aiohttp]\n",
      "   ------------------------------- -------- 49/63 [aiohttp]\n",
      "   ------------------------------- -------- 49/63 [aiohttp]\n",
      "   ------------------------------- -------- 49/63 [aiohttp]\n",
      "   ------------------------------- -------- 49/63 [aiohttp]\n",
      "   ------------------------------- -------- 49/63 [aiohttp]\n",
      "   ------------------------------- -------- 49/63 [aiohttp]\n",
      "   ------------------------------- -------- 49/63 [aiohttp]\n",
      "   ------------------------------- -------- 49/63 [aiohttp]\n",
      "   ------------------------------- -------- 49/63 [aiohttp]\n",
      "   ------------------------------- -------- 49/63 [aiohttp]\n",
      "   ------------------------------- -------- 49/63 [aiohttp]\n",
      "   ------------------------------- -------- 50/63 [ag-ui-protocol]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   -------------------------------- ------- 51/63 [pydantic-ai-slim]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 52/63 [opentelemetry-semantic-conventions]\n",
      "   --------------------------------- ------ 53/63 [mcp]\n",
      "   --------------------------------- ------ 53/63 [mcp]\n",
      "   --------------------------------- ------ 53/63 [mcp]\n",
      "   --------------------------------- ------ 53/63 [mcp]\n",
      "   --------------------------------- ------ 53/63 [mcp]\n",
      "   --------------------------------- ------ 53/63 [mcp]\n",
      "   --------------------------------- ------ 53/63 [mcp]\n",
      "   --------------------------------- ------ 53/63 [mcp]\n",
      "   --------------------------------- ------ 53/63 [mcp]\n",
      "   --------------------------------- ------ 53/63 [mcp]\n",
      "   --------------------------------- ------ 53/63 [mcp]\n",
      "   --------------------------------- ------ 53/63 [mcp]\n",
      "   --------------------------------- ------ 53/63 [mcp]\n",
      "   --------------------------------- ------ 53/63 [mcp]\n",
      "   --------------------------------- ------ 53/63 [mcp]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 54/63 [cohere]\n",
      "   ---------------------------------- ----- 55/63 [boto3]\n",
      "   ---------------------------------- ----- 55/63 [boto3]\n",
      "   ---------------------------------- ----- 55/63 [boto3]\n",
      "   ---------------------------------- ----- 55/63 [boto3]\n",
      "   ---------------------------------- ----- 55/63 [boto3]\n",
      "   ---------------------------------- ----- 55/63 [boto3]\n",
      "   ---------------------------------- ----- 55/63 [boto3]\n",
      "   ----------------------------------- ---- 56/63 [pydantic-evals]\n",
      "   ----------------------------------- ---- 56/63 [pydantic-evals]\n",
      "   ----------------------------------- ---- 56/63 [pydantic-evals]\n",
      "   ------------------------------------ --- 57/63 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 57/63 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 57/63 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 57/63 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 57/63 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 57/63 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 57/63 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 57/63 [opentelemetry-sdk]\n",
      "   ---------------------------------- --- 58/63 [opentelemetry-instrumentation]\n",
      "   ---------------------------------- --- 58/63 [opentelemetry-instrumentation]\n",
      "   ---------------------------------- --- 58/63 [opentelemetry-instrumentation]\n",
      "   ---------------------------------- --- 58/63 [opentelemetry-instrumentation]\n",
      "   ---------------------------------- --- 58/63 [opentelemetry-instrumentation]\n",
      "   --------------------------- - 60/63 [opentelemetry-exporter-otlp-proto-http]\n",
      "   --------------------------- - 60/63 [opentelemetry-exporter-otlp-proto-http]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   -------------------------------------- - 61/63 [logfire]\n",
      "   ---------------------------------------  62/63 [pydantic-ai]\n",
      "   ---------------------------------------- 63/63 [pydantic-ai]\n",
      "\n",
      "Successfully installed ag-ui-protocol-0.1.9 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 anthropic-0.68.1 argcomplete-3.6.2 boto3-1.40.40 botocore-1.40.40 click-8.3.0 cohere-5.18.0 distro-1.9.0 docstring-parser-0.17.0 eval-type-backport-0.2.2 fastavro-1.12.0 frozenlist-1.7.0 genai-prices-0.0.27 google-genai-1.39.0 griffe-1.14.0 groq-0.31.1 httpx-sse-0.4.0 importlib-metadata-8.7.0 invoke-2.2.0 jiter-0.11.0 jmespath-1.0.1 logfire-4.10.0 logfire-api-4.10.0 markdown-it-py-4.0.0 mcp-1.15.0 mdurl-0.1.2 mistralai-1.9.10 multidict-6.6.4 nexus-rpc-1.1.0 openai-1.109.1 opentelemetry-api-1.37.0 opentelemetry-exporter-otlp-proto-common-1.37.0 opentelemetry-exporter-otlp-proto-http-1.37.0 opentelemetry-instrumentation-0.58b0 opentelemetry-instrumentation-httpx-0.58b0 opentelemetry-proto-1.37.0 opentelemetry-sdk-1.37.0 opentelemetry-semantic-conventions-0.58b0 opentelemetry-util-http-0.58b0 propcache-0.3.2 pydantic-ai-1.0.10 pydantic-ai-slim-1.0.10 pydantic-evals-1.0.10 pydantic-graph-1.0.10 pydantic-settings-2.11.0 pyperclip-1.11.0 python-multipart-0.0.20 rich-14.1.0 s3transfer-0.14.0 sse-starlette-3.0.2 starlette-0.48.0 temporalio-1.17.0 tenacity-9.1.2 types-protobuf-6.32.1.20250918 types-requests-2.32.4.20250913 uvicorn-0.37.0 websockets-15.0.1 wrapt-1.17.3 yarl-1.20.1 zipp-3.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pydantic-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065a82b",
   "metadata": {},
   "source": [
    "For Pydantic AI (and for other agents libraries), we don't need to describe the function in the JSON format like we did with the plain OpenAI API. The libraries take care of it.\n",
    "\n",
    "But we do need to add docstrings and type hints to our function. Here's our hybrid search function with proper typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab955f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def hybrid_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a hybrid search combining text and vector search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of search results combining text and vector search.\n",
    "    \"\"\"\n",
    "    return hybrid_results(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db6a0e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def hybrid_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a hybrid search combining text and vector search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of search results combining text and vector search.\n",
    "    \"\"\"\n",
    "    return hybrid_results(query, num_results=5)\n",
    "\n",
    "# Configure Gemini without built-in tools (manual function calling)\n",
    "import google.generativeai as genai\n",
    "\n",
    "# API key is already configured\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\n",
    "You have access to a hybrid_search function that can search the course FAQ database using hybrid search (text + vector).\n",
    "\n",
    "When a user asks a question, if you need to search for information, respond with a function call in this exact format:\n",
    "FUNCTION_CALL: hybrid_search(query=\"the search query here\")\n",
    "\n",
    "After receiving search results, use that information to provide a helpful answer.\n",
    "\n",
    "If you can answer without searching, just provide the answer directly.\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-2.0-flash',\n",
    "    system_instruction=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dcc684f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\nPlease retry in 17.540056756s. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 200\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 17\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mWhat are the main topics covered in Day 1 of the course?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m chat = model.start_chat()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Check if the model wants to call a function\u001b[39;00m\n\u001b[32m      8\u001b[39m response_text = response.text.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\aihero\\course\\.venv\\Lib\\site-packages\\google\\generativeai\\generative_models.py:578\u001b[39m, in \u001b[36mChatSession.send_message\u001b[39m\u001b[34m(self, content, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generation_config.get(\u001b[33m\"\u001b[39m\u001b[33mcandidate_count\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m    574\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mInvalid configuration: The chat functionality does not support `candidate_count` greater than 1.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m578\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m    \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools_lib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[38;5;28mself\u001b[39m._check_response(response=response, stream=stream)\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.enable_automatic_function_calling \u001b[38;5;129;01mand\u001b[39;00m tools_lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\aihero\\course\\.venv\\Lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[39m, in \u001b[36mGenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[39m\n\u001b[32m    329\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_iterator(iterator)\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_response(response)\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\aihero\\course\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\aihero\\course\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\aihero\\course\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\aihero\\course\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\aihero\\course\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\aihero\\course\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\aihero\\course\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\aihero\\course\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200\nPlease retry in 17.540056756s. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 200\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 17\n}\n]"
     ]
    }
   ],
   "source": [
    "question = \"What are the main topics covered in Day 1 of the course?\"\n",
    "\n",
    "chat = model.start_chat()\n",
    "\n",
    "response = chat.send_message(question)\n",
    "\n",
    "# Check if the model wants to call a function\n",
    "response_text = response.text.strip()\n",
    "\n",
    "if response_text.startswith(\"FUNCTION_CALL:\"):\n",
    "    # Parse the function call\n",
    "    func_call = response_text.replace(\"FUNCTION_CALL:\", \"\").strip()\n",
    "    if func_call.startswith(\"hybrid_search(\"):\n",
    "        # Extract the query\n",
    "        import re\n",
    "        query_match = re.search(r'query=\"([^\"]*)\"', func_call)\n",
    "        if query_match:\n",
    "            query = query_match.group(1)\n",
    "            print(f\"Model called hybrid_search with query: {query}\")\n",
    "            \n",
    "            # Execute the function\n",
    "            search_results = hybrid_search(query)\n",
    "            print(f\"Search results: {search_results}\")\n",
    "            \n",
    "            # Send results back to model\n",
    "            follow_up = f\"Search results for '{query}': {search_results}\\n\\nPlease provide a helpful answer based on this information.\"\n",
    "            final_response = chat.send_message(follow_up)\n",
    "            print(\"Agent response:\")\n",
    "            print(final_response.text)\n",
    "        else:\n",
    "            print(\"Could not parse query from function call\")\n",
    "    else:\n",
    "        print(f\"Unknown function call: {func_call}\")\n",
    "else:\n",
    "    # Direct response\n",
    "    print(\"Agent response:\")\n",
    "    print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c34f33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history:\n"
     ]
    }
   ],
   "source": [
    "# The chat history shows the conversation flow\n",
    "print(\"Chat history:\")\n",
    "for message in chat.history:\n",
    "    print(f\"Role: {message.role}\")\n",
    "    if message.parts:\n",
    "        for part in message.parts:\n",
    "            if hasattr(part, 'text') and part.text:\n",
    "                print(f\"Text: {part.text[:200]}...\")\n",
    "            elif hasattr(part, 'function_call') and part.function_call:\n",
    "                print(f\"Function call: {part.function_call.name} with args {part.function_call.args}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a3d24a",
   "metadata": {},
   "source": [
    "Pydantic AI and other frameworks handle all the complexity of function calling for us. We don't need to manually parse responses, handle tool calls, or manage conversation history. This makes our code cleaner and less error-prone.\n",
    "\n",
    "We implemented an agent. Great! But how good is it? Is the prompt we came up good? What's better for our agent, text search, vector search or hybrid? Tomorrow we will be able to answer these questions: we will learn how to use AI to evaluate our agent.\n",
    "\n",
    "**MAKE SURE THAT YOU USE GEMINI NOT OPENAI BECAUSE I AM USING GEMINI API KEY AND MAKE SURE THAT THE WHOLE CODE IS SYNCHRONIZED AND WORKS EFFICIENTLY**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
