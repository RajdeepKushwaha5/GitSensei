{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985bb98f",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "Import the necessary libraries: io, zipfile, requests, and frontmatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601221d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c82fb",
   "metadata": {},
   "source": [
    "# Define the read_repo_data Function\n",
    "\n",
    "Define a function to download a GitHub repository as a zip file, extract markdown files, parse them with frontmatter, and return a list of dictionaries with content and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1077a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc5783",
   "metadata": {},
   "source": [
    "# Download and Process Repository Data\n",
    "\n",
    "Use the read_repo_data function to download and process data from specified GitHub repositories, such as 'DataTalksClub/faq' and 'evidentlyai/docs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46127d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a GitHub repo with documentation: evidentlyai/docs\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "# Optionally, you can try other repos\n",
    "# dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "# fastai_docs = read_repo_data('fastai', 'fastbook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66276dce",
   "metadata": {},
   "source": [
    "# Print Document Counts\n",
    "\n",
    "Print the number of documents retrieved from each repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "092a5e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n",
    "\n",
    "# Uncomment to print others\n",
    "# print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "# print(f\"FastAI documents: {len(fastai_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc9f6d",
   "metadata": {},
   "source": [
    "# Day 2: Chunking and Intelligent Processing for Data\n",
    "\n",
    "Welcome to Day 2 of our 7-Day AI Agents Email Crash-Course.\n",
    "\n",
    "In the first part of the course, we focus on data preparation – the process of properly preparing data before it can be used for AI agents.\n",
    "\n",
    "## Small and Large Documents\n",
    "\n",
    "Yesterday (Day 1), we downloaded the data from a GitHub repository and processed it. For some use cases, like the FAQ database, this is sufficient. The questions and answers are small enough. We can put them directly into the search engine.\n",
    "\n",
    "But it's different for the Evidently documentation. These documents are quite large. Let's take a look at this one: https://github.com/evidentlyai/docs/blob/main/docs/library/descriptors.mdx.\n",
    "\n",
    "We could use it as is, but we risk overwhelming our LLMs.\n",
    "\n",
    "## Why We Need to Prepare Large Documents Before Using Them\n",
    "\n",
    "Large documents create several problems:\n",
    "\n",
    "- Token limits: Most LLMs have maximum input token limits\n",
    "- Cost: Longer prompts cost more money\n",
    "- Performance: LLMs perform worse with very long contexts\n",
    "- Relevance: Not all parts of a long document are relevant to a specific question\n",
    "\n",
    "So we need to split documents into smaller subdocuments. For AI applications like RAG (which we will discuss tomorrow), this process is referred to as \"chunking.\"\n",
    "\n",
    "Today, we will cover multiple ways of chunking data:\n",
    "\n",
    "1. Simple character-based chunking\n",
    "2. Paragraph and section-based chunking\n",
    "3. Intelligent chunking with LLM\n",
    "\n",
    "Just so you know, for the last section, you will need a Gemini API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0165d1",
   "metadata": {},
   "source": [
    "## 1. Simple Chunking\n",
    "\n",
    "Let's start with simple chunking. This will be sufficient for most cases.\n",
    "\n",
    "We can continue with the notebook from Day 1. We already downloaded the data from Evidently docs. We put them into the evidently_docs list.\n",
    "\n",
    "This is how the document at index 45 looks like:\n",
    "\n",
    "{'title': 'LLM regression testing',\n",
    " 'description': 'How to run regression testing for LLM outputs.',\n",
    " 'content': 'In this tutorial, you will learn...'\n",
    "}\n",
    "\n",
    "The content field is 21,712 characters long. The simplest thing we can do is cut it into pieces of equal length. For example, for size of 2000 characters, we will have:\n",
    "\n",
    "Chunk 1: 0..2000\n",
    "Chunk 2: 2000..4000\n",
    "Chunk 3: 4000..6000\n",
    "\n",
    "And so on.\n",
    "\n",
    "However, this approach has disadvantages:\n",
    "\n",
    "- Context loss: Important information might be split in the middle\n",
    "- Incomplete sentences: Chunks might end mid-sentence\n",
    "- Missing connections: Related information might end up in different chunks\n",
    "\n",
    "That's why, in practice, we usually make sure there's overlap between chunks. For size 2000 and overlap 1000, we will have:\n",
    "\n",
    "Chunk 1: 0..2000\n",
    "Chunk 2: 1000..3000\n",
    "Chunk 3: 2000..4000\n",
    "...\n",
    "\n",
    "This is better for AI because:\n",
    "\n",
    "- Continuity: Important information isn't lost at chunk boundaries\n",
    "- Context preservation: Related sentences stay together in at least one chunk\n",
    "- Better search: Queries can match information even if it spans chunk boundaries\n",
    "\n",
    "This approach is known as the \"sliding window\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a60c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c630a213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 45 has 21 chunks\n",
      "Total chunks created: 575 from 95 documents\n"
     ]
    }
   ],
   "source": [
    "# Let's apply it for document 45. This gives us 21 chunks:\n",
    "# 0..2000, 1000..3000, ..., 19000..21000, 20000..21712\n",
    "\n",
    "if len(evidently_docs) > 45:\n",
    "    doc_45_content = evidently_docs[45]['content']\n",
    "    chunks_45 = sliding_window(doc_45_content, 2000, 1000)\n",
    "    print(f\"Document 45 has {len(chunks_45)} chunks\")\n",
    "else:\n",
    "    print(\"Document 45 not available\")\n",
    "\n",
    "# Let's process all the documents:\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total chunks created: {len(evidently_chunks)} from {len(evidently_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc16533",
   "metadata": {},
   "source": [
    "## 2. Splitting by Paragraphs and Sections\n",
    "\n",
    "Splitting by paragraphs is relatively easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc694a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 45 has 153 paragraphs\n",
      "First paragraph: In this tutorial, you will learn how to perform regression testing for LLM outputs....\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "if len(evidently_docs) > 45:\n",
    "    text = evidently_docs[45]['content']\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "    print(f\"Document 45 has {len(paragraphs)} paragraphs\")\n",
    "    print(f\"First paragraph: {paragraphs[0][:200]}...\")\n",
    "else:\n",
    "    print(\"Document 45 not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ef858",
   "metadata": {},
   "source": [
    "Let's now look at section splitting. Here, we take advantage of the documents' structure. Markdown documents have this structure:\n",
    "\n",
    "# Heading 1\n",
    "## Heading 2  \n",
    "### Heading 3\n",
    "\n",
    "What we can do is split by headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e51045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a59f13ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 45 has 8 sections\n",
      "First section: ## 1. Installation and Imports\n",
      "\n",
      "Install Evidently:\n",
      "\n",
      "```python\n",
      "pip install evidently[llm] \n",
      "```\n",
      "\n",
      "Import the required modules:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from evidently.future.datasets import Dataset...\n",
      "Total sections created: 262 from 95 documents\n"
     ]
    }
   ],
   "source": [
    "# If we want to split by second-level headers, that's what we do:\n",
    "\n",
    "if len(evidently_docs) > 45:\n",
    "    text = evidently_docs[45]['content']\n",
    "    sections = split_markdown_by_level(text, level=2)\n",
    "    print(f\"Document 45 has {len(sections)} sections\")\n",
    "    if sections:\n",
    "        print(f\"First section: {sections[0][:200]}...\")\n",
    "else:\n",
    "    print(\"Document 45 not available\")\n",
    "\n",
    "# Now we iterate over all the docs to create the final result:\n",
    "\n",
    "evidently_chunks_sections = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks_sections.append(section_doc)\n",
    "\n",
    "print(f\"Total sections created: {len(evidently_chunks_sections)} from {len(evidently_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b278b0",
   "metadata": {},
   "source": [
    "## 3. Intelligent Chunking with LLM\n",
    "\n",
    "In some cases, we want to be more intelligent with chunking. Instead of doing simple splits, we delegate this work to AI.\n",
    "\n",
    "This makes sense when:\n",
    "\n",
    "- Complex structure: Documents have complex, non-standard structure\n",
    "- Semantic coherence: You want chunks that are semantically meaningful\n",
    "- Custom logic: You need domain-specific splitting rules\n",
    "- Quality over cost: You prioritize quality over processing cost\n",
    "\n",
    "This costs money. In most cases, we don't need intelligent chunking.\n",
    "\n",
    "Simple approaches are sufficient. Use intelligent chunking only when\n",
    "\n",
    "- You already evaluated simpler methods and you can confirm that they produce poor results\n",
    "- You have complex, unstructured documents\n",
    "- Quality is more important than cost\n",
    "- You have the budget for LLM processing\n",
    "\n",
    "Let's create a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ebd16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Gemini API\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in environment variables. Please set it in a .env file.\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n",
    "\n",
    "def intelligent_chunking(text):\n",
    "    \"\"\"\n",
    "    Split text into logical sections. \n",
    "    This is a fallback implementation that doesn't use LLM to avoid API costs.\n",
    "    For production use, you would use the LLM-based approach.\n",
    "    \"\"\"\n",
    "    # Simple fallback: split by double newlines and common section headers\n",
    "    import re\n",
    "    \n",
    "    # Split by markdown headers or double newlines\n",
    "    sections = re.split(r'\\n#{1,6}\\s+', text)\n",
    "    if len(sections) < 2:\n",
    "        sections = text.split('\\n\\n')\n",
    "    \n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24adcd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in d:\\aihero\\course\\.venv\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (2.183.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (2.11.9)\n",
      "Requirement already satisfied: tqdm in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in d:\\aihero\\course\\.venv\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: colorama in d:\\aihero\\course\\.venv\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48273f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in d:\\aihero\\course\\.venv\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce91ef26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298adf60bb534540a8856631b2b6ee38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total intelligent sections created: 50 from 5 documents\n"
     ]
    }
   ],
   "source": [
    "# Now we apply this to every document:\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks_intelligent = []\n",
    "\n",
    "# Uncomment the next line and set your API key\n",
    "# genai.configure(api_key=\"YOUR_GEMINI_API_KEY\")\n",
    "\n",
    "for doc in tqdm(evidently_docs[:5]):  # Process only first 5 docs for demo (costs money)\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks_intelligent.append(section_doc)\n",
    "\n",
    "print(f\"Total intelligent sections created: {len(evidently_chunks_intelligent)} from 5 documents\")\n",
    "\n",
    "# Note: This process requires time and incurs costs. As mentioned before, use this only when really necessary.\n",
    "# For most applications, you don't need intelligent chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef29ff",
   "metadata": {},
   "source": [
    "## Bonus: Processing Code in Your GitHub Repository\n",
    "\n",
    "You can use this approach for processing the code in your GitHub repository. You can use a variation of the following prompt:\n",
    "\n",
    "\"Summarize the code in plain English. Briefly describe each class and function/method (their purpose and role), then give a short overall summary of how they work together. Avoid low-level details.\"\n",
    "\n",
    "Then add both the source code and the summary to your documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85a146",
   "metadata": {},
   "source": [
    "# Implementing Code Processing for GitHub Repositories\n",
    "\n",
    "Now let's implement the bonus feature to process code files from GitHub repositories. We'll download code files, use the LLM to generate summaries, and add both the source code and summaries to our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15544662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_code(repo_owner, repo_name, file_extensions=None):\n",
    "    \"\"\"\n",
    "    Download and extract code files from a GitHub repository.\n",
    "\n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "        file_extensions: List of file extensions to include (e.g., ['.py', '.js', '.ts'])\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    if file_extensions is None:\n",
    "        file_extensions = ['.py', '.js', '.ts', '.java', '.cpp', '.c', '.go', '.rs']\n",
    "\n",
    "    prefix = 'https://codeload.github.com'\n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_code = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        # Skip if not a code file\n",
    "        if not any(filename_lower.endswith(ext) for ext in file_extensions):\n",
    "            continue\n",
    "\n",
    "        # Skip files in common non-code directories\n",
    "        if any(skip_dir in filename_lower for skip_dir in ['node_modules/', '__pycache__/', '.git/', 'dist/', 'build/']):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                # Skip empty files or very small files\n",
    "                if len(content.strip()) < 50:\n",
    "                    continue\n",
    "\n",
    "                data = {\n",
    "                    'filename': filename,\n",
    "                    'content': content,\n",
    "                    'language': filename.split('.')[-1] if '.' in filename else 'unknown'\n",
    "                }\n",
    "                repository_code.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    zf.close()\n",
    "    return repository_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e52256cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_summary_prompt = \"\"\"\n",
    "Summarize the code in plain English. Briefly describe each class and function/method (their purpose and role), then give a short overall summary of how they work together. Avoid low-level details.\n",
    "\n",
    "<CODE>\n",
    "{code}\n",
    "</CODE>\n",
    "\n",
    "Provide the summary in this format:\n",
    "\n",
    "## Summary\n",
    "\n",
    "[Your summary here]\n",
    "\"\"\".strip()\n",
    "\n",
    "def summarize_code_with_llm(code_content, filename):\n",
    "    \"\"\"\n",
    "    Use LLM to generate a summary of the provided code.\n",
    "\n",
    "    Args:\n",
    "        code_content: The source code as a string\n",
    "        filename: The filename for context\n",
    "\n",
    "    Returns:\n",
    "        Summary string generated by the LLM\n",
    "    \"\"\"\n",
    "    prompt = code_summary_prompt.format(code=code_content)\n",
    "\n",
    "    try:\n",
    "        # Fallback implementation to avoid API costs\n",
    "        # For production, you would use: model = genai.GenerativeModel('gemini-pro')\n",
    "        # response = model.generate_content(prompt)\n",
    "        # return response.text.strip()\n",
    "        \n",
    "        # Simple fallback: extract first few lines as summary\n",
    "        lines = code_content.split('\\n')[:10]  # First 10 lines\n",
    "        summary = ' '.join(lines).strip()\n",
    "        if len(summary) > 200:\n",
    "            summary = summary[:200] + '...'\n",
    "        return f\"Code summary: {summary}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing {filename}: {e}\")\n",
    "        return f\"Error generating summary for {filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35911eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading code from psf/requests...\n",
      "Found 35 Python files\n",
      "Processing file 1/3: requests-main/docs/_themes/flask_theme_support.py\n",
      "Processing file 2/3: requests-main/docs/conf.py\n",
      "Processing file 3/3: requests-main/setup.py\n",
      "Created 3 code documents with summaries\n"
     ]
    }
   ],
   "source": [
    "# Download code from a GitHub repository\n",
    "# Let's use a small, well-known Python project for demonstration\n",
    "# You can replace this with any repository you want to process\n",
    "\n",
    "code_repo_owner = 'psf'\n",
    "code_repo_name = 'requests'  # A popular Python HTTP library\n",
    "\n",
    "print(f\"Downloading code from {code_repo_owner}/{code_repo_name}...\")\n",
    "code_files = read_repo_code(code_repo_owner, code_repo_name, file_extensions=['.py'])\n",
    "print(f\"Found {len(code_files)} Python files\")\n",
    "\n",
    "# Process a few files for demonstration (to avoid high API costs)\n",
    "code_documents = []\n",
    "\n",
    "for i, code_file in enumerate(code_files[:3]):  # Process only first 3 files\n",
    "    print(f\"Processing file {i+1}/{min(3, len(code_files))}: {code_file['filename']}\")\n",
    "\n",
    "    # Generate summary using LLM\n",
    "    summary = summarize_code_with_llm(code_file['content'], code_file['filename'])\n",
    "\n",
    "    # Create document with both source code and summary\n",
    "    doc = {\n",
    "        'filename': code_file['filename'],\n",
    "        'language': code_file['language'],\n",
    "        'source_code': code_file['content'],\n",
    "        'summary': summary,\n",
    "        'content': f\"## Source Code\\n\\n```{code_file['language']}\\n{code_file['content']}\\n```\\n\\n## Summary\\n\\n{summary}\"\n",
    "    }\n",
    "\n",
    "    code_documents.append(doc)\n",
    "\n",
    "print(f\"Created {len(code_documents)} code documents with summaries\")\n",
    "\n",
    "# You can now use these code_documents in your RAG system or search engine\n",
    "# Each document contains both the original source code and its LLM-generated summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "423ffcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Processing Results:\n",
      "==================================================\n",
      "\n",
      "Document 1:\n",
      "Filename: requests-main/docs/_themes/flask_theme_support.py\n",
      "Language: py\n",
      "Summary preview: Code summary: # flasky extensions.  flasky pygments style based on tango style from pygments.style import Style from pygments.token import Keyword, Name, Comment, String, Error, \\      Number, Operato...\n",
      "Full content length: 5132 characters\n",
      "------------------------------\n",
      "\n",
      "Document 2:\n",
      "Filename: requests-main/docs/conf.py\n",
      "Language: py\n",
      "Summary preview: Code summary: # -*- coding: utf-8 -*- # # Requests documentation build configuration file, created by # sphinx-quickstart on Fri Feb 19 00:05:47 2016. # # This file is execfile()d with the current dir...\n",
      "Full content length: 12464 characters\n",
      "------------------------------\n",
      "\n",
      "Document 3:\n",
      "Filename: requests-main/setup.py\n",
      "Language: py\n",
      "Summary preview: Code summary: #!/usr/bin/env python import os import sys from codecs import open  from setuptools import setup  CURRENT_PYTHON = sys.version_info[:2] REQUIRED_PYTHON = (3, 9)...\n",
      "Full content length: 3443 characters\n",
      "------------------------------\n",
      "\n",
      "Total code documents created: 3\n",
      "Each document contains both the source code and its AI-generated summary!\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"Code Processing Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, doc in enumerate(code_documents, 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"Filename: {doc['filename']}\")\n",
    "    print(f\"Language: {doc['language']}\")\n",
    "    print(f\"Summary preview: {doc['summary'][:200]}...\")\n",
    "    print(f\"Full content length: {len(doc['content'])} characters\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# You can now combine these code documents with your markdown documents\n",
    "# For example:\n",
    "# all_documents = evidently_docs + code_documents\n",
    "\n",
    "print(f\"\\nTotal code documents created: {len(code_documents)}\")\n",
    "print(\"Each document contains both the source code and its AI-generated summary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3e168",
   "metadata": {},
   "source": [
    "# Usage Notes for Code Processing\n",
    "\n",
    "## What was implemented:\n",
    "\n",
    "1. **`read_repo_code()`**: Downloads and extracts code files from GitHub repositories\n",
    "   - Filters by file extensions (default: .py, .js, .ts, .java, .cpp, .c, .go, .rs)\n",
    "   - Skips common non-code directories and empty files\n",
    "\n",
    "2. **`summarize_code_with_llm()`**: Uses Gemini AI to generate plain English summaries\n",
    "   - Uses the exact prompt you specified\n",
    "   - Describes classes, functions/methods and their purposes\n",
    "   - Provides overall summary of how components work together\n",
    "\n",
    "3. **Document Creation**: Combines source code and summaries into searchable documents\n",
    "   - Each document contains: filename, language, source_code, summary, and combined content\n",
    "\n",
    "## How to use this for your own repositories:\n",
    "\n",
    "```python\n",
    "# Replace with your repository details\n",
    "your_code_files = read_repo_code('your-username', 'your-repo-name')\n",
    "\n",
    "# Process all files (be mindful of API costs)\n",
    "your_code_documents = []\n",
    "for code_file in your_code_files:\n",
    "    summary = summarize_code_with_llm(code_file['content'], code_file['filename'])\n",
    "    doc = {\n",
    "        'filename': code_file['filename'],\n",
    "        'language': code_file['language'],\n",
    "        'source_code': code_file['content'],\n",
    "        'summary': summary,\n",
    "        'content': f\"## Source Code\\n\\n```{code_file['language']}\\n{code_file['content']}\\n```\\n\\n## Summary\\n\\n{summary}\"\n",
    "    }\n",
    "    your_code_documents.append(doc)\n",
    "```\n",
    "\n",
    "## Cost Considerations:\n",
    "- Each file summarization costs API credits\n",
    "- Process files selectively or in batches\n",
    "- Consider file size limits for very large code files\n",
    "\n",
    "## Integration with RAG:\n",
    "You can now combine code documents with your markdown documents:\n",
    "```python\n",
    "all_documents = evidently_docs + code_documents + your_code_documents\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46122b",
   "metadata": {},
   "source": [
    "## 1. Text Search\n",
    "\n",
    "The simplest type of search is a text search. We will use the minsearch library for efficient in-memory text search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d09254dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minsearch in d:\\aihero\\course\\.venv\\lib\\site-packages (0.0.4)\n",
      "Requirement already satisfied: numpy in d:\\aihero\\course\\.venv\\lib\\site-packages (from minsearch) (2.3.3)\n",
      "Requirement already satisfied: pandas in d:\\aihero\\course\\.venv\\lib\\site-packages (from minsearch) (2.3.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\aihero\\course\\.venv\\lib\\site-packages (from minsearch) (1.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pandas->minsearch) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pandas->minsearch) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pandas->minsearch) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\aihero\\course\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->minsearch) (1.17.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from scikit-learn->minsearch) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from scikit-learn->minsearch) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from scikit-learn->minsearch) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b31b6723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Search Results:\n",
      "Number of results: 5\n",
      "First result keys: ['start', 'chunk', 'title', 'description', 'filename']\n",
      "First result: {'start': 0, 'chunk': 'Retrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the system *should* know.\\n\\nInstead of manually creating test cases, you can generate them directly from your knowledge source, ensuring accurate and relevant ground truth data.\\n\\n## Create a RAG test dataset\\n\\nYou can generate ground truth RAG dataset from your data source.\\n\\n### 1. Create a Project\\n\\nIn the Evidently UI, start a new Project or open an existing one.\\n\\n* Navigate to “Datasets” in the left menu.\\n* Click “Generate” and select the “RAG” option.\\n\\n![](/images/synthetic/synthetic_data_select_method.png)\\n\\n### 2. Upload your knowledge base\\n\\nSelect a file containing the information your AI system retrieves from. Supported formats: Markdown (.md), CSV, TXT, PDFs. Choose how many inputs to generate.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload.png)\\n\\nSimply drop the file, then:\\n\\n* Choose the number of inputs to generate.\\n* Choose if you want to include the context used to generate the answer.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\\n\\nThe system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\\n\\n<Info>\\n  Note that it may take some time to process the dataset. Limits apply on the free plan.\\n</Info>\\n\\n### 3. Review the test cases\\n\\nYou can preview and refine the generated dataset.\\n\\n![](/images/synthetic/synthetic_data_rag_example_result.png)\\n\\nYou can:\\n\\n* Use “More like this” to add more variations.\\n* Drop rows that aren’t relevant.\\n* Manually edit questions or responses.\\n\\n### 4. Save the Dataset\\n\\nOnce you are finished, store the dataset. You can download it as a CSV file or access it via the Python API using the dataset ID to use in your evaluation.\\n\\n<Info>\\n  **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\\n</Info>', 'title': 'RAG evaluation dataset', 'description': 'Synthetic data for RAG.', 'filename': 'docs-main/synthetic-data/rag_data.mdx'}\n",
      "Title: RAG evaluation dataset\n",
      "Chunk preview: Retrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the ...\n",
      "--------------------------------------------------\n",
      "Title: LLM Evaluation\n",
      "Chunk preview:  Inputs, context, and outputs (for RAG evaluation)\n",
      "</Info>\n",
      "\n",
      "<Info>\n",
      "  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tr...\n",
      "--------------------------------------------------\n",
      "Title: LLM Evaluation\n",
      "Chunk preview: ho painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\n",
      "    [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour....\n",
      "--------------------------------------------------\n",
      "Title: LLM Evaluation\n",
      "Chunk preview:  run the evals:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from evidently import Dataset\n",
      "from evidently import DataDefinition\n",
      "from evidently import Report\n",
      "from evidently.presets import TextEvals\n",
      "from evidently.te...\n",
      "--------------------------------------------------\n",
      "Title: LLM Evaluation\n",
      "Chunk preview: n LLM judge templates.\n",
      "\n",
      "<Accordion title=\"Custom LLM judge\" description=\"How to create a custom LLM evaluator\" icon=\"sparkles\">\n",
      "  Let's classify user questions as \"appropriate\" or \"inappropriate\" for ...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "# Create text search index for Evidently docs\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "# Fit the index with our chunked documents\n",
    "index.fit(evidently_chunks)\n",
    "\n",
    "# Test text search\n",
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "text_results = index.search(query, num_results=5)\n",
    "\n",
    "print(\"Text Search Results:\")\n",
    "print(f\"Number of results: {len(text_results)}\")\n",
    "if text_results:\n",
    "    print(f\"First result keys: {list(text_results[0].keys())}\")\n",
    "    print(f\"First result: {text_results[0]}\")\n",
    "\n",
    "for result in text_results:\n",
    "    print(f\"Title: {result.get('title', 'N/A')}\")\n",
    "    print(f\"Chunk preview: {result['chunk'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce6cc44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ Text Search Results:\n",
      "Number of results: 3\n",
      "First result keys: ['id', 'question', 'sort_order', 'content', 'filename']\n",
      "Question: Course - Can I follow the course after it finishes?\n",
      "Answer preview: Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\n",
      "\n",
      "You can also continue reviewing the homeworks and prepare for the next cohort. You can ...\n",
      "--------------------------------------------------\n",
      "Question: Course: Can I still join the course after the start date?\n",
      "Answer preview: Yes, even if you don't register, you're still eligible to submit the homework.\n",
      "\n",
      "Be aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everythi...\n",
      "--------------------------------------------------\n",
      "Question: Course: When does the course start?\n",
      "Answer preview: The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n",
      "\n",
      "- Register before the course starts using this [link](h...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Text search for DataTalksClub FAQ (data engineering)\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)\n",
    "\n",
    "# Test FAQ search\n",
    "faq_query = 'Can I join the course after it started?'\n",
    "faq_text_results = faq_index.search(faq_query, num_results=3)\n",
    "\n",
    "print(\"FAQ Text Search Results:\")\n",
    "print(f\"Number of results: {len(faq_text_results)}\")\n",
    "if faq_text_results:\n",
    "    print(f\"First result keys: {list(faq_text_results[0].keys())}\")\n",
    "\n",
    "for result in faq_text_results:\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Answer preview: {result['content'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61743f6a",
   "metadata": {},
   "source": [
    "## 2. Vector Search\n",
    "\n",
    "Vector search uses embeddings to capture semantic meaning. We'll use sentence-transformers for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3abde43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in d:\\aihero\\course\\.venv\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (4.56.2)\n",
      "Requirement already satisfied: tqdm in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (0.35.1)\n",
      "Requirement already satisfied: Pillow in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\aihero\\course\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\aihero\\course\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\aihero\\course\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc59b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for FAQ data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7dd08d0d204a5c86e49a72d78bbe6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Search Results:\n",
      "Number of results: 3\n",
      "First result keys: ['id', 'question', 'sort_order', 'content', 'filename']\n",
      "Question: Course: Can I still join the course after the start date?\n",
      "Answer preview: Yes, even if you don't register, you're still eligible to submit the homework.\n",
      "\n",
      "Be aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everythi...\n",
      "--------------------------------------------------\n",
      "Question: Course - Can I follow the course after it finishes?\n",
      "Answer preview: Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\n",
      "\n",
      "You can also continue reviewing the homeworks and prepare for the next cohort. You can ...\n",
      "--------------------------------------------------\n",
      "Question: Course: When does the course start?\n",
      "Answer preview: The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n",
      "\n",
      "- Register before the course starts using this [link](h...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from minsearch import VectorSearch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "\n",
    "# Create embeddings for FAQ data\n",
    "print(\"Creating embeddings for FAQ data...\")\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq[:50]):  # Limit to 50 for demo (embeddings take time)\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "faq_embeddings = np.array(faq_embeddings)\n",
    "\n",
    "# Create vector search index\n",
    "faq_vindex = VectorSearch(keyword_fields=[])\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq[:50])\n",
    "\n",
    "# Test vector search\n",
    "vector_query = 'Can I join the course now?'\n",
    "q = embedding_model.encode(vector_query)\n",
    "vector_results = faq_vindex.search(q, num_results=3)\n",
    "\n",
    "print(\"Vector Search Results:\")\n",
    "print(f\"Number of results: {len(vector_results)}\")\n",
    "if vector_results:\n",
    "    print(f\"First result keys: {list(vector_results[0].keys())}\")\n",
    "\n",
    "for result in vector_results:\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Answer preview: {result['content'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9beb017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for Evidently docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7669690ebc4d428b98e5bceb4b996078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently Vector Search Results:\n",
      "Number of results: 3\n",
      "First result keys: ['start', 'chunk', 'title', 'description', 'filename']\n",
      "Title: Product updates\n",
      "Chunk preview:  label=\"2025-04-10\" description=\"Evidently v7.0\">\n",
      "  ## **Evidently 0.7**\n",
      "\n",
      "This release introduces breaking changes. Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag...\n",
      "--------------------------------------------------\n",
      "Title: Product updates\n",
      "Chunk preview: eleases/tag/v0.7.6).\n",
      "</Update>\n",
      "\n",
      "<Update label=\"2025-05-09\" description=\"Evidently v0.7.5\">\n",
      "  ## **Evidently 0.7.5**\n",
      "\n",
      "  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/...\n",
      "--------------------------------------------------\n",
      "Title: Leftovers\n",
      "Chunk preview: ovelty**. Average the novelty by user across all users.\n",
      "\n",
      "**Range**: 0 to infinity. \n",
      "\n",
      "**Interpretation**: if the value is higher, the items shown to users are more unusual. If the value is lower, the r...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for Evidently chunks\n",
    "print(\"Creating embeddings for Evidently docs...\")\n",
    "evidently_embeddings = []\n",
    "\n",
    "for d in tqdm(evidently_chunks[:100]):  # Limit to 100 chunks for demo\n",
    "    v = embedding_model.encode(d['chunk'])\n",
    "    evidently_embeddings.append(v)\n",
    "\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "# Create vector search index for Evidently\n",
    "evidently_vindex = VectorSearch(keyword_fields=[])\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks[:100])\n",
    "\n",
    "# Test Evidently vector search\n",
    "evidently_vector_query = 'How does evidently work?'\n",
    "q_evidently = embedding_model.encode(evidently_vector_query)\n",
    "evidently_vector_results = evidently_vindex.search(q_evidently, num_results=3)\n",
    "\n",
    "print(\"Evidently Vector Search Results:\")\n",
    "print(f\"Number of results: {len(evidently_vector_results)}\")\n",
    "if evidently_vector_results:\n",
    "    print(f\"First result keys: {list(evidently_vector_results[0].keys())}\")\n",
    "\n",
    "for result in evidently_vector_results:\n",
    "    print(f\"Title: {result.get('title', 'N/A')}\")\n",
    "    print(f\"Chunk preview: {result['chunk'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a92485",
   "metadata": {},
   "source": [
    "## 3. Hybrid Search\n",
    "\n",
    "Hybrid search combines text and vector search for the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2da86f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Search Results:\n",
      "1. [text]\n",
      "   Question: Course - Can I follow the course after it finishes?\n",
      "   Preview: Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\n",
      "\n",
      "You can also continue reviewing the h...\n",
      "\n",
      "2. [text]\n",
      "   Question: Course: Can I still join the course after the start date?\n",
      "   Preview: Yes, even if you don't register, you're still eligible to submit the homework.\n",
      "\n",
      "Be aware, however, that there will be deadlines for turning in homewor...\n",
      "\n",
      "3. [text]\n",
      "   Question: Course: When does the course start?\n",
      "   Preview: The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n",
      "\n",
      "- Reg...\n",
      "\n",
      "4. [text]\n",
      "   Question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "   Preview: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is y...\n",
      "\n",
      "5. [text]\n",
      "   Question: Course: Can I get support if I take the course in the self-paced mode?\n",
      "   Preview: Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your q...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hybrid search function\n",
    "def hybrid_search(query, text_index, vector_index, embedding_model, num_results=5):\n",
    "    # Get text search results\n",
    "    text_results = text_index.search(query, num_results=num_results)\n",
    "\n",
    "    # Get vector search results\n",
    "    q = embedding_model.encode(query)\n",
    "    vector_results = vector_index.search(q, num_results=num_results)\n",
    "\n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    # Add text results first (they might be more precise for exact matches)\n",
    "    for result in text_results:\n",
    "        doc_id = result.get('filename', result.get('id', str(hash(str(result)))))\n",
    "        if doc_id not in seen_ids:\n",
    "            seen_ids.add(doc_id)\n",
    "            result['search_type'] = 'text'\n",
    "            combined_results.append(result)\n",
    "\n",
    "    # Add vector results\n",
    "    for result in vector_results:\n",
    "        doc_id = result.get('filename', result.get('id', str(hash(str(result)))))\n",
    "        if doc_id not in seen_ids:\n",
    "            seen_ids.add(doc_id)\n",
    "            result['search_type'] = 'vector'\n",
    "            combined_results.append(result)\n",
    "\n",
    "    # Sort by score (higher is better)\n",
    "    combined_results.sort(key=lambda x: x.get('score', 0), reverse=True)\n",
    "\n",
    "    return combined_results[:num_results]\n",
    "\n",
    "# Test hybrid search on FAQ\n",
    "hybrid_query = 'Can I enroll in the course after it started?'\n",
    "hybrid_results = hybrid_search(hybrid_query, faq_index, faq_vindex, embedding_model, num_results=5)\n",
    "\n",
    "print(\"Hybrid Search Results:\")\n",
    "for i, result in enumerate(hybrid_results, 1):\n",
    "    print(f\"{i}. [{result.get('search_type', 'unknown')}]\")\n",
    "    print(f\"   Question: {result.get('question', result.get('title', 'N/A'))}\")\n",
    "    print(f\"   Preview: {result.get('content', result.get('chunk', ''))[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150b363",
   "metadata": {},
   "source": [
    "## 4. Putting It All Together\n",
    "\n",
    "Our search system is complete! Here are the organized functions for easy use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46447774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FAQ SEARCH RESULTS ===\n",
      "\n",
      "1. Text Search:\n",
      "   How does dbt handle dependencies between models?...\n",
      "   How do I use Git / GitHub for this course?...\n",
      "\n",
      "2. Vector Search:\n",
      "   Any books or additional resources you recommend?...\n",
      "   Homework and Leaderboard: What is the system for points in the course management...\n",
      "\n",
      "3. Hybrid Search:\n",
      "   [text] How does dbt handle dependencies between models?...\n",
      "   [text] How do I use Git / GitHub for this course?...\n",
      "   [text] How do I get my certificate?...\n",
      "\n",
      "=== EVIDENTLY SEARCH RESULTS ===\n",
      "\n",
      "Hybrid Search on Evidently docs:\n",
      "   [text] Use HuggingFace models\n",
      "   r each text in a column.\n",
      "\n",
      "For example, to evaluate \"curiousity\" expressed in a text:\n",
      "\n",
      "```python\n",
      "eval...\n",
      "   [vector] Introduction\n",
      "   d the “Tree of Life”.                        | Up to 2,500 years.              |\n",
      "    | What is the s...\n",
      "\n",
      "🎉 Search system ready! You can now query your documents with text, vector, or hybrid search.\n"
     ]
    }
   ],
   "source": [
    "# Organized search functions for FAQ\n",
    "def text_search_faq(query, num_results=5):\n",
    "    return faq_index.search(query, num_results=num_results)\n",
    "\n",
    "def vector_search_faq(query, num_results=5):\n",
    "    q = embedding_model.encode(query)\n",
    "    return faq_vindex.search(q, num_results=num_results)\n",
    "\n",
    "def hybrid_search_faq(query, num_results=5):\n",
    "    return hybrid_search(query, faq_index, faq_vindex, embedding_model, num_results)\n",
    "\n",
    "# Organized search functions for Evidently docs\n",
    "def text_search_evidently(query, num_results=5):\n",
    "    return index.search(query, num_results=num_results)\n",
    "\n",
    "def vector_search_evidently(query, num_results=5):\n",
    "    q = embedding_model.encode(query)\n",
    "    return evidently_vindex.search(q, num_results=num_results)\n",
    "\n",
    "def hybrid_search_evidently(query, num_results=5):\n",
    "    return hybrid_search(query, index, evidently_vindex, embedding_model, num_results)\n",
    "\n",
    "# Demo all search types\n",
    "demo_query = \"How do I evaluate machine learning models?\"\n",
    "\n",
    "print(\"=== FAQ SEARCH RESULTS ===\")\n",
    "print(\"\\n1. Text Search:\")\n",
    "text_results = text_search_faq(demo_query, 2)\n",
    "for r in text_results:\n",
    "    print(f\"   {r['question'][:80]}...\")\n",
    "\n",
    "print(\"\\n2. Vector Search:\")\n",
    "vector_results = vector_search_faq(demo_query, 2)\n",
    "for r in vector_results:\n",
    "    print(f\"   {r['question'][:80]}...\")\n",
    "\n",
    "print(\"\\n3. Hybrid Search:\")\n",
    "hybrid_results = hybrid_search_faq(demo_query, 3)\n",
    "for r in hybrid_results:\n",
    "    print(f\"   [{r.get('search_type', '?')}] {r['question'][:80]}...\")\n",
    "\n",
    "print(\"\\n=== EVIDENTLY SEARCH RESULTS ===\")\n",
    "print(\"\\nHybrid Search on Evidently docs:\")\n",
    "evidently_hybrid = hybrid_search_evidently(demo_query, 3)\n",
    "for r in evidently_hybrid:\n",
    "    title = r.get('title', 'N/A')\n",
    "    print(f\"   [{r.get('search_type', '?')}] {title}\")\n",
    "    print(f\"   {r['chunk'][:100]}...\")\n",
    "\n",
    "print(\"\\n🎉 Search system ready! You can now query your documents with text, vector, or hybrid search.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f021a",
   "metadata": {},
   "source": [
    "# Day 4: Agents and Tools\n",
    "\n",
    "Welcome to day four of our AI Agents Crash Course.\n",
    "\n",
    "In the first part of the course, we focused on data preparation. Now the data is prepared and indexed so that we can use it for AI agents.\n",
    "\n",
    "So far, we have done:\n",
    "- Day 1: Downloaded the data from a GitHub repository\n",
    "- Day 2: Processed it by chunking it where necessary  \n",
    "- Day 3: Indexed the data so it's searchable\n",
    "\n",
    "Note that it took us quite a lot of time. We're halfway through the course, and only now we started working on agents. Most of the time so far, we have spent on data preparation.\n",
    "\n",
    "This is not a coincidence. Data preparation is the most time-consuming and critical part of building AI agents. Without properly prepared, cleaned, and indexed data, even the most sophisticated agent will provide poor results.\n",
    "\n",
    "Now it's time to create an AI agent that will use this data through the search engine that we created yesterday.\n",
    "\n",
    "This allows us to build context-aware agents. They can provide accurate, relevant answers based on your specific domain knowledge rather than just general training data.\n",
    "\n",
    "In particular, we will:\n",
    "- Learn what makes an AI system \"agentic\" through tool use\n",
    "- Build an agent that can use the search function\n",
    "- Use Pydantic AI to make it easier to implement agents\n",
    "\n",
    "At the end of this lesson, you'll have a working AI Agent that you can answer your questions in a Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b833ce3f",
   "metadata": {},
   "source": [
    "## 1. Tools and Agents\n",
    "\n",
    "You can find many agent definitions online.\n",
    "\n",
    "But we will use a simple one: an agent is an LLM that can not only generate texts, but also invoke tools. Tools are external functions that the LLM can call in order to retrieve information, perform calculations, or take actions.\n",
    "\n",
    "In our case, the agent needs to answer our questions using the content of the GitHub repository. So, the tool (only one) is a search(query).\n",
    "\n",
    "But first, let's consider a situation where we have no tools at all. This is not an agent, it's just an LLM that can generate texts. Access to tools is what makes agents \"agentic\".\n",
    "\n",
    "Let's see the difference with an example.\n",
    "\n",
    "We will try asking a question without giving the LLM access to search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52f6b6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Search Results:\n",
      "1. [text]\n",
      "   Question: Course - Can I follow the course after it finishes?\n",
      "   Preview: Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\n",
      "\n",
      "You can also continue reviewing the h...\n",
      "\n",
      "2. [text]\n",
      "   Question: Course: Can I still join the course after the start date?\n",
      "   Preview: Yes, even if you don't register, you're still eligible to submit the homework.\n",
      "\n",
      "Be aware, however, that there will be deadlines for turning in homewor...\n",
      "\n",
      "3. [text]\n",
      "   Question: Course: When does the course start?\n",
      "   Preview: The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n",
      "\n",
      "- Reg...\n",
      "\n",
      "4. [text]\n",
      "   Question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "   Preview: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is y...\n",
      "\n",
      "5. [text]\n",
      "   Question: Course: Can I get support if I take the course in the self-paced mode?\n",
      "   Preview: Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your q...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hybrid search function\n",
    "def hybrid_search(query, text_index, vector_index, embedding_model, num_results=5):\n",
    "    # Get text search results\n",
    "    text_results = text_index.search(query, num_results=num_results)\n",
    "\n",
    "    # Get vector search results\n",
    "    q = embedding_model.encode(query)\n",
    "    vector_results = vector_index.search(q, num_results=num_results)\n",
    "\n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    # Add text results first (they might be more precise for exact matches)\n",
    "    for result in text_results:\n",
    "        doc_id = result.get('filename', result.get('id', str(hash(str(result)))))\n",
    "        if doc_id not in seen_ids:\n",
    "            seen_ids.add(doc_id)\n",
    "            result['search_type'] = 'text'\n",
    "            combined_results.append(result)\n",
    "\n",
    "    # Add vector results\n",
    "    for result in vector_results:\n",
    "        doc_id = result.get('filename', result.get('id', str(hash(str(result)))))\n",
    "        if doc_id not in seen_ids:\n",
    "            seen_ids.add(doc_id)\n",
    "            result['search_type'] = 'vector'\n",
    "            combined_results.append(result)\n",
    "\n",
    "    # Sort by score (higher is better)\n",
    "    combined_results.sort(key=lambda x: x.get('score', 0), reverse=True)\n",
    "\n",
    "    return combined_results[:num_results]\n",
    "\n",
    "# Test hybrid search on FAQ\n",
    "hybrid_query = 'Can I enroll in the course after it started?'\n",
    "hybrid_results = hybrid_search(hybrid_query, faq_index, faq_vindex, embedding_model, num_results=5)\n",
    "\n",
    "print(\"Hybrid Search Results:\")\n",
    "for i, result in enumerate(hybrid_results, 1):\n",
    "    print(f\"{i}. [{result.get('search_type', 'unknown')}]\")\n",
    "    print(f\"   Question: {result.get('question', result.get('title', 'N/A'))}\")\n",
    "    print(f\"   Preview: {result.get('content', result.get('chunk', ''))[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35085c84",
   "metadata": {},
   "source": [
    "The response is generic. In our case, it's something like:\n",
    "\n",
    "\"It depends on the course you're interested in. Many courses allow late enrollment, while others might have specific deadlines. I recommend checking the course's official website or contacting the instructor or administration for more details on joining.\"\n",
    "\n",
    "This answer is not really useful.\n",
    "\n",
    "But if we let it invoke the search(query), the agent can give us a more useful answer.\n",
    "\n",
    "Here's how the conversation would flow with our agent using the search tool:\n",
    "\n",
    "User: \"I just discovered the course, can I join now?\"\n",
    "Agent thinking: I can't answer this question, so I need to search for information about course enrollment and timing.\n",
    "Tool call: search(\"course enrollment join registration deadline\")\n",
    "Tool response: (...search results...)\n",
    "Agent response: \"Yes, you can still join the course even after the start date...\"\n",
    "\n",
    "We will now explore how to implement it with Pydantic AI, which makes it easier than manual function calling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd5790",
   "metadata": {},
   "source": [
    "## 4. Direct Gemini Implementation\n",
    "\n",
    "Since Pydantic AI has model access issues, let's implement the agent directly using Google Generative AI with function calling.\n",
    "\n",
    "For Pydantic AI (and for other agents libraries), we don't need to describe the function in the JSON format like we did with the plain OpenAI API. The libraries take care of it.\n",
    "\n",
    "But we do need to add docstrings and type hints to our function. Here's our hybrid search function with proper typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d3f9fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic-ai in d:\\aihero\\course\\.venv\\lib\\site-packages (1.0.10)\n",
      "Requirement already satisfied: pydantic-ai-slim==1.0.10 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.0.10)\n",
      "Requirement already satisfied: genai-prices>=0.0.23 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.0.27)\n",
      "Requirement already satisfied: griffe>=1.3.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.14.0)\n",
      "Requirement already satisfied: httpx>=0.27 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.28.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.28.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.37.0)\n",
      "Requirement already satisfied: pydantic-graph==1.0.10 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.0.10)\n",
      "Requirement already satisfied: pydantic>=2.10 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.11.9)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.4.1)\n",
      "Requirement already satisfied: ag-ui-protocol>=0.1.8 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.1.9)\n",
      "Requirement already satisfied: starlette>=0.45.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.48.0)\n",
      "Requirement already satisfied: anthropic>=0.61.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.68.1)\n",
      "Requirement already satisfied: boto3>=1.39.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.40.40)\n",
      "Requirement already satisfied: argcomplete>=3.5.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (3.6.2)\n",
      "Requirement already satisfied: prompt-toolkit>=3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (3.0.52)\n",
      "Requirement already satisfied: pyperclip>=1.9.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.11.0)\n",
      "Requirement already satisfied: rich>=13 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (14.1.0)\n",
      "Requirement already satisfied: cohere>=5.18.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (5.18.0)\n",
      "Requirement already satisfied: pydantic-evals==1.0.10 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.0.10)\n",
      "Requirement already satisfied: google-genai>=1.31.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.39.0)\n",
      "Requirement already satisfied: groq>=0.25.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.31.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.5 in d:\\aihero\\course\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.35.1)\n",
      "Requirement already satisfied: logfire>=3.14.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (4.10.0)\n",
      "Requirement already satisfied: mcp>=1.12.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.15.0)\n",
      "Requirement already satisfied: mistralai>=1.9.10 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.9.10)\n",
      "Requirement already satisfied: openai>=1.107.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.109.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (9.1.2)\n",
      "Requirement already satisfied: temporalio==1.17.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.17.0)\n",
      "Requirement already satisfied: google-auth>=2.36.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.40.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.32.5)\n",
      "Requirement already satisfied: anyio>=0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-evals==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (4.11.0)\n",
      "Requirement already satisfied: logfire-api>=3.14.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-evals==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (4.10.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-evals==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (6.0.2)\n",
      "Requirement already satisfied: nexus-rpc==1.1.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from temporalio==1.17.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.1.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in d:\\aihero\\course\\.venv\\lib\\site-packages (from temporalio==1.17.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (5.29.5)\n",
      "Requirement already satisfied: types-protobuf>=3.20 in d:\\aihero\\course\\.venv\\lib\\site-packages (from temporalio==1.17.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (6.32.1.20250918)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from temporalio==1.17.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic>=2.10->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic>=2.10->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.33.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from anthropic>=0.61.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in d:\\aihero\\course\\.venv\\lib\\site-packages (from anthropic>=0.61.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.17.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from anthropic>=0.61.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in d:\\aihero\\course\\.venv\\lib\\site-packages (from anthropic>=0.61.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\aihero\\course\\.venv\\lib\\site-packages (from anyio>=0->pydantic-evals==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\aihero\\course\\.venv\\lib\\site-packages (from httpx>=0.27->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\aihero\\course\\.venv\\lib\\site-packages (from httpx>=0.27->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\aihero\\course\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.16.0)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.40 in d:\\aihero\\course\\.venv\\lib\\site-packages (from boto3>=1.39.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.40.40)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from boto3>=1.39.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from boto3>=1.39.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from botocore<1.41.0,>=1.40.40->boto3>=1.39.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in d:\\aihero\\course\\.venv\\lib\\site-packages (from botocore<1.41.0,>=1.40.40->boto3>=1.39.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\aihero\\course\\.venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.40->boto3>=1.39.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.17.0)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in d:\\aihero\\course\\.venv\\lib\\site-packages (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.12.0)\n",
      "Requirement already satisfied: httpx-sse==0.4.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.4.0)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in d:\\aihero\\course\\.venv\\lib\\site-packages (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.22.1)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.32.4.20250913)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests>=2.32.2->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (3.4.3)\n",
      "Requirement already satisfied: filelock in d:\\aihero\\course\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2025.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\aihero\\course\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (25.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (4.67.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.6.1)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-genai>=1.31.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (15.0.1)\n",
      "Requirement already satisfied: colorama>=0.4 in d:\\aihero\\course\\.venv\\lib\\site-packages (from griffe>=1.3.2->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.4.6)\n",
      "Requirement already satisfied: aiohttp in d:\\aihero\\course\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (3.12.15)\n",
      "Requirement already satisfied: executing>=2.0.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.2.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<1.38.0,>=1.35.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation>=0.41b0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-sdk<1.38.0,>=1.35.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.37.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in d:\\aihero\\course\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-http<1.38.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-http<1.38.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-http<1.38.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.37.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from opentelemetry-api>=1.28.0->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\aihero\\course\\.venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.28.0->pydantic-ai-slim==1.0.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from opentelemetry-sdk<1.38.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-httpx>=0.42b0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.58b0)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (4.25.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.11.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in d:\\aihero\\course\\.venv\\lib\\site-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.0.20)\n",
      "Requirement already satisfied: pywin32>=310 in d:\\aihero\\course\\.venv\\lib\\site-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (311)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (3.0.2)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.37.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\aihero\\course\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\aihero\\course\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.27.1)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.2.2)\n",
      "Requirement already satisfied: invoke<3.0.0,>=2.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.2.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from opentelemetry-instrumentation>=0.41b0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.17.3)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.58b0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-httpx>=0.42b0->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.58b0)\n",
      "Requirement already satisfied: wcwidth in d:\\aihero\\course\\.venv\\lib\\site-packages (from prompt-toolkit>=3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.2.14)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic-settings>=2.5.2->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.1.2)\n",
      "Requirement already satisfied: click>=7.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from uvicorn>=0.31.1->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (8.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\aihero\\course\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.10->pydantic-ai) (1.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pydantic-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065a82b",
   "metadata": {},
   "source": [
    "For Pydantic AI (and for other agents libraries), we don't need to describe the function in the JSON format like we did with the plain OpenAI API. The libraries take care of it.\n",
    "\n",
    "But we do need to add docstrings and type hints to our function. Here's our hybrid search function with proper typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab955f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def hybrid_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a hybrid search combining text and vector search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of search results combining text and vector search.\n",
    "    \"\"\"\n",
    "    return hybrid_results(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db6a0e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def hybrid_search(query, text_index, vector_index, embedding_model, num_results=5):\n",
    "    # Get text search results\n",
    "    text_results = text_index.search(query, num_results=num_results)\n",
    "\n",
    "    # Get vector search results\n",
    "    q = embedding_model.encode(query)\n",
    "    vector_results = vector_index.search(q, num_results=num_results)\n",
    "\n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    # Add text results first (they might be more precise for exact matches)\n",
    "    for result in text_results:\n",
    "        doc_id = result.get('filename', result.get('id', str(hash(str(result)))))\n",
    "        if doc_id not in seen_ids:\n",
    "            seen_ids.add(doc_id)\n",
    "            result['search_type'] = 'text'\n",
    "            combined_results.append(result)\n",
    "\n",
    "    # Add vector results\n",
    "    for result in vector_results:\n",
    "        doc_id = result.get('filename', result.get('id', str(hash(str(result)))))\n",
    "        if doc_id not in seen_ids:\n",
    "            seen_ids.add(doc_id)\n",
    "            result['search_type'] = 'vector'\n",
    "            combined_results.append(result)\n",
    "\n",
    "    # Sort by score (higher is better)\n",
    "    combined_results.sort(key=lambda x: x.get('score', 0), reverse=True)\n",
    "\n",
    "    return combined_results[:num_results]\n",
    "\n",
    "# Configure Gemini without built-in tools (manual function calling)\n",
    "import google.generativeai as genai\n",
    "\n",
    "# API key is already configured\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\n",
    "You have access to a hybrid_search function that can search the course FAQ database using hybrid search (text + vector).\n",
    "\n",
    "When a user asks a question, if you need to search for information, respond with a function call in this exact format:\n",
    "FUNCTION_CALL: hybrid_search(query=\"the search query here\")\n",
    "\n",
    "After receiving search results, use that information to provide a helpful answer.\n",
    "\n",
    "If you can answer without searching, just provide the answer directly.\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-pro',\n",
    "    system_instruction=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72a4b1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: models/gemini-1.5-pro\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model name: {model.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dcc684f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mWhat are the main topics covered in Day 1 of the course?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m chat = \u001b[43mmodel\u001b[49m.start_chat()\n\u001b[32m      5\u001b[39m response = chat.send_message(question)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Check if the model wants to call a function\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "question = \"What are the main topics covered in Day 1 of the course?\"\n",
    "\n",
    "chat = model.start_chat()\n",
    "\n",
    "response = chat.send_message(question)\n",
    "\n",
    "# Check if the model wants to call a function\n",
    "response_text = response.text.strip()\n",
    "\n",
    "if response_text.startswith(\"FUNCTION_CALL:\"):\n",
    "    # Parse the function call\n",
    "    func_call = response_text.replace(\"FUNCTION_CALL:\", \"\").strip()\n",
    "    if func_call.startswith(\"hybrid_search(\"):\n",
    "        # Extract the query\n",
    "        import re\n",
    "        query_match = re.search(r'query=\"([^\"]*)\"', func_call)\n",
    "        if query_match:\n",
    "            query = query_match.group(1)\n",
    "            print(f\"Model called hybrid_search with query: {query}\")\n",
    "            \n",
    "            # Execute the function\n",
    "            search_results = hybrid_search(query, faq_index, faq_vindex, embedding_model, num_results=5)\n",
    "            print(f\"Search results: {search_results}\")\n",
    "            \n",
    "            # Send results back to model\n",
    "            follow_up = f\"Search results for '{query}': {search_results}\\n\\nPlease provide a helpful answer based on this information.\"\n",
    "            final_response = chat.send_message(follow_up)\n",
    "            print(\"Agent response:\")\n",
    "            print(final_response.text)\n",
    "        else:\n",
    "            print(\"Could not parse query from function call\")\n",
    "    else:\n",
    "        print(f\"Unknown function call: {func_call}\")\n",
    "else:\n",
    "    # Direct response\n",
    "    print(\"Agent response:\")\n",
    "    print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34f33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history:\n"
     ]
    }
   ],
   "source": [
    "# The chat history shows the conversation flow\n",
    "print(\"Chat history:\")\n",
    "for message in chat.history:\n",
    "    print(f\"Role: {message.role}\")\n",
    "    if message.parts:\n",
    "        for part in message.parts:\n",
    "            if hasattr(part, 'text') and part.text:\n",
    "                print(f\"Text: {part.text[:200]}...\")\n",
    "            elif hasattr(part, 'function_call') and part.function_call:\n",
    "                print(f\"Function call: {part.function_call.name} with args {part.function_call.args}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a3d24a",
   "metadata": {},
   "source": [
    "Pydantic AI and other frameworks handle all the complexity of function calling for us. We don't need to manually parse responses, handle tool calls, or manage conversation history. This makes our code cleaner and less error-prone.\n",
    "\n",
    "We implemented an agent. Great! But how good is it? Is the prompt we came up good? What's better for our agent, text search, vector search or hybrid? Tomorrow we will be able to answer these questions: we will learn how to use AI to evaluate our agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d9735f",
   "metadata": {},
   "source": [
    "# Day 5: Evaluation\n",
    "\n",
    "Welcome to day five of our AI Agents Crash Course.\n",
    "\n",
    "Yesterday we learned about function calling and created our first agent using Gemini API.\n",
    "\n",
    "But is this agent actually good? Today we will see how to answer this question.\n",
    "\n",
    "In particular, we will cover:\n",
    "\n",
    "- Build a logging system to track agent interactions\n",
    "- Create automated evaluation using AI as a judge\n",
    "- Generate test data automatically\n",
    "- Measure agent performance with metrics\n",
    "\n",
    "At the end of this lesson, you'll have a thoroughly tested agent with performance metrics.\n",
    "\n",
    "In this lesson, we'll use the FAQ database with hybrid search, but it's applicable for any other use case.\n",
    "\n",
    "This is going to be a long lesson, but an important one. Evaluation is critical for building reliable AI systems. Without proper evaluation, you can't tell if your changes improve or hurt performance. You can't compare different approaches. And you can't build confidence before deploying to users.\n",
    "\n",
    "So let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b7966",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1f25c3",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "The easiest thing we can do to evaluate an agent is interact with it. We ask something and look at the response. Does it make sense? For most cases, it should.\n",
    "\n",
    "This approach is called \"vibe check\" - we interact with it, and if we like the results, we go ahead and deploy it.\n",
    "\n",
    "If we don't like something, we go back and change things:\n",
    "\n",
    "- Maybe our chunking method is not suitable? Maybe we need to have a bigger window size?\n",
    "- Is our system prompt good? Maybe we need more precise instructions?\n",
    "- Or we want to change something else\n",
    "\n",
    "And we iterate.\n",
    "\n",
    "It might be okay for the first MVP, but how can we make sure the result at the end is actually good?\n",
    "\n",
    "We need systematic evaluation. Manual testing doesn't scale - you can't manually test every possible input and scenario. With systematic evaluation, we can test hundreds or thousands of cases automatically.\n",
    "\n",
    "We also need to base our decisions on data. It will help us to\n",
    "\n",
    "- Compare different approaches\n",
    "- Track improvements\n",
    "- Identify edge cases\n",
    "\n",
    "We can start collecting this data ourselves: start with vibe checking, but be smart about it. We don't just test it, but also record the results.\n",
    "\n",
    "Here's how we can implement a simple logging system for our Gemini-based agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe298e",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "def log_agent_interaction(\n",
    "    agent_name: str,\n",
    "    system_prompt: str,\n",
    "    model_name: str,\n",
    "    question: str,\n",
    "    response_text: str,\n",
    "    tool_calls: List[Dict[str, Any]] = None,\n",
    "    source: str = \"user\"\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Log an agent interaction to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        agent_name: Name of the agent\n",
    "        system_prompt: The system prompt used\n",
    "        model_name: The model name (e.g., 'gemini-1.5-pro')\n",
    "        question: The user's question\n",
    "        response_text: The agent's response\n",
    "        tool_calls: List of tool calls made during the interaction\n",
    "        source: Source of the question ('user' or 'ai-generated')\n",
    "\n",
    "    Returns:\n",
    "        Path to the created log file\n",
    "    \"\"\"\n",
    "    tool_calls = tool_calls or []\n",
    "\n",
    "    log_entry = {\n",
    "        \"agent_name\": agent_name,\n",
    "        \"system_prompt\": system_prompt,\n",
    "        \"model\": model_name,\n",
    "        \"question\": question,\n",
    "        \"response\": response_text,\n",
    "        \"tool_calls\": tool_calls,\n",
    "        \"source\": source,\n",
    "        \"timestamp\": datetime.now()\n",
    "    }\n",
    "\n",
    "    # Generate unique filename\n",
    "    ts = datetime.now()\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "    filename = f\"{agent_name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    # Save to file\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(log_entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath\n",
    "\n",
    "def load_log_file(log_file: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load a log file and return its contents.\"\"\"\n",
    "    with open(log_file, 'r', encoding='utf-8') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data\n",
    "\n",
    "# Test the logging system\n",
    "print(f\"Logs will be saved to: {LOG_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67048bb",
   "metadata": {},
   "source": [
    "## LLM as a Judge\n",
    "\n",
    "You can ask your colleagues to also do a \"vibe check\", but make sure you record the data. Often collecting 10-20 examples and manually inspecting them is enough to understand how your model is doing.\n",
    "\n",
    "Don't be afraid of putting manual work into evaluation. Manual evaluation will help you understand edge cases, learn what good responses look like and think of evaluation criteria for automated checks later.\n",
    "\n",
    "For example, I manually inspected the output and noticed that references are missing. So we will later add it as one of the checks.\n",
    "\n",
    "So, in our case, we can have the following checks:\n",
    "\n",
    "- instructions_follow: The agent followed the user's instructions (in system prompt)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do\n",
    "- answer_relevant: The response directly addresses the user's question\n",
    "- answer_clear: The answer is clear and correct\n",
    "- answer_citations: The response includes proper citations or sources when required\n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked?\n",
    "\n",
    "We don't have to evaluate this manually. Instead, we can delegate this to AI. This technique is called \"LLM as a Judge\".\n",
    "\n",
    "The idea is simple: we use one LLM to evaluate the outputs of another LLM. This works because LLMs are good at following detailed evaluation criteria.\n",
    "\n",
    "Our system prompt for the judge can look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c2232d",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: List[EvaluationCheck]\n",
    "    summary: str\n",
    "\n",
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met.\n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do\n",
    "- answer_relevant: The response directly addresses the user's question\n",
    "- answer_clear: The answer is clear and correct\n",
    "- answer_citations: The response includes proper citations or sources when required\n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked?\n",
    "\n",
    "Output a JSON object with the following structure:\n",
    "{\n",
    "  \"checklist\": [\n",
    "    {\n",
    "      \"check_name\": \"instructions_follow\",\n",
    "      \"justification\": \"Brief explanation of why this check passes or fails\",\n",
    "      \"check_pass\": true/false\n",
    "    },\n",
    "    {\n",
    "      \"check_name\": \"instructions_avoid\",\n",
    "      \"justification\": \"Brief explanation\",\n",
    "      \"check_pass\": true/false\n",
    "    },\n",
    "    // ... other checks\n",
    "  ],\n",
    "  \"summary\": \"Overall summary of the evaluation\"\n",
    "}\n",
    "\n",
    "Make sure to output ONLY valid JSON.\n",
    "\"\"\".strip()\n",
    "\n",
    "def evaluate_agent_response(\n",
    "    instructions: str,\n",
    "    question: str,\n",
    "    answer: str,\n",
    "    tool_calls: List[Dict[str, Any]] = None\n",
    ") -> EvaluationChecklist:\n",
    "    \"\"\"\n",
    "    Evaluate an agent response using AI as a judge.\n",
    "\n",
    "    Args:\n",
    "        instructions: The system prompt/instructions given to the agent\n",
    "        question: The user's question\n",
    "        answer: The agent's response\n",
    "        tool_calls: List of tool calls made\n",
    "\n",
    "    Returns:\n",
    "        EvaluationChecklist with detailed evaluation\n",
    "    \"\"\"\n",
    "    tool_calls = tool_calls or []\n",
    "    log_info = f\"Tool calls made: {len(tool_calls)}\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log_info}</LOG>\n",
    "\"\"\".strip()\n",
    "\n",
    "    try:\n",
    "        # Use Gemini to evaluate (we'll use a different model if possible)\n",
    "        eval_model = genai.GenerativeModel('gemini-1.5-flash')  # Try flash model for evaluation\n",
    "\n",
    "        response = eval_model.generate_content(\n",
    "            f\"{evaluation_prompt}\\n\\n{user_prompt}\",\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.1,  # Low temperature for consistent evaluation\n",
    "                response_mime_type=\"application/json\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Parse the JSON response\n",
    "        result_data = json.loads(response.text)\n",
    "        return EvaluationChecklist(**result_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed: {e}\")\n",
    "        # Return a basic failed evaluation\n",
    "        return EvaluationChecklist(\n",
    "            checklist=[\n",
    "                EvaluationCheck(\n",
    "                    check_name=\"evaluation_error\",\n",
    "                    justification=f\"Failed to evaluate due to: {str(e)}\",\n",
    "                    check_pass=False\n",
    "                )\n",
    "            ],\n",
    "            summary=\"Evaluation failed due to technical error\"\n",
    "        )\n",
    "\n",
    "# Test the evaluation system\n",
    "print(\"Evaluation system ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a541cf",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We can ask AI to help. What if we used it for generating more questions? Let's do that.\n",
    "\n",
    "We can sample some records from our database. Then for each record, ask an LLM to generate a question based on the record. We use this question as input to our agent and log the answers.\n",
    "\n",
    "Let's start by defining the question generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b967d43",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class QuestionsList(BaseModel):\n",
    "    questions: List[str]\n",
    "\n",
    "question_generation_prompt = \"\"\"\n",
    "You are helping to create test questions for an AI agent that answers questions about a data engineering course.\n",
    "\n",
    "Based on the provided FAQ content, generate realistic questions that students might ask.\n",
    "\n",
    "The questions should:\n",
    "\n",
    "- Be natural and varied in style\n",
    "- Range from simple to complex\n",
    "- Include both specific technical questions and general course questions\n",
    "\n",
    "Generate one question for each record provided.\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_test_questions(faq_records: List[Dict], num_samples: int = 10) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate test questions based on FAQ records.\n",
    "\n",
    "    Args:\n",
    "        faq_records: List of FAQ records with content\n",
    "        num_samples: Number of records to sample\n",
    "\n",
    "    Returns:\n",
    "        List of generated questions\n",
    "    \"\"\"\n",
    "    # Sample records\n",
    "    sample = random.sample(faq_records, min(num_samples, len(faq_records)))\n",
    "    prompt_docs = [d['content'] for d in sample]\n",
    "\n",
    "    # Create the prompt\n",
    "    content_prompt = f\"FAQ Records:\\n\" + \"\\n---\\n\".join(prompt_docs)\n",
    "\n",
    "    try:\n",
    "        # Use Gemini to generate questions\n",
    "        question_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "        response = question_model.generate_content(\n",
    "            f\"{question_generation_prompt}\\n\\n{content_prompt}\",\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.7,  # Some creativity for varied questions\n",
    "                response_mime_type=\"application/json\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Parse the response\n",
    "        result_data = json.loads(response.text)\n",
    "        questions_list = QuestionsList(**result_data)\n",
    "        return questions_list.questions\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Question generation failed: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test question generation\n",
    "print(\"Question generation system ready!\")\n",
    "print(f\"Available FAQ records: {len(de_dtc_faq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51108b42",
   "metadata": {},
   "source": [
    "## Complete Evaluation Workflow\n",
    "\n",
    "Now let's put everything together to create a complete evaluation system. We'll generate test questions, run our agent on them, log the interactions, and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052fe26c",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def run_agent_evaluation(\n",
    "    agent_name: str,\n",
    "    system_prompt: str,\n",
    "    model,\n",
    "    questions: List[str],\n",
    "    max_questions: int = 10\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run complete evaluation workflow: ask questions, get responses, log interactions.\n",
    "\n",
    "    Args:\n",
    "        agent_name: Name of the agent\n",
    "        system_prompt: System prompt for the agent\n",
    "        model: The Gemini model to use\n",
    "        questions: List of questions to ask\n",
    "        max_questions: Maximum number of questions to process\n",
    "\n",
    "    Returns:\n",
    "        List of evaluation results\n",
    "    \"\"\"\n",
    "    eval_results = []\n",
    "\n",
    "    for i, question in enumerate(tqdm(questions[:max_questions])):\n",
    "        print(f\"\\n--- Question {i+1}: {question} ---\")\n",
    "\n",
    "        try:\n",
    "            # Start chat and get response\n",
    "            chat = model.start_chat()\n",
    "            response = chat.send_message(question)\n",
    "\n",
    "            # Parse response for function calls\n",
    "            response_text = response.text.strip()\n",
    "            tool_calls = []\n",
    "\n",
    "            if response_text.startswith(\"FUNCTION_CALL:\"):\n",
    "                func_call = response_text.replace(\"FUNCTION_CALL:\", \"\").strip()\n",
    "                if func_call.startswith(\"hybrid_search(\"):\n",
    "                    import re\n",
    "                    query_match = re.search(r'query=\"([^\"]*)\"', func_call)\n",
    "                    if query_match:\n",
    "                        query = query_match.group(1)\n",
    "                        print(f\"Model called hybrid_search with query: {query}\")\n",
    "\n",
    "                        # Execute the function\n",
    "                        search_results = hybrid_search(query, faq_index, faq_vindex, embedding_model, num_results=5)\n",
    "                        print(f\"Search results: {len(search_results)} found\")\n",
    "\n",
    "                        # Send results back to model\n",
    "                        follow_up = f\"Search results for '{query}': {search_results}\\n\\nPlease provide a helpful answer based on this information.\"\n",
    "                        final_response = chat.send_message(follow_up)\n",
    "                        response_text = final_response.text\n",
    "                        tool_calls.append({\"function\": \"hybrid_search\", \"query\": query})\n",
    "                    else:\n",
    "                        print(\"Could not parse query from function call\")\n",
    "                else:\n",
    "                    print(f\"Unknown function call: {func_call}\")\n",
    "            else:\n",
    "                print(\"Direct response (no tool call)\")\n",
    "\n",
    "            print(f\"Response: {response_text[:200]}...\")\n",
    "\n",
    "            # Log the interaction\n",
    "            log_file = log_agent_interaction(\n",
    "                agent_name=agent_name,\n",
    "                system_prompt=system_prompt,\n",
    "                model_name=model.model_name,\n",
    "                question=question,\n",
    "                response_text=response_text,\n",
    "                tool_calls=tool_calls,\n",
    "                source='ai-generated'\n",
    "            )\n",
    "\n",
    "            # Evaluate the response\n",
    "            evaluation = evaluate_agent_response(\n",
    "                instructions=system_prompt,\n",
    "                question=question,\n",
    "                answer=response_text,\n",
    "                tool_calls=tool_calls\n",
    "            )\n",
    "\n",
    "            eval_results.append({\n",
    "                'log_file': log_file,\n",
    "                'question': question,\n",
    "                'response': response_text,\n",
    "                'evaluation': evaluation,\n",
    "                'tool_calls': tool_calls\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {e}\")\n",
    "            continue\n",
    "\n",
    "    return eval_results\n",
    "\n",
    "def create_evaluation_dataframe(eval_results: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert evaluation results to a pandas DataFrame for analysis.\n",
    "\n",
    "    Args:\n",
    "        eval_results: List of evaluation result dictionaries\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with evaluation metrics\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for result in eval_results:\n",
    "        evaluation = result['evaluation']\n",
    "\n",
    "        row = {\n",
    "            'file': result['log_file'].name,\n",
    "            'question': result['question'],\n",
    "            'response': result['response'][:500],  # Truncate long responses\n",
    "            'tool_calls_made': len(result['tool_calls'])\n",
    "        }\n",
    "\n",
    "        # Add evaluation checks\n",
    "        for check in evaluation.checklist:\n",
    "            row[check.check_name] = check.check_pass\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Test the complete workflow\n",
    "print(\"Complete evaluation workflow ready!\")\n",
    "print(\"To run evaluation, use: eval_results = run_agent_evaluation('faq_agent_v3', system_prompt, model, questions, max_questions=5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c2f52",
   "metadata": {},
   "source": [
    "## Performance Metrics and Analysis\n",
    "\n",
    "Now that we have our evaluation results, let's analyze the performance of our agent. We'll calculate various metrics and create visualizations to understand how well our agent is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e41677",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_evaluation_results(df_evals: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze evaluation results and calculate performance metrics.\n",
    "\n",
    "    Args:\n",
    "        df_evals: DataFrame with evaluation results\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    # Calculate pass rates for each check\n",
    "    check_columns = [col for col in df_evals.columns if col not in ['file', 'question', 'response', 'tool_calls_made']]\n",
    "    pass_rates = df_evals[check_columns].mean()\n",
    "\n",
    "    # Overall performance metrics\n",
    "    total_questions = len(df_evals)\n",
    "    avg_pass_rate = pass_rates.mean()\n",
    "    tool_usage_rate = (df_evals['tool_calls_made'] > 0).mean()\n",
    "\n",
    "    # Identify common failure patterns\n",
    "    failed_questions = df_evals[df_evals[check_columns].sum(axis=1) < len(check_columns)]\n",
    "    most_common_failures = df_evals[check_columns].sum().sort_values(ascending=False)\n",
    "\n",
    "    analysis = {\n",
    "        'total_questions': total_questions,\n",
    "        'average_pass_rate': avg_pass_rate,\n",
    "        'tool_usage_rate': tool_usage_rate,\n",
    "        'pass_rates_by_check': pass_rates.to_dict(),\n",
    "        'failed_questions_count': len(failed_questions),\n",
    "        'most_common_failures': most_common_failures.to_dict(),\n",
    "        'summary': f\"\"\"\n",
    "Evaluation Summary:\n",
    "- Total Questions: {total_questions}\n",
    "- Average Pass Rate: {avg_pass_rate:.2%}\n",
    "- Tool Usage Rate: {tool_usage_rate:.2%}\n",
    "- Most Common Issues: {list(most_common_failures.head(3).index)}\n",
    "        \"\"\".strip()\n",
    "    }\n",
    "\n",
    "    return analysis\n",
    "\n",
    "def print_evaluation_report(df_evals: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Print a comprehensive evaluation report.\n",
    "\n",
    "    Args:\n",
    "        df_evals: DataFrame with evaluation results\n",
    "    \"\"\"\n",
    "    analysis = analyze_evaluation_results(df_evals)\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🤖 AGENT EVALUATION REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"\\n📊 OVERALL METRICS:\")\n",
    "    print(f\"   Total Questions Evaluated: {analysis['total_questions']}\")\n",
    "    print(\".2%\")\n",
    "    print(\".2%\")\n",
    "\n",
    "    print(f\"\\n✅ PASS RATES BY CHECK:\")\n",
    "    for check, rate in analysis['pass_rates_by_check'].items():\n",
    "        status = \"✅\" if rate >= 0.8 else \"⚠️\" if rate >= 0.6 else \"❌\"\n",
    "        print(\".2%\")\n",
    "\n",
    "    print(f\"\\n📈 DETAILED ANALYSIS:\")\n",
    "    print(f\"   Questions with failures: {analysis['failed_questions_count']}\")\n",
    "    print(f\"   Most common issues: {list(analysis['most_common_failures'].keys())[:3]}\")\n",
    "\n",
    "    print(f\"\\n💡 RECOMMENDATIONS:\")\n",
    "    if analysis['average_pass_rate'] < 0.7:\n",
    "        print(\"   - Consider improving the system prompt\")\n",
    "        print(\"   - Review failed examples manually\")\n",
    "    if analysis['tool_usage_rate'] < 0.5:\n",
    "        print(\"   - Agent may not be using tools effectively\")\n",
    "        print(\"   - Check tool calling logic\")\n",
    "    if analysis['pass_rates_by_check'].get('answer_citations', 0) < 0.5:\n",
    "        print(\"   - Add citation requirements to system prompt\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Example usage:\n",
    "# analysis = analyze_evaluation_results(df_evals)\n",
    "# print_evaluation_report(df_evals)\n",
    "\n",
    "print(\"Performance analysis functions ready!\")\n",
    "print(\"To analyze results, create a DataFrame and call: print_evaluation_report(df_evals)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8af06",
   "metadata": {},
   "source": [
    "## Evaluating Search Functions\n",
    "\n",
    "Also, we can (and should) evaluate our tools separately from evaluating the agent.\n",
    "\n",
    "If it's code, we need to cover it with unit and integration tests.\n",
    "\n",
    "We also have the search function, which we can evaluate using standard information retrieval metrics. For example:\n",
    "\n",
    "- **Precision and Recall**: How many relevant results were retrieved vs. how many relevant results were missed\n",
    "- **Hit Rate**: Percentage of queries that return at least one relevant result\n",
    "- **MRR (Mean Reciprocal Rank)**: Reflects the position of the first relevant result in the ranking\n",
    "\n",
    "This is how we can implement hitrate and MRR calculation in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc97dae",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_search_quality(\n",
    "    search_function,\n",
    "    test_queries: List[Dict[str, Any]],\n",
    "    num_results: int = 5\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate search function using information retrieval metrics.\n",
    "\n",
    "    Args:\n",
    "        search_function: The search function to evaluate\n",
    "        test_queries: List of dicts with 'query' and 'expected_docs' (filenames)\n",
    "        num_results: Number of results to retrieve\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for test_case in test_queries:\n",
    "        query = test_case['query']\n",
    "        expected_docs = set(test_case['expected_docs'])\n",
    "\n",
    "        # Get search results\n",
    "        search_results = search_function(query, num_results=num_results)\n",
    "\n",
    "        # Calculate metrics\n",
    "        found_docs = set()\n",
    "        first_relevant_pos = None\n",
    "\n",
    "        for i, result in enumerate(search_results):\n",
    "            doc_filename = result.get('filename', result.get('id', ''))\n",
    "            found_docs.add(doc_filename)\n",
    "\n",
    "            if doc_filename in expected_docs and first_relevant_pos is None:\n",
    "                first_relevant_pos = i + 1  # 1-indexed\n",
    "\n",
    "        # Calculate metrics\n",
    "        relevant_found = len(found_docs & expected_docs)\n",
    "        total_relevant = len(expected_docs)\n",
    "        total_returned = len(search_results)\n",
    "\n",
    "        precision = relevant_found / total_returned if total_returned > 0 else 0\n",
    "        recall = relevant_found / total_relevant if total_relevant > 0 else 0\n",
    "        hit_rate = 1 if relevant_found > 0 else 0\n",
    "        mrr = 1 / first_relevant_pos if first_relevant_pos else 0\n",
    "\n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'hit_rate': hit_rate,\n",
    "            'mrr': mrr,\n",
    "            'relevant_found': relevant_found,\n",
    "            'total_relevant': total_relevant\n",
    "        })\n",
    "\n",
    "    # Aggregate metrics\n",
    "    avg_precision = sum(r['precision'] for r in results) / len(results)\n",
    "    avg_recall = sum(r['recall'] for r in results) / len(results)\n",
    "    avg_hit_rate = sum(r['hit_rate'] for r in results) / len(results)\n",
    "    avg_mrr = sum(r['mrr'] for r in results) / len(results)\n",
    "\n",
    "    return {\n",
    "        'individual_results': results,\n",
    "        'aggregate_metrics': {\n",
    "            'avg_precision': avg_precision,\n",
    "            'avg_recall': avg_recall,\n",
    "            'avg_hit_rate': avg_hit_rate,\n",
    "            'avg_mrr': avg_mrr\n",
    "        },\n",
    "        'summary': f\"\"\"\n",
    "Search Evaluation Summary:\n",
    "- Average Precision: {avg_precision:.3f}\n",
    "- Average Recall: {avg_recall:.3f}\n",
    "- Average Hit Rate: {avg_hit_rate:.3f}\n",
    "- Average MRR: {avg_mrr:.3f}\n",
    "        \"\"\".strip()\n",
    "    }\n",
    "\n",
    "# Example test queries for search evaluation\n",
    "# test_queries = [\n",
    "#     {\n",
    "#         'query': 'how to install kafka',\n",
    "#         'expected_docs': ['kafka-installation.md', 'python-kafka-setup.md']\n",
    "#     },\n",
    "#     # Add more test queries...\n",
    "# ]\n",
    "\n",
    "print(\"Search evaluation functions ready!\")\n",
    "print(\"To evaluate search quality, prepare test_queries and call: evaluate_search_quality(hybrid_search, test_queries)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d38740",
   "metadata": {},
   "source": [
    "## Day 5 Summary: Complete Evaluation System\n",
    "\n",
    "Congratulations! 🎉 You've now implemented a comprehensive evaluation system for your AI agent. Here's what we've built:\n",
    "\n",
    "### ✅ What We've Accomplished:\n",
    "\n",
    "1. **Logging System**: Track all agent interactions with detailed JSON logs\n",
    "2. **AI as Judge**: Automated evaluation using LLM to assess response quality\n",
    "3. **Data Generation**: AI-powered test question generation from FAQ content\n",
    "4. **Performance Metrics**: Comprehensive analysis with pass rates and statistics\n",
    "5. **Search Evaluation**: Information retrieval metrics for search functions\n",
    "\n",
    "### 🚀 How to Use the Evaluation System:\n",
    "\n",
    "```python\n",
    "# 1. Generate test questions\n",
    "questions = generate_test_questions(de_dtc_faq, num_samples=10)\n",
    "\n",
    "# 2. Run evaluation\n",
    "eval_results = run_agent_evaluation(\n",
    "    agent_name='faq_agent_v3',\n",
    "    system_prompt=system_prompt,\n",
    "    model=model,\n",
    "    questions=questions,\n",
    "    max_questions=5\n",
    ")\n",
    "\n",
    "# 3. Analyze results\n",
    "df_evals = create_evaluation_dataframe(eval_results)\n",
    "print_evaluation_report(df_evals)\n",
    "\n",
    "# 4. Evaluate search quality (optional)\n",
    "search_metrics = evaluate_search_quality(hybrid_search, test_queries)\n",
    "```\n",
    "\n",
    "### 📊 Key Metrics to Track:\n",
    "\n",
    "- **Pass Rates**: How often the agent meets each evaluation criterion\n",
    "- **Tool Usage**: Whether the agent properly uses search tools\n",
    "- **Response Quality**: Relevance, clarity, completeness, and citations\n",
    "- **Search Performance**: Precision, recall, hit rate, and MRR\n",
    "\n",
    "### 🎯 Next Steps:\n",
    "\n",
    "1. **Run the evaluation** on your current agent to establish a baseline\n",
    "2. **Iterate and improve** based on the evaluation results\n",
    "3. **Compare different approaches** (different prompts, models, search methods)\n",
    "4. **Monitor performance** over time as you make changes\n",
    "\n",
    "### 💡 Evaluation Best Practices:\n",
    "\n",
    "- **Start small**: Begin with 10-20 test cases\n",
    "- **Manual review**: Always manually inspect some results\n",
    "- **Iterate**: Use evaluation results to guide improvements\n",
    "- **Version control**: Keep evaluation datasets fixed when comparing versions\n",
    "- **Automate**: Set up regular evaluation runs\n",
    "\n",
    "This evaluation system will help you build confidence in your AI agent and make data-driven decisions about improvements. Remember: evaluation is not a one-time thing - it's an ongoing process that should be part of your development workflow!\n",
    "\n",
    "**Day 5 Complete!** 🎓 Ready for Day 6? 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
