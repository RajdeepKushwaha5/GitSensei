{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985bb98f",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "Import the necessary libraries: io, zipfile, requests, and frontmatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "601221d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c82fb",
   "metadata": {},
   "source": [
    "# Define the read_repo_data Function\n",
    "\n",
    "Define a function to download a GitHub repository as a zip file, extract markdown files, parse them with frontmatter, and return a list of dictionaries with content and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1077a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc5783",
   "metadata": {},
   "source": [
    "# Download and Process Repository Data\n",
    "\n",
    "Use the read_repo_data function to download and process data from specified GitHub repositories, such as 'DataTalksClub/faq' and 'evidentlyai/docs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46127d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For homework, select a GitHub repo with documentation: evidentlyai/docs\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "# Optionally, you can try other repos\n",
    "# dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "# fastai_docs = read_repo_data('fastai', 'fastbook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66276dce",
   "metadata": {},
   "source": [
    "# Print Document Counts\n",
    "\n",
    "Print the number of documents retrieved from each repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "092a5e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n",
    "\n",
    "# Uncomment to print others\n",
    "# print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "# print(f\"FastAI documents: {len(fastai_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc9f6d",
   "metadata": {},
   "source": [
    "# Day 2: Chunking and Intelligent Processing for Data\n",
    "\n",
    "Welcome to Day 2 of our 7-Day AI Agents Email Crash-Course.\n",
    "\n",
    "In the first part of the course, we focus on data preparation â€“ the process of properly preparing data before it can be used for AI agents.\n",
    "\n",
    "## Small and Large Documents\n",
    "\n",
    "Yesterday (Day 1), we downloaded the data from a GitHub repository and processed it. For some use cases, like the FAQ database, this is sufficient. The questions and answers are small enough. We can put them directly into the search engine.\n",
    "\n",
    "But it's different for the Evidently documentation. These documents are quite large. Let's take a look at this one: https://github.com/evidentlyai/docs/blob/main/docs/library/descriptors.mdx.\n",
    "\n",
    "We could use it as is, but we risk overwhelming our LLMs.\n",
    "\n",
    "## Why We Need to Prepare Large Documents Before Using Them\n",
    "\n",
    "Large documents create several problems:\n",
    "\n",
    "- Token limits: Most LLMs have maximum input token limits\n",
    "- Cost: Longer prompts cost more money\n",
    "- Performance: LLMs perform worse with very long contexts\n",
    "- Relevance: Not all parts of a long document are relevant to a specific question\n",
    "\n",
    "So we need to split documents into smaller subdocuments. For AI applications like RAG (which we will discuss tomorrow), this process is referred to as \"chunking.\"\n",
    "\n",
    "Today, we will cover multiple ways of chunking data:\n",
    "\n",
    "1. Simple character-based chunking\n",
    "2. Paragraph and section-based chunking\n",
    "3. Intelligent chunking with LLM\n",
    "\n",
    "Just so you know, for the last section, you will need a Gemini API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0165d1",
   "metadata": {},
   "source": [
    "## 1. Simple Chunking\n",
    "\n",
    "Let's start with simple chunking. This will be sufficient for most cases.\n",
    "\n",
    "We can continue with the notebook from Day 1. We already downloaded the data from Evidently docs. We put them into the evidently_docs list.\n",
    "\n",
    "This is how the document at index 45 looks like:\n",
    "\n",
    "{'title': 'LLM regression testing',\n",
    " 'description': 'How to run regression testing for LLM outputs.',\n",
    " 'content': 'In this tutorial, you will learn...'\n",
    "}\n",
    "\n",
    "The content field is 21,712 characters long. The simplest thing we can do is cut it into pieces of equal length. For example, for size of 2000 characters, we will have:\n",
    "\n",
    "Chunk 1: 0..2000\n",
    "Chunk 2: 2000..4000\n",
    "Chunk 3: 4000..6000\n",
    "\n",
    "And so on.\n",
    "\n",
    "However, this approach has disadvantages:\n",
    "\n",
    "- Context loss: Important information might be split in the middle\n",
    "- Incomplete sentences: Chunks might end mid-sentence\n",
    "- Missing connections: Related information might end up in different chunks\n",
    "\n",
    "That's why, in practice, we usually make sure there's overlap between chunks. For size 2000 and overlap 1000, we will have:\n",
    "\n",
    "Chunk 1: 0..2000\n",
    "Chunk 2: 1000..3000\n",
    "Chunk 3: 2000..4000\n",
    "...\n",
    "\n",
    "This is better for AI because:\n",
    "\n",
    "- Continuity: Important information isn't lost at chunk boundaries\n",
    "- Context preservation: Related sentences stay together in at least one chunk\n",
    "- Better search: Queries can match information even if it spans chunk boundaries\n",
    "\n",
    "This approach is known as the \"sliding window\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36a60c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c630a213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 45 has 21 chunks\n",
      "Total chunks created: 575 from 95 documents\n"
     ]
    }
   ],
   "source": [
    "# Let's apply it for document 45. This gives us 21 chunks:\n",
    "# 0..2000, 1000..3000, ..., 19000..21000, 20000..21712\n",
    "\n",
    "if len(evidently_docs) > 45:\n",
    "    doc_45_content = evidently_docs[45]['content']\n",
    "    chunks_45 = sliding_window(doc_45_content, 2000, 1000)\n",
    "    print(f\"Document 45 has {len(chunks_45)} chunks\")\n",
    "else:\n",
    "    print(\"Document 45 not available\")\n",
    "\n",
    "# Let's process all the documents:\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total chunks created: {len(evidently_chunks)} from {len(evidently_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc16533",
   "metadata": {},
   "source": [
    "## 2. Splitting by Paragraphs and Sections\n",
    "\n",
    "Splitting by paragraphs is relatively easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc694a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 45 has 153 paragraphs\n",
      "First paragraph: In this tutorial, you will learn how to perform regression testing for LLM outputs....\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "if len(evidently_docs) > 45:\n",
    "    text = evidently_docs[45]['content']\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "    print(f\"Document 45 has {len(paragraphs)} paragraphs\")\n",
    "    print(f\"First paragraph: {paragraphs[0][:200]}...\")\n",
    "else:\n",
    "    print(\"Document 45 not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ef858",
   "metadata": {},
   "source": [
    "Let's now look at section splitting. Here, we take advantage of the documents' structure. Markdown documents have this structure:\n",
    "\n",
    "# Heading 1\n",
    "## Heading 2  \n",
    "### Heading 3\n",
    "\n",
    "What we can do is split by headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e51045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a59f13ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 45 has 8 sections\n",
      "First section: ## 1. Installation and Imports\n",
      "\n",
      "Install Evidently:\n",
      "\n",
      "```python\n",
      "pip install evidently[llm] \n",
      "```\n",
      "\n",
      "Import the required modules:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from evidently.future.datasets import Dataset...\n",
      "Total sections created: 262 from 95 documents\n"
     ]
    }
   ],
   "source": [
    "# If we want to split by second-level headers, that's what we do:\n",
    "\n",
    "if len(evidently_docs) > 45:\n",
    "    text = evidently_docs[45]['content']\n",
    "    sections = split_markdown_by_level(text, level=2)\n",
    "    print(f\"Document 45 has {len(sections)} sections\")\n",
    "    if sections:\n",
    "        print(f\"First section: {sections[0][:200]}...\")\n",
    "else:\n",
    "    print(\"Document 45 not available\")\n",
    "\n",
    "# Now we iterate over all the docs to create the final result:\n",
    "\n",
    "evidently_chunks_sections = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks_sections.append(section_doc)\n",
    "\n",
    "print(f\"Total sections created: {len(evidently_chunks_sections)} from {len(evidently_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b278b0",
   "metadata": {},
   "source": [
    "## 3. Intelligent Chunking with LLM\n",
    "\n",
    "In some cases, we want to be more intelligent with chunking. Instead of doing simple splits, we delegate this work to AI.\n",
    "\n",
    "This makes sense when:\n",
    "\n",
    "- Complex structure: Documents have complex, non-standard structure\n",
    "- Semantic coherence: You want chunks that are semantically meaningful\n",
    "- Custom logic: You need domain-specific splitting rules\n",
    "- Quality over cost: You prioritize quality over processing cost\n",
    "\n",
    "This costs money. In most cases, we don't need intelligent chunking.\n",
    "\n",
    "Simple approaches are sufficient. Use intelligent chunking only when\n",
    "\n",
    "- You already evaluated simpler methods and you can confirm that they produce poor results\n",
    "- You have complex, unstructured documents\n",
    "- Quality is more important than cost\n",
    "- You have the budget for LLM processing\n",
    "\n",
    "Let's create a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ebd16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Gemini API\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in environment variables. Please set it in a .env file.\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n",
    "\n",
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    \n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    sections = response.text.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24adcd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aihero\\project\\.venv\\Scripts\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "%pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48273f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aihero\\project\\.venv\\Scripts\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce91ef26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3608c4dd6e5048f3a450ca9830314ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we apply this to every document:\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks_intelligent = []\n",
    "\n",
    "# Uncomment the next line and set your API key\n",
    "# genai.configure(api_key=\"YOUR_GEMINI_API_KEY\")\n",
    "\n",
    "for doc in tqdm(evidently_docs[:5]):  # Process only first 5 docs for demo (costs money)\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks_intelligent.append(section_doc)\n",
    "\n",
    "print(f\"Total intelligent sections created: {len(evidently_chunks_intelligent)} from 5 documents\")\n",
    "\n",
    "# Note: This process requires time and incurs costs. As mentioned before, use this only when really necessary.\n",
    "# For most applications, you don't need intelligent chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef29ff",
   "metadata": {},
   "source": [
    "## Bonus: Processing Code in Your GitHub Repository\n",
    "\n",
    "You can use this approach for processing the code in your GitHub repository. You can use a variation of the following prompt:\n",
    "\n",
    "\"Summarize the code in plain English. Briefly describe each class and function/method (their purpose and role), then give a short overall summary of how they work together. Avoid low-level details.\"\n",
    "\n",
    "Then add both the source code and the summary to your documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85a146",
   "metadata": {},
   "source": [
    "# Implementing Code Processing for GitHub Repositories\n",
    "\n",
    "Now let's implement the bonus feature to process code files from GitHub repositories. We'll download code files, use the LLM to generate summaries, and add both the source code and summaries to our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15544662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_code(repo_owner, repo_name, file_extensions=None):\n",
    "    \"\"\"\n",
    "    Download and extract code files from a GitHub repository.\n",
    "\n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "        file_extensions: List of file extensions to include (e.g., ['.py', '.js', '.ts'])\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    if file_extensions is None:\n",
    "        file_extensions = ['.py', '.js', '.ts', '.java', '.cpp', '.c', '.go', '.rs']\n",
    "\n",
    "    prefix = 'https://codeload.github.com'\n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_code = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        # Skip if not a code file\n",
    "        if not any(filename_lower.endswith(ext) for ext in file_extensions):\n",
    "            continue\n",
    "\n",
    "        # Skip files in common non-code directories\n",
    "        if any(skip_dir in filename_lower for skip_dir in ['node_modules/', '__pycache__/', '.git/', 'dist/', 'build/']):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                # Skip empty files or very small files\n",
    "                if len(content.strip()) < 50:\n",
    "                    continue\n",
    "\n",
    "                data = {\n",
    "                    'filename': filename,\n",
    "                    'content': content,\n",
    "                    'language': filename.split('.')[-1] if '.' in filename else 'unknown'\n",
    "                }\n",
    "                repository_code.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    zf.close()\n",
    "    return repository_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52256cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_summary_prompt = \"\"\"\n",
    "Summarize the code in plain English. Briefly describe each class and function/method (their purpose and role), then give a short overall summary of how they work together. Avoid low-level details.\n",
    "\n",
    "<CODE>\n",
    "{code}\n",
    "</CODE>\n",
    "\n",
    "Provide the summary in this format:\n",
    "\n",
    "## Summary\n",
    "\n",
    "[Your summary here]\n",
    "\"\"\".strip()\n",
    "\n",
    "def summarize_code_with_llm(code_content, filename):\n",
    "    \"\"\"\n",
    "    Use LLM to generate a summary of the provided code.\n",
    "\n",
    "    Args:\n",
    "        code_content: The source code as a string\n",
    "        filename: The filename for context\n",
    "\n",
    "    Returns:\n",
    "        Summary string generated by the LLM\n",
    "    \"\"\"\n",
    "    prompt = code_summary_prompt.format(code=code_content)\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing {filename}: {e}\")\n",
    "        return f\"Error generating summary for {filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35911eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download code from a GitHub repository\n",
    "# Let's use a small, well-known Python project for demonstration\n",
    "# You can replace this with any repository you want to process\n",
    "\n",
    "code_repo_owner = 'psf'\n",
    "code_repo_name = 'requests'  # A popular Python HTTP library\n",
    "\n",
    "print(f\"Downloading code from {code_repo_owner}/{code_repo_name}...\")\n",
    "code_files = read_repo_code(code_repo_owner, code_repo_name, file_extensions=['.py'])\n",
    "print(f\"Found {len(code_files)} Python files\")\n",
    "\n",
    "# Process a few files for demonstration (to avoid high API costs)\n",
    "code_documents = []\n",
    "\n",
    "for i, code_file in enumerate(code_files[:3]):  # Process only first 3 files\n",
    "    print(f\"Processing file {i+1}/{min(3, len(code_files))}: {code_file['filename']}\")\n",
    "\n",
    "    # Generate summary using LLM\n",
    "    summary = summarize_code_with_llm(code_file['content'], code_file['filename'])\n",
    "\n",
    "    # Create document with both source code and summary\n",
    "    doc = {\n",
    "        'filename': code_file['filename'],\n",
    "        'language': code_file['language'],\n",
    "        'source_code': code_file['content'],\n",
    "        'summary': summary,\n",
    "        'content': f\"## Source Code\\n\\n```{code_file['language']}\\n{code_file['content']}\\n```\\n\\n## Summary\\n\\n{summary}\"\n",
    "    }\n",
    "\n",
    "    code_documents.append(doc)\n",
    "\n",
    "print(f\"Created {len(code_documents)} code documents with summaries\")\n",
    "\n",
    "# You can now use these code_documents in your RAG system or search engine\n",
    "# Each document contains both the original source code and its LLM-generated summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ffcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "print(\"Code Processing Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, doc in enumerate(code_documents, 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"Filename: {doc['filename']}\")\n",
    "    print(f\"Language: {doc['language']}\")\n",
    "    print(f\"Summary preview: {doc['summary'][:200]}...\")\n",
    "    print(f\"Full content length: {len(doc['content'])} characters\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# You can now combine these code documents with your markdown documents\n",
    "# For example:\n",
    "# all_documents = evidently_docs + code_documents\n",
    "\n",
    "print(f\"\\nTotal code documents created: {len(code_documents)}\")\n",
    "print(\"Each document contains both the source code and its AI-generated summary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3e168",
   "metadata": {},
   "source": [
    "# Usage Notes for Code Processing\n",
    "\n",
    "## What was implemented:\n",
    "\n",
    "1. **`read_repo_code()`**: Downloads and extracts code files from GitHub repositories\n",
    "   - Filters by file extensions (default: .py, .js, .ts, .java, .cpp, .c, .go, .rs)\n",
    "   - Skips common non-code directories and empty files\n",
    "\n",
    "2. **`summarize_code_with_llm()`**: Uses Gemini AI to generate plain English summaries\n",
    "   - Uses the exact prompt you specified\n",
    "   - Describes classes, functions/methods and their purposes\n",
    "   - Provides overall summary of how components work together\n",
    "\n",
    "3. **Document Creation**: Combines source code and summaries into searchable documents\n",
    "   - Each document contains: filename, language, source_code, summary, and combined content\n",
    "\n",
    "## How to use this for your own repositories:\n",
    "\n",
    "```python\n",
    "# Replace with your repository details\n",
    "your_code_files = read_repo_code('your-username', 'your-repo-name')\n",
    "\n",
    "# Process all files (be mindful of API costs)\n",
    "your_code_documents = []\n",
    "for code_file in your_code_files:\n",
    "    summary = summarize_code_with_llm(code_file['content'], code_file['filename'])\n",
    "    doc = {\n",
    "        'filename': code_file['filename'],\n",
    "        'language': code_file['language'],\n",
    "        'source_code': code_file['content'],\n",
    "        'summary': summary,\n",
    "        'content': f\"## Source Code\\n\\n```{code_file['language']}\\n{code_file['content']}\\n```\\n\\n## Summary\\n\\n{summary}\"\n",
    "    }\n",
    "    your_code_documents.append(doc)\n",
    "```\n",
    "\n",
    "## Cost Considerations:\n",
    "- Each file summarization costs API credits\n",
    "- Process files selectively or in batches\n",
    "- Consider file size limits for very large code files\n",
    "\n",
    "## Integration with RAG:\n",
    "You can now combine code documents with your markdown documents:\n",
    "```python\n",
    "all_documents = evidently_docs + code_documents + your_code_documents\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
