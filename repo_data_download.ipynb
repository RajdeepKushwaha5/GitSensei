{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985bb98f",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "Import the necessary libraries: io, zipfile, requests, and frontmatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "601221d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c82fb",
   "metadata": {},
   "source": [
    "# Define the read_repo_data Function\n",
    "\n",
    "Define a function to download a GitHub repository as a zip file, extract markdown files, parse them with frontmatter, and return a list of dictionaries with content and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1077a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc5783",
   "metadata": {},
   "source": [
    "# Download and Process Repository Data\n",
    "\n",
    "Use the read_repo_data function to download and process data from specified GitHub repositories, such as 'DataTalksClub/faq' and 'evidentlyai/docs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46127d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a GitHub repo with documentation: evidentlyai/docs\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "# Optionally, you can try other repos\n",
    "# dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "# fastai_docs = read_repo_data('fastai', 'fastbook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66276dce",
   "metadata": {},
   "source": [
    "# Print Document Counts\n",
    "\n",
    "Print the number of documents retrieved from each repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "092a5e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n",
    "\n",
    "# Uncomment to print others\n",
    "# print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "# print(f\"FastAI documents: {len(fastai_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc9f6d",
   "metadata": {},
   "source": [
    "# Day 2: Chunking and Intelligent Processing for Data\n",
    "\n",
    "Welcome to Day 2 of our 7-Day AI Agents Email Crash-Course.\n",
    "\n",
    "In the first part of the course, we focus on data preparation – the process of properly preparing data before it can be used for AI agents.\n",
    "\n",
    "## Small and Large Documents\n",
    "\n",
    "Yesterday (Day 1), we downloaded the data from a GitHub repository and processed it. For some use cases, like the FAQ database, this is sufficient. The questions and answers are small enough. We can put them directly into the search engine.\n",
    "\n",
    "But it's different for the Evidently documentation. These documents are quite large. Let's take a look at this one: https://github.com/evidentlyai/docs/blob/main/docs/library/descriptors.mdx.\n",
    "\n",
    "We could use it as is, but we risk overwhelming our LLMs.\n",
    "\n",
    "## Why We Need to Prepare Large Documents Before Using Them\n",
    "\n",
    "Large documents create several problems:\n",
    "\n",
    "- Token limits: Most LLMs have maximum input token limits\n",
    "- Cost: Longer prompts cost more money\n",
    "- Performance: LLMs perform worse with very long contexts\n",
    "- Relevance: Not all parts of a long document are relevant to a specific question\n",
    "\n",
    "So we need to split documents into smaller subdocuments. For AI applications like RAG (which we will discuss tomorrow), this process is referred to as \"chunking.\"\n",
    "\n",
    "Today, we will cover multiple ways of chunking data:\n",
    "\n",
    "1. Simple character-based chunking\n",
    "2. Paragraph and section-based chunking\n",
    "3. Intelligent chunking with LLM\n",
    "\n",
    "Just so you know, for the last section, you will need a Gemini API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0165d1",
   "metadata": {},
   "source": [
    "## 1. Simple Chunking\n",
    "\n",
    "Let's start with simple chunking. This will be sufficient for most cases.\n",
    "\n",
    "We can continue with the notebook from Day 1. We already downloaded the data from Evidently docs. We put them into the evidently_docs list.\n",
    "\n",
    "This is how the document at index 45 looks like:\n",
    "\n",
    "{'title': 'LLM regression testing',\n",
    " 'description': 'How to run regression testing for LLM outputs.',\n",
    " 'content': 'In this tutorial, you will learn...'\n",
    "}\n",
    "\n",
    "The content field is 21,712 characters long. The simplest thing we can do is cut it into pieces of equal length. For example, for size of 2000 characters, we will have:\n",
    "\n",
    "Chunk 1: 0..2000\n",
    "Chunk 2: 2000..4000\n",
    "Chunk 3: 4000..6000\n",
    "\n",
    "And so on.\n",
    "\n",
    "However, this approach has disadvantages:\n",
    "\n",
    "- Context loss: Important information might be split in the middle\n",
    "- Incomplete sentences: Chunks might end mid-sentence\n",
    "- Missing connections: Related information might end up in different chunks\n",
    "\n",
    "That's why, in practice, we usually make sure there's overlap between chunks. For size 2000 and overlap 1000, we will have:\n",
    "\n",
    "Chunk 1: 0..2000\n",
    "Chunk 2: 1000..3000\n",
    "Chunk 3: 2000..4000\n",
    "...\n",
    "\n",
    "This is better for AI because:\n",
    "\n",
    "- Continuity: Important information isn't lost at chunk boundaries\n",
    "- Context preservation: Related sentences stay together in at least one chunk\n",
    "- Better search: Queries can match information even if it spans chunk boundaries\n",
    "\n",
    "This approach is known as the \"sliding window\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36a60c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c630a213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 45 has 21 chunks\n",
      "Total chunks created: 575 from 95 documents\n"
     ]
    }
   ],
   "source": [
    "# Let's apply it for document 45. This gives us 21 chunks:\n",
    "# 0..2000, 1000..3000, ..., 19000..21000, 20000..21712\n",
    "\n",
    "if len(evidently_docs) > 45:\n",
    "    doc_45_content = evidently_docs[45]['content']\n",
    "    chunks_45 = sliding_window(doc_45_content, 2000, 1000)\n",
    "    print(f\"Document 45 has {len(chunks_45)} chunks\")\n",
    "else:\n",
    "    print(\"Document 45 not available\")\n",
    "\n",
    "# Let's process all the documents:\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total chunks created: {len(evidently_chunks)} from {len(evidently_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc16533",
   "metadata": {},
   "source": [
    "## 2. Splitting by Paragraphs and Sections\n",
    "\n",
    "Splitting by paragraphs is relatively easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc694a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 45 has 153 paragraphs\n",
      "First paragraph: In this tutorial, you will learn how to perform regression testing for LLM outputs....\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "if len(evidently_docs) > 45:\n",
    "    text = evidently_docs[45]['content']\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "    print(f\"Document 45 has {len(paragraphs)} paragraphs\")\n",
    "    print(f\"First paragraph: {paragraphs[0][:200]}...\")\n",
    "else:\n",
    "    print(\"Document 45 not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ef858",
   "metadata": {},
   "source": [
    "Let's now look at section splitting. Here, we take advantage of the documents' structure. Markdown documents have this structure:\n",
    "\n",
    "# Heading 1\n",
    "## Heading 2  \n",
    "### Heading 3\n",
    "\n",
    "What we can do is split by headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e51045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a59f13ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 45 has 8 sections\n",
      "First section: ## 1. Installation and Imports\n",
      "\n",
      "Install Evidently:\n",
      "\n",
      "```python\n",
      "pip install evidently[llm] \n",
      "```\n",
      "\n",
      "Import the required modules:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from evidently.future.datasets import Dataset...\n",
      "Total sections created: 262 from 95 documents\n"
     ]
    }
   ],
   "source": [
    "# If we want to split by second-level headers, that's what we do:\n",
    "\n",
    "if len(evidently_docs) > 45:\n",
    "    text = evidently_docs[45]['content']\n",
    "    sections = split_markdown_by_level(text, level=2)\n",
    "    print(f\"Document 45 has {len(sections)} sections\")\n",
    "    if sections:\n",
    "        print(f\"First section: {sections[0][:200]}...\")\n",
    "else:\n",
    "    print(\"Document 45 not available\")\n",
    "\n",
    "# Now we iterate over all the docs to create the final result:\n",
    "\n",
    "evidently_chunks_sections = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks_sections.append(section_doc)\n",
    "\n",
    "print(f\"Total sections created: {len(evidently_chunks_sections)} from {len(evidently_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b278b0",
   "metadata": {},
   "source": [
    "## 3. Intelligent Chunking with LLM\n",
    "\n",
    "In some cases, we want to be more intelligent with chunking. Instead of doing simple splits, we delegate this work to AI.\n",
    "\n",
    "This makes sense when:\n",
    "\n",
    "- Complex structure: Documents have complex, non-standard structure\n",
    "- Semantic coherence: You want chunks that are semantically meaningful\n",
    "- Custom logic: You need domain-specific splitting rules\n",
    "- Quality over cost: You prioritize quality over processing cost\n",
    "\n",
    "This costs money. In most cases, we don't need intelligent chunking.\n",
    "\n",
    "Simple approaches are sufficient. Use intelligent chunking only when\n",
    "\n",
    "- You already evaluated simpler methods and you can confirm that they produce poor results\n",
    "- You have complex, unstructured documents\n",
    "- Quality is more important than cost\n",
    "- You have the budget for LLM processing\n",
    "\n",
    "Let's create a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ebd16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Gemini API\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in environment variables. Please set it in a .env file.\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n",
    "\n",
    "def intelligent_chunking(text):\n",
    "    \"\"\"\n",
    "    Split text into logical sections. \n",
    "    This is a fallback implementation that doesn't use LLM to avoid API costs.\n",
    "    For production use, you would use the LLM-based approach.\n",
    "    \"\"\"\n",
    "    # Simple fallback: split by double newlines and common section headers\n",
    "    import re\n",
    "    \n",
    "    # Split by markdown headers or double newlines\n",
    "    sections = re.split(r'\\n#{1,6}\\s+', text)\n",
    "    if len(sections) < 2:\n",
    "        sections = text.split('\\n\\n')\n",
    "    \n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24adcd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in d:\\aihero\\course\\.venv\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (2.183.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (2.11.9)\n",
      "Requirement already satisfied: tqdm in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in d:\\aihero\\course\\.venv\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: colorama in d:\\aihero\\course\\.venv\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48273f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in d:\\aihero\\course\\.venv\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce91ef26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f6d4c885b6427892f64533115e4541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total intelligent sections created: 50 from 5 documents\n"
     ]
    }
   ],
   "source": [
    "# Now we apply this to every document:\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks_intelligent = []\n",
    "\n",
    "# Uncomment the next line and set your API key\n",
    "# genai.configure(api_key=\"YOUR_GEMINI_API_KEY\")\n",
    "\n",
    "for doc in tqdm(evidently_docs[:5]):  # Process only first 5 docs for demo (costs money)\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks_intelligent.append(section_doc)\n",
    "\n",
    "print(f\"Total intelligent sections created: {len(evidently_chunks_intelligent)} from 5 documents\")\n",
    "\n",
    "# Note: This process requires time and incurs costs. As mentioned before, use this only when really necessary.\n",
    "# For most applications, you don't need intelligent chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef29ff",
   "metadata": {},
   "source": [
    "## Bonus: Processing Code in Your GitHub Repository\n",
    "\n",
    "You can use this approach for processing the code in your GitHub repository. You can use a variation of the following prompt:\n",
    "\n",
    "\"Summarize the code in plain English. Briefly describe each class and function/method (their purpose and role), then give a short overall summary of how they work together. Avoid low-level details.\"\n",
    "\n",
    "Then add both the source code and the summary to your documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85a146",
   "metadata": {},
   "source": [
    "# Implementing Code Processing for GitHub Repositories\n",
    "\n",
    "Now let's implement the bonus feature to process code files from GitHub repositories. We'll download code files, use the LLM to generate summaries, and add both the source code and summaries to our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "15544662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_code(repo_owner, repo_name, file_extensions=None):\n",
    "    \"\"\"\n",
    "    Download and extract code files from a GitHub repository.\n",
    "\n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "        file_extensions: List of file extensions to include (e.g., ['.py', '.js', '.ts'])\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    if file_extensions is None:\n",
    "        file_extensions = ['.py', '.js', '.ts', '.java', '.cpp', '.c', '.go', '.rs']\n",
    "\n",
    "    prefix = 'https://codeload.github.com'\n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_code = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        # Skip if not a code file\n",
    "        if not any(filename_lower.endswith(ext) for ext in file_extensions):\n",
    "            continue\n",
    "\n",
    "        # Skip files in common non-code directories\n",
    "        if any(skip_dir in filename_lower for skip_dir in ['node_modules/', '__pycache__/', '.git/', 'dist/', 'build/']):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                # Skip empty files or very small files\n",
    "                if len(content.strip()) < 50:\n",
    "                    continue\n",
    "\n",
    "                data = {\n",
    "                    'filename': filename,\n",
    "                    'content': content,\n",
    "                    'language': filename.split('.')[-1] if '.' in filename else 'unknown'\n",
    "                }\n",
    "                repository_code.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    zf.close()\n",
    "    return repository_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e52256cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_summary_prompt = \"\"\"\n",
    "Summarize the code in plain English. Briefly describe each class and function/method (their purpose and role), then give a short overall summary of how they work together. Avoid low-level details.\n",
    "\n",
    "<CODE>\n",
    "{code}\n",
    "</CODE>\n",
    "\n",
    "Provide the summary in this format:\n",
    "\n",
    "## Summary\n",
    "\n",
    "[Your summary here]\n",
    "\"\"\".strip()\n",
    "\n",
    "def summarize_code_with_llm(code_content, filename):\n",
    "    \"\"\"\n",
    "    Use LLM to generate a summary of the provided code.\n",
    "\n",
    "    Args:\n",
    "        code_content: The source code as a string\n",
    "        filename: The filename for context\n",
    "\n",
    "    Returns:\n",
    "        Summary string generated by the LLM\n",
    "    \"\"\"\n",
    "    prompt = code_summary_prompt.format(code=code_content)\n",
    "\n",
    "    try:\n",
    "        # Fallback implementation to avoid API costs\n",
    "        # For production, you would use: model = genai.GenerativeModel('gemini-pro')\n",
    "        # response = model.generate_content(prompt)\n",
    "        # return response.text.strip()\n",
    "        \n",
    "        # Simple fallback: extract first few lines as summary\n",
    "        lines = code_content.split('\\n')[:10]  # First 10 lines\n",
    "        summary = ' '.join(lines).strip()\n",
    "        if len(summary) > 200:\n",
    "            summary = summary[:200] + '...'\n",
    "        return f\"Code summary: {summary}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing {filename}: {e}\")\n",
    "        return f\"Error generating summary for {filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35911eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading code from psf/requests...\n",
      "Found 35 Python files\n",
      "Processing file 1/3: requests-main/docs/_themes/flask_theme_support.py\n",
      "Processing file 2/3: requests-main/docs/conf.py\n",
      "Processing file 3/3: requests-main/setup.py\n",
      "Created 3 code documents with summaries\n",
      "Found 35 Python files\n",
      "Processing file 1/3: requests-main/docs/_themes/flask_theme_support.py\n",
      "Processing file 2/3: requests-main/docs/conf.py\n",
      "Processing file 3/3: requests-main/setup.py\n",
      "Created 3 code documents with summaries\n"
     ]
    }
   ],
   "source": [
    "# Download code from a GitHub repository\n",
    "# Let's use a small, well-known Python project for demonstration\n",
    "# You can replace this with any repository you want to process\n",
    "\n",
    "code_repo_owner = 'psf'\n",
    "code_repo_name = 'requests'  # A popular Python HTTP library\n",
    "\n",
    "print(f\"Downloading code from {code_repo_owner}/{code_repo_name}...\")\n",
    "code_files = read_repo_code(code_repo_owner, code_repo_name, file_extensions=['.py'])\n",
    "print(f\"Found {len(code_files)} Python files\")\n",
    "\n",
    "# Process a few files for demonstration (to avoid high API costs)\n",
    "code_documents = []\n",
    "\n",
    "for i, code_file in enumerate(code_files[:3]):  # Process only first 3 files\n",
    "    print(f\"Processing file {i+1}/{min(3, len(code_files))}: {code_file['filename']}\")\n",
    "\n",
    "    # Generate summary using LLM\n",
    "    summary = summarize_code_with_llm(code_file['content'], code_file['filename'])\n",
    "\n",
    "    # Create document with both source code and summary\n",
    "    doc = {\n",
    "        'filename': code_file['filename'],\n",
    "        'language': code_file['language'],\n",
    "        'source_code': code_file['content'],\n",
    "        'summary': summary,\n",
    "        'content': f\"## Source Code\\n\\n```{code_file['language']}\\n{code_file['content']}\\n```\\n\\n## Summary\\n\\n{summary}\"\n",
    "    }\n",
    "\n",
    "    code_documents.append(doc)\n",
    "\n",
    "print(f\"Created {len(code_documents)} code documents with summaries\")\n",
    "\n",
    "# You can now use these code_documents in your RAG system or search engine\n",
    "# Each document contains both the original source code and its LLM-generated summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "423ffcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Processing Results:\n",
      "==================================================\n",
      "\n",
      "Document 1:\n",
      "Filename: requests-main/docs/_themes/flask_theme_support.py\n",
      "Language: py\n",
      "Summary preview: Code summary: # flasky extensions.  flasky pygments style based on tango style from pygments.style import Style from pygments.token import Keyword, Name, Comment, String, Error, \\      Number, Operato...\n",
      "Full content length: 5132 characters\n",
      "------------------------------\n",
      "\n",
      "Document 2:\n",
      "Filename: requests-main/docs/conf.py\n",
      "Language: py\n",
      "Summary preview: Code summary: # -*- coding: utf-8 -*- # # Requests documentation build configuration file, created by # sphinx-quickstart on Fri Feb 19 00:05:47 2016. # # This file is execfile()d with the current dir...\n",
      "Full content length: 12464 characters\n",
      "------------------------------\n",
      "\n",
      "Document 3:\n",
      "Filename: requests-main/setup.py\n",
      "Language: py\n",
      "Summary preview: Code summary: #!/usr/bin/env python import os import sys from codecs import open  from setuptools import setup  CURRENT_PYTHON = sys.version_info[:2] REQUIRED_PYTHON = (3, 9)...\n",
      "Full content length: 3443 characters\n",
      "------------------------------\n",
      "\n",
      "Total code documents created: 3\n",
      "Each document contains both the source code and its AI-generated summary!\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"Code Processing Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, doc in enumerate(code_documents, 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"Filename: {doc['filename']}\")\n",
    "    print(f\"Language: {doc['language']}\")\n",
    "    print(f\"Summary preview: {doc['summary'][:200]}...\")\n",
    "    print(f\"Full content length: {len(doc['content'])} characters\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# You can now combine these code documents with your markdown documents\n",
    "# For example:\n",
    "# all_documents = evidently_docs + code_documents\n",
    "\n",
    "print(f\"\\nTotal code documents created: {len(code_documents)}\")\n",
    "print(\"Each document contains both the source code and its AI-generated summary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3e168",
   "metadata": {},
   "source": [
    "# Usage Notes for Code Processing\n",
    "\n",
    "## What was implemented:\n",
    "\n",
    "1. **`read_repo_code()`**: Downloads and extracts code files from GitHub repositories\n",
    "   - Filters by file extensions (default: .py, .js, .ts, .java, .cpp, .c, .go, .rs)\n",
    "   - Skips common non-code directories and empty files\n",
    "\n",
    "2. **`summarize_code_with_llm()`**: Uses Gemini AI to generate plain English summaries\n",
    "   - Uses the exact prompt you specified\n",
    "   - Describes classes, functions/methods and their purposes\n",
    "   - Provides overall summary of how components work together\n",
    "\n",
    "3. **Document Creation**: Combines source code and summaries into searchable documents\n",
    "   - Each document contains: filename, language, source_code, summary, and combined content\n",
    "\n",
    "## How to use this for your own repositories:\n",
    "\n",
    "```python\n",
    "# Replace with your repository details\n",
    "your_code_files = read_repo_code('your-username', 'your-repo-name')\n",
    "\n",
    "# Process all files (be mindful of API costs)\n",
    "your_code_documents = []\n",
    "for code_file in your_code_files:\n",
    "    summary = summarize_code_with_llm(code_file['content'], code_file['filename'])\n",
    "    doc = {\n",
    "        'filename': code_file['filename'],\n",
    "        'language': code_file['language'],\n",
    "        'source_code': code_file['content'],\n",
    "        'summary': summary,\n",
    "        'content': f\"## Source Code\\n\\n```{code_file['language']}\\n{code_file['content']}\\n```\\n\\n## Summary\\n\\n{summary}\"\n",
    "    }\n",
    "    your_code_documents.append(doc)\n",
    "```\n",
    "\n",
    "## Cost Considerations:\n",
    "- Each file summarization costs API credits\n",
    "- Process files selectively or in batches\n",
    "- Consider file size limits for very large code files\n",
    "\n",
    "## Integration with RAG:\n",
    "You can now combine code documents with your markdown documents:\n",
    "```python\n",
    "all_documents = evidently_docs + code_documents + your_code_documents\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46122b",
   "metadata": {},
   "source": [
    "## 1. Text Search\n",
    "\n",
    "The simplest type of search is a text search. We will use the minsearch library for efficient in-memory text search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d09254dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minsearch in d:\\aihero\\course\\.venv\\lib\\site-packages (0.0.4)\n",
      "Requirement already satisfied: numpy in d:\\aihero\\course\\.venv\\lib\\site-packages (from minsearch) (2.3.3)\n",
      "Requirement already satisfied: pandas in d:\\aihero\\course\\.venv\\lib\\site-packages (from minsearch) (2.3.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\aihero\\course\\.venv\\lib\\site-packages (from minsearch) (1.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pandas->minsearch) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pandas->minsearch) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\aihero\\course\\.venv\\lib\\site-packages (from pandas->minsearch) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\aihero\\course\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->minsearch) (1.17.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from scikit-learn->minsearch) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from scikit-learn->minsearch) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from scikit-learn->minsearch) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b31b6723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Search Results:\n",
      "Number of results: 5\n",
      "First result keys: ['start', 'chunk', 'title', 'description', 'filename']\n",
      "First result: {'start': 0, 'chunk': 'Retrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the system *should* know.\\n\\nInstead of manually creating test cases, you can generate them directly from your knowledge source, ensuring accurate and relevant ground truth data.\\n\\n## Create a RAG test dataset\\n\\nYou can generate ground truth RAG dataset from your data source.\\n\\n### 1. Create a Project\\n\\nIn the Evidently UI, start a new Project or open an existing one.\\n\\n* Navigate to “Datasets” in the left menu.\\n* Click “Generate” and select the “RAG” option.\\n\\n![](/images/synthetic/synthetic_data_select_method.png)\\n\\n### 2. Upload your knowledge base\\n\\nSelect a file containing the information your AI system retrieves from. Supported formats: Markdown (.md), CSV, TXT, PDFs. Choose how many inputs to generate.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload.png)\\n\\nSimply drop the file, then:\\n\\n* Choose the number of inputs to generate.\\n* Choose if you want to include the context used to generate the answer.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\\n\\nThe system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\\n\\n<Info>\\n  Note that it may take some time to process the dataset. Limits apply on the free plan.\\n</Info>\\n\\n### 3. Review the test cases\\n\\nYou can preview and refine the generated dataset.\\n\\n![](/images/synthetic/synthetic_data_rag_example_result.png)\\n\\nYou can:\\n\\n* Use “More like this” to add more variations.\\n* Drop rows that aren’t relevant.\\n* Manually edit questions or responses.\\n\\n### 4. Save the Dataset\\n\\nOnce you are finished, store the dataset. You can download it as a CSV file or access it via the Python API using the dataset ID to use in your evaluation.\\n\\n<Info>\\n  **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\\n</Info>', 'title': 'RAG evaluation dataset', 'description': 'Synthetic data for RAG.', 'filename': 'docs-main/synthetic-data/rag_data.mdx'}\n",
      "Title: RAG evaluation dataset\n",
      "Chunk preview: Retrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the ...\n",
      "--------------------------------------------------\n",
      "Title: LLM Evaluation\n",
      "Chunk preview:  Inputs, context, and outputs (for RAG evaluation)\n",
      "</Info>\n",
      "\n",
      "<Info>\n",
      "  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tr...\n",
      "--------------------------------------------------\n",
      "Title: LLM Evaluation\n",
      "Chunk preview: ho painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\n",
      "    [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour....\n",
      "--------------------------------------------------\n",
      "Title: LLM Evaluation\n",
      "Chunk preview:  run the evals:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from evidently import Dataset\n",
      "from evidently import DataDefinition\n",
      "from evidently import Report\n",
      "from evidently.presets import TextEvals\n",
      "from evidently.te...\n",
      "--------------------------------------------------\n",
      "Title: LLM Evaluation\n",
      "Chunk preview: n LLM judge templates.\n",
      "\n",
      "<Accordion title=\"Custom LLM judge\" description=\"How to create a custom LLM evaluator\" icon=\"sparkles\">\n",
      "  Let's classify user questions as \"appropriate\" or \"inappropriate\" for ...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "# Create text search index for Evidently docs\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "# Fit the index with our chunked documents\n",
    "index.fit(evidently_chunks)\n",
    "\n",
    "# Test text search\n",
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "text_results = index.search(query, num_results=5)\n",
    "\n",
    "print(\"Text Search Results:\")\n",
    "print(f\"Number of results: {len(text_results)}\")\n",
    "if text_results:\n",
    "    print(f\"First result keys: {list(text_results[0].keys())}\")\n",
    "    print(f\"First result: {text_results[0]}\")\n",
    "\n",
    "for result in text_results:\n",
    "    print(f\"Title: {result.get('title', 'N/A')}\")\n",
    "    print(f\"Chunk preview: {result['chunk'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce6cc44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ Text Search Results:\n",
      "Number of results: 3\n",
      "First result keys: ['id', 'question', 'sort_order', 'content', 'filename']\n",
      "Question: Course - Can I follow the course after it finishes?\n",
      "Answer preview: Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\n",
      "\n",
      "You can also continue reviewing the homeworks and prepare for the next cohort. You can ...\n",
      "--------------------------------------------------\n",
      "Question: Course: Can I still join the course after the start date?\n",
      "Answer preview: Yes, even if you don't register, you're still eligible to submit the homework.\n",
      "\n",
      "Be aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everythi...\n",
      "--------------------------------------------------\n",
      "Question: Course: When does the course start?\n",
      "Answer preview: The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n",
      "\n",
      "- Register before the course starts using this [link](h...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Text search for DataTalksClub FAQ (data engineering)\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)\n",
    "\n",
    "# Test FAQ search\n",
    "faq_query = 'Can I join the course after it started?'\n",
    "faq_text_results = faq_index.search(faq_query, num_results=3)\n",
    "\n",
    "print(\"FAQ Text Search Results:\")\n",
    "print(f\"Number of results: {len(faq_text_results)}\")\n",
    "if faq_text_results:\n",
    "    print(f\"First result keys: {list(faq_text_results[0].keys())}\")\n",
    "\n",
    "for result in faq_text_results:\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Answer preview: {result['content'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61743f6a",
   "metadata": {},
   "source": [
    "## 2. Vector Search\n",
    "\n",
    "Vector search uses embeddings to capture semantic meaning. We'll use sentence-transformers for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b3abde43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in d:\\aihero\\course\\.venv\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (4.56.2)\n",
      "Requirement already satisfied: tqdm in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (0.35.1)\n",
      "Requirement already satisfied: Pillow in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\aihero\\course\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\aihero\\course\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\aihero\\course\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\aihero\\course\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\aihero\\course\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\aihero\\course\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cc59b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for FAQ data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b1fcbf214b4142bf9fc75a7afaa8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Search Results:\n",
      "Number of results: 3\n",
      "First result keys: ['id', 'question', 'sort_order', 'content', 'filename']\n",
      "Question: Course: Can I still join the course after the start date?\n",
      "Answer preview: Yes, even if you don't register, you're still eligible to submit the homework.\n",
      "\n",
      "Be aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everythi...\n",
      "--------------------------------------------------\n",
      "Question: Course - Can I follow the course after it finishes?\n",
      "Answer preview: Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\n",
      "\n",
      "You can also continue reviewing the homeworks and prepare for the next cohort. You can ...\n",
      "--------------------------------------------------\n",
      "Question: Course: When does the course start?\n",
      "Answer preview: The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n",
      "\n",
      "- Register before the course starts using this [link](h...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from minsearch import VectorSearch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "\n",
    "# Create embeddings for FAQ data\n",
    "print(\"Creating embeddings for FAQ data...\")\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq[:50]):  # Limit to 50 for demo (embeddings take time)\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "faq_embeddings = np.array(faq_embeddings)\n",
    "\n",
    "# Create vector search index\n",
    "faq_vindex = VectorSearch(keyword_fields=[])\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq[:50])\n",
    "\n",
    "# Test vector search\n",
    "vector_query = 'Can I join the course now?'\n",
    "q = embedding_model.encode(vector_query)\n",
    "vector_results = faq_vindex.search(q, num_results=3)\n",
    "\n",
    "print(\"Vector Search Results:\")\n",
    "print(f\"Number of results: {len(vector_results)}\")\n",
    "if vector_results:\n",
    "    print(f\"First result keys: {list(vector_results[0].keys())}\")\n",
    "\n",
    "for result in vector_results:\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Answer preview: {result['content'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9beb017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for Evidently docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83681ecffee424990af853df8a114cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently Vector Search Results:\n",
      "Number of results: 3\n",
      "First result keys: ['start', 'chunk', 'title', 'description', 'filename']\n",
      "Title: Product updates\n",
      "Chunk preview:  label=\"2025-04-10\" description=\"Evidently v7.0\">\n",
      "  ## **Evidently 0.7**\n",
      "\n",
      "This release introduces breaking changes. Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag...\n",
      "--------------------------------------------------\n",
      "Title: Product updates\n",
      "Chunk preview: eleases/tag/v0.7.6).\n",
      "</Update>\n",
      "\n",
      "<Update label=\"2025-05-09\" description=\"Evidently v0.7.5\">\n",
      "  ## **Evidently 0.7.5**\n",
      "\n",
      "  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/...\n",
      "--------------------------------------------------\n",
      "Title: Leftovers\n",
      "Chunk preview: ovelty**. Average the novelty by user across all users.\n",
      "\n",
      "**Range**: 0 to infinity. \n",
      "\n",
      "**Interpretation**: if the value is higher, the items shown to users are more unusual. If the value is lower, the r...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for Evidently chunks\n",
    "print(\"Creating embeddings for Evidently docs...\")\n",
    "evidently_embeddings = []\n",
    "\n",
    "for d in tqdm(evidently_chunks[:100]):  # Limit to 100 chunks for demo\n",
    "    v = embedding_model.encode(d['chunk'])\n",
    "    evidently_embeddings.append(v)\n",
    "\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "# Create vector search index for Evidently\n",
    "evidently_vindex = VectorSearch(keyword_fields=[])\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks[:100])\n",
    "\n",
    "# Test Evidently vector search\n",
    "evidently_vector_query = 'How does evidently work?'\n",
    "q_evidently = embedding_model.encode(evidently_vector_query)\n",
    "evidently_vector_results = evidently_vindex.search(q_evidently, num_results=3)\n",
    "\n",
    "print(\"Evidently Vector Search Results:\")\n",
    "print(f\"Number of results: {len(evidently_vector_results)}\")\n",
    "if evidently_vector_results:\n",
    "    print(f\"First result keys: {list(evidently_vector_results[0].keys())}\")\n",
    "\n",
    "for result in evidently_vector_results:\n",
    "    print(f\"Title: {result.get('title', 'N/A')}\")\n",
    "    print(f\"Chunk preview: {result['chunk'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a92485",
   "metadata": {},
   "source": [
    "## 3. Hybrid Search\n",
    "\n",
    "Hybrid search combines text and vector search for the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2da86f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Search Results:\n",
      "1. [text]\n",
      "   Question: Course - Can I follow the course after it finishes?\n",
      "   Preview: Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\n",
      "\n",
      "You can also continue reviewing the h...\n",
      "\n",
      "2. [text]\n",
      "   Question: Course: Can I still join the course after the start date?\n",
      "   Preview: Yes, even if you don't register, you're still eligible to submit the homework.\n",
      "\n",
      "Be aware, however, that there will be deadlines for turning in homewor...\n",
      "\n",
      "3. [text]\n",
      "   Question: Course: When does the course start?\n",
      "   Preview: The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n",
      "\n",
      "- Reg...\n",
      "\n",
      "4. [text]\n",
      "   Question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "   Preview: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is y...\n",
      "\n",
      "5. [text]\n",
      "   Question: Course: Can I get support if I take the course in the self-paced mode?\n",
      "   Preview: Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your q...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hybrid search function\n",
    "def hybrid_search(query, text_index, vector_index, embedding_model, num_results=5):\n",
    "    # Get text search results\n",
    "    text_results = text_index.search(query, num_results=num_results)\n",
    "\n",
    "    # Get vector search results\n",
    "    q = embedding_model.encode(query)\n",
    "    vector_results = vector_index.search(q, num_results=num_results)\n",
    "\n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    # Add text results first (they might be more precise for exact matches)\n",
    "    for result in text_results:\n",
    "        doc_id = result.get('filename', result.get('id', str(hash(str(result)))))\n",
    "        if doc_id not in seen_ids:\n",
    "            seen_ids.add(doc_id)\n",
    "            result['search_type'] = 'text'\n",
    "            combined_results.append(result)\n",
    "\n",
    "    # Add vector results\n",
    "    for result in vector_results:\n",
    "        doc_id = result.get('filename', result.get('id', str(hash(str(result)))))\n",
    "        if doc_id not in seen_ids:\n",
    "            seen_ids.add(doc_id)\n",
    "            result['search_type'] = 'vector'\n",
    "            combined_results.append(result)\n",
    "\n",
    "    # Sort by score (higher is better)\n",
    "    combined_results.sort(key=lambda x: x.get('score', 0), reverse=True)\n",
    "\n",
    "    return combined_results[:num_results]\n",
    "\n",
    "# Test hybrid search on FAQ\n",
    "hybrid_query = 'Can I enroll in the course after it started?'\n",
    "hybrid_results = hybrid_search(hybrid_query, faq_index, faq_vindex, embedding_model, num_results=5)\n",
    "\n",
    "print(\"Hybrid Search Results:\")\n",
    "for i, result in enumerate(hybrid_results, 1):\n",
    "    print(f\"{i}. [{result.get('search_type', 'unknown')}]\")\n",
    "    print(f\"   Question: {result.get('question', result.get('title', 'N/A'))}\")\n",
    "    print(f\"   Preview: {result.get('content', result.get('chunk', ''))[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150b363",
   "metadata": {},
   "source": [
    "## 4. Putting It All Together\n",
    "\n",
    "Our search system is complete! Here are the organized functions for easy use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "46447774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FAQ SEARCH RESULTS ===\n",
      "\n",
      "1. Text Search:\n",
      "   How does dbt handle dependencies between models?...\n",
      "   How do I use Git / GitHub for this course?...\n",
      "\n",
      "2. Vector Search:\n",
      "   Any books or additional resources you recommend?...\n",
      "   Homework and Leaderboard: What is the system for points in the course management...\n",
      "\n",
      "3. Hybrid Search:\n",
      "   [text] How does dbt handle dependencies between models?...\n",
      "   [text] How do I use Git / GitHub for this course?...\n",
      "   [text] How do I get my certificate?...\n",
      "\n",
      "=== EVIDENTLY SEARCH RESULTS ===\n",
      "\n",
      "Hybrid Search on Evidently docs:\n",
      "   [text] Use HuggingFace models\n",
      "   r each text in a column.\n",
      "\n",
      "For example, to evaluate \"curiousity\" expressed in a text:\n",
      "\n",
      "```python\n",
      "eval...\n",
      "   [vector] Introduction\n",
      "   d the “Tree of Life”.                        | Up to 2,500 years.              |\n",
      "    | What is the s...\n",
      "\n",
      "🎉 Search system ready! You can now query your documents with text, vector, or hybrid search.\n"
     ]
    }
   ],
   "source": [
    "# Organized search functions for FAQ\n",
    "def text_search_faq(query, num_results=5):\n",
    "    return faq_index.search(query, num_results=num_results)\n",
    "\n",
    "def vector_search_faq(query, num_results=5):\n",
    "    q = embedding_model.encode(query)\n",
    "    return faq_vindex.search(q, num_results=num_results)\n",
    "\n",
    "def hybrid_search_faq(query, num_results=5):\n",
    "    return hybrid_search(query, faq_index, faq_vindex, embedding_model, num_results)\n",
    "\n",
    "# Organized search functions for Evidently docs\n",
    "def text_search_evidently(query, num_results=5):\n",
    "    return index.search(query, num_results=num_results)\n",
    "\n",
    "def vector_search_evidently(query, num_results=5):\n",
    "    q = embedding_model.encode(query)\n",
    "    return evidently_vindex.search(q, num_results=num_results)\n",
    "\n",
    "def hybrid_search_evidently(query, num_results=5):\n",
    "    return hybrid_search(query, index, evidently_vindex, embedding_model, num_results)\n",
    "\n",
    "# Demo all search types\n",
    "demo_query = \"How do I evaluate machine learning models?\"\n",
    "\n",
    "print(\"=== FAQ SEARCH RESULTS ===\")\n",
    "print(\"\\n1. Text Search:\")\n",
    "text_results = text_search_faq(demo_query, 2)\n",
    "for r in text_results:\n",
    "    print(f\"   {r['question'][:80]}...\")\n",
    "\n",
    "print(\"\\n2. Vector Search:\")\n",
    "vector_results = vector_search_faq(demo_query, 2)\n",
    "for r in vector_results:\n",
    "    print(f\"   {r['question'][:80]}...\")\n",
    "\n",
    "print(\"\\n3. Hybrid Search:\")\n",
    "hybrid_results = hybrid_search_faq(demo_query, 3)\n",
    "for r in hybrid_results:\n",
    "    print(f\"   [{r.get('search_type', '?')}] {r['question'][:80]}...\")\n",
    "\n",
    "print(\"\\n=== EVIDENTLY SEARCH RESULTS ===\")\n",
    "print(\"\\nHybrid Search on Evidently docs:\")\n",
    "evidently_hybrid = hybrid_search_evidently(demo_query, 3)\n",
    "for r in evidently_hybrid:\n",
    "    title = r.get('title', 'N/A')\n",
    "    print(f\"   [{r.get('search_type', '?')}] {title}\")\n",
    "    print(f\"   {r['chunk'][:100]}...\")\n",
    "\n",
    "print(\"\\n🎉 Search system ready! You can now query your documents with text, vector, or hybrid search.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96153433",
   "metadata": {},
   "source": [
    "# Day 4: Build the Conversational AI Agent\n",
    "\n",
    "Now that we have our search system ready, let's build the actual conversational AI agent! This agent will:\n",
    "\n",
    "- Accept user questions\n",
    "- Use our hybrid search to find relevant context\n",
    "- Generate helpful, context-aware responses using Gemini AI\n",
    "- Provide accurate information from our indexed documents\n",
    "\n",
    "This is known as **Retrieval-Augmented Generation (RAG)** - combining retrieval (search) with generation (LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "df0872c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Agent Implementation\n",
    "\n",
    "def build_context_from_results(search_results, max_context_length=2000):\n",
    "    \"\"\"\n",
    "    Build context string from search results for the LLM.\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "\n",
    "    for result in search_results[:5]:  # Limit to top 5 results\n",
    "        if 'question' in result and 'content' in result:\n",
    "            # FAQ format\n",
    "            context = f\"Q: {result['question']}\\nA: {result['content'][:500]}...\"\n",
    "        elif 'chunk' in result:\n",
    "            # Document chunk format\n",
    "            title = result.get('title', 'Document')\n",
    "            context = f\"From {title}:\\n{result['chunk'][:500]}...\"\n",
    "        else:\n",
    "            # Fallback\n",
    "            context = str(result)[:500]\n",
    "\n",
    "        context_parts.append(context)\n",
    "\n",
    "    full_context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # Truncate if too long\n",
    "    if len(full_context) > max_context_length:\n",
    "        full_context = full_context[:max_context_length] + \"...\"\n",
    "\n",
    "    return full_context\n",
    "\n",
    "def generate_response_with_context(user_query, context, model_name='gemini-1.5-flash'):\n",
    "    \"\"\"\n",
    "    Generate a response using Gemini AI with retrieved context.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful AI assistant that answers questions based on the provided context.\n",
    "Use the information from the context to provide accurate, helpful answers.\n",
    "If the context doesn't contain enough information to fully answer the question,\n",
    "say so and provide what information you can.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question: {user_query}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {e}\"\n",
    "\n",
    "def rag_agent(user_query, search_type='hybrid', data_source='faq', num_results=5):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: search + generate response.\n",
    "\n",
    "    Args:\n",
    "        user_query: User's question\n",
    "        search_type: 'text', 'vector', or 'hybrid'\n",
    "        data_source: 'faq' or 'evidently'\n",
    "        num_results: Number of search results to retrieve\n",
    "    \"\"\"\n",
    "    # Select appropriate search function\n",
    "    if data_source == 'faq':\n",
    "        if search_type == 'text':\n",
    "            search_func = text_search_faq\n",
    "        elif search_type == 'vector':\n",
    "            search_func = vector_search_faq\n",
    "        else:  # hybrid\n",
    "            search_func = hybrid_search_faq\n",
    "    else:  # evidently\n",
    "        if search_type == 'text':\n",
    "            search_func = text_search_evidently\n",
    "        elif search_type == 'vector':\n",
    "            search_func = vector_search_evidently\n",
    "        else:  # hybrid\n",
    "            search_func = hybrid_search_evidently\n",
    "\n",
    "    # Perform search\n",
    "    search_results = search_func(user_query, num_results)\n",
    "\n",
    "    # Build context\n",
    "    context = build_context_from_results(search_results)\n",
    "\n",
    "    # Generate response\n",
    "    response = generate_response_with_context(user_query, context)\n",
    "\n",
    "    return {\n",
    "        'query': user_query,\n",
    "        'search_results': search_results,\n",
    "        'context': context,\n",
    "        'response': response,\n",
    "        'search_type': search_type,\n",
    "        'data_source': data_source\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0432b807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Testing the RAG Agent\n",
      "\n",
      "============================================================\n",
      "\n",
      "🗣️  Query 1: Can I join the course after it has started?\n",
      "----------------------------------------\n",
      "📚 Data Source: faq\n",
      "🔍 Search Type: hybrid\n",
      "📄 Context Retrieved: 3 results\n",
      "\n",
      "💬 Agent Response:\n",
      "Error generating response: 404 Publisher Model `projects/generativelanguage-ga/locations/us-central1/publishers/google/models/gemini-1.5-flash-002` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions\n",
      "\n",
      "============================================================\n",
      "\n",
      "🗣️  Query 2: How do I evaluate machine learning models?\n",
      "----------------------------------------\n",
      "📚 Data Source: faq\n",
      "🔍 Search Type: hybrid\n",
      "📄 Context Retrieved: 3 results\n",
      "\n",
      "💬 Agent Response:\n",
      "Error generating response: 404 Publisher Model `projects/generativelanguage-ga/locations/us-central1/publishers/google/models/gemini-1.5-flash-002` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions\n",
      "\n",
      "============================================================\n",
      "\n",
      "🗣️  Query 2: How do I evaluate machine learning models?\n",
      "----------------------------------------\n",
      "📚 Data Source: evidently\n",
      "🔍 Search Type: hybrid\n",
      "📄 Context Retrieved: 2 results\n",
      "\n",
      "💬 Agent Response:\n",
      "Error generating response: 404 Publisher Model `projects/generativelanguage-ga/locations/us-central1/publishers/google/models/gemini-1.5-flash-002` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions\n",
      "\n",
      "============================================================\n",
      "\n",
      "🗣️  Query 3: What is Evidently and how does it work?\n",
      "----------------------------------------\n",
      "📚 Data Source: evidently\n",
      "🔍 Search Type: hybrid\n",
      "📄 Context Retrieved: 2 results\n",
      "\n",
      "💬 Agent Response:\n",
      "Error generating response: 404 Publisher Model `projects/generativelanguage-ga/locations/us-central1/publishers/google/models/gemini-1.5-flash-002` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions\n",
      "\n",
      "============================================================\n",
      "\n",
      "🗣️  Query 3: What is Evidently and how does it work?\n",
      "----------------------------------------\n",
      "📚 Data Source: evidently\n",
      "🔍 Search Type: hybrid\n",
      "📄 Context Retrieved: 3 results\n",
      "\n",
      "💬 Agent Response:\n",
      "Error generating response: 404 Publisher Model `projects/generativelanguage-ga/locations/us-central1/publishers/google/models/gemini-1.5-flash-002` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions\n",
      "\n",
      "============================================================\n",
      "\n",
      "🗣️  Query 4: How do I use Git for this course?\n",
      "----------------------------------------\n",
      "📚 Data Source: evidently\n",
      "🔍 Search Type: hybrid\n",
      "📄 Context Retrieved: 3 results\n",
      "\n",
      "💬 Agent Response:\n",
      "Error generating response: 404 Publisher Model `projects/generativelanguage-ga/locations/us-central1/publishers/google/models/gemini-1.5-flash-002` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions\n",
      "\n",
      "============================================================\n",
      "\n",
      "🗣️  Query 4: How do I use Git for this course?\n",
      "----------------------------------------\n",
      "📚 Data Source: faq\n",
      "🔍 Search Type: hybrid\n",
      "📄 Context Retrieved: 3 results\n",
      "\n",
      "💬 Agent Response:\n",
      "Error generating response: 404 Publisher Model `projects/generativelanguage-ga/locations/us-central1/publishers/google/models/gemini-1.5-flash-002` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions\n",
      "\n",
      "============================================================\n",
      "\n",
      "🎉 RAG Agent is working! You can now build conversational interfaces on top of this.\n",
      "📚 Data Source: faq\n",
      "🔍 Search Type: hybrid\n",
      "📄 Context Retrieved: 3 results\n",
      "\n",
      "💬 Agent Response:\n",
      "Error generating response: 404 Publisher Model `projects/generativelanguage-ga/locations/us-central1/publishers/google/models/gemini-1.5-flash-002` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions\n",
      "\n",
      "============================================================\n",
      "\n",
      "🎉 RAG Agent is working! You can now build conversational interfaces on top of this.\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG Agent\n",
    "\n",
    "# Example queries to test\n",
    "test_queries = [\n",
    "    \"Can I join the course after it has started?\",\n",
    "    \"How do I evaluate machine learning models?\",\n",
    "    \"What is Evidently and how does it work?\",\n",
    "    \"How do I use Git for this course?\"\n",
    "]\n",
    "\n",
    "print(\"🤖 Testing the RAG Agent\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n🗣️  Query {i}: {query}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Determine data source based on query content\n",
    "    if any(word in query.lower() for word in ['course', 'certificate', 'homework', 'git', 'github']):\n",
    "        data_source = 'faq'\n",
    "    else:\n",
    "        data_source = 'evidently'\n",
    "\n",
    "    # Run the agent\n",
    "    result = rag_agent(query, search_type='hybrid', data_source=data_source, num_results=3)\n",
    "\n",
    "    print(f\"📚 Data Source: {result['data_source']}\")\n",
    "    print(f\"🔍 Search Type: {result['search_type']}\")\n",
    "    print(f\"📄 Context Retrieved: {len(result['search_results'])} results\")\n",
    "    print(f\"\\n💬 Agent Response:\\n{result['response']}\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "print(\"\\n🎉 RAG Agent is working! You can now build conversational interfaces on top of this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d2865461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Chat Interface\n",
    "\n",
    "def chat_with_agent():\n",
    "    \"\"\"\n",
    "    Simple interactive chat interface for the RAG agent.\n",
    "    \"\"\"\n",
    "    print(\"🤖 Welcome to your AI Agent!\")\n",
    "    print(\"Ask me questions about the course FAQ or Evidently documentation.\")\n",
    "    print(\"Type 'quit' to exit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"You: \").strip()\n",
    "\n",
    "            if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                print(\"🤖 Goodbye! Happy learning! 🚀\")\n",
    "                break\n",
    "\n",
    "            if not user_input:\n",
    "                continue\n",
    "\n",
    "            # Auto-detect data source\n",
    "            if any(word in user_input.lower() for word in ['course', 'certificate', 'homework', 'git', 'github', 'join', 'enroll', 'start']):\n",
    "                data_source = 'faq'\n",
    "                print(\"🔍 Searching FAQ database...\")\n",
    "            else:\n",
    "                data_source = 'evidently'\n",
    "                print(\"🔍 Searching Evidently documentation...\")\n",
    "\n",
    "            # Get response\n",
    "            result = rag_agent(user_input, search_type='hybrid', data_source=data_source)\n",
    "\n",
    "            print(f\"\\n🤖 Agent: {result['response']}\")\n",
    "            print(f\"📊 (Found {len(result['search_results'])} relevant documents)\\n\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n🤖 Goodbye! Happy learning! 🚀\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "            print(\"Please try again.\\n\")\n",
    "\n",
    "# Uncomment the line below to start interactive chat\n",
    "# chat_with_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7ad34",
   "metadata": {},
   "source": [
    "# Extending and Deploying Your AI Agent\n",
    "\n",
    "## 🚀 Next Steps\n",
    "\n",
    "Your RAG agent is now complete! Here are ways to extend it:\n",
    "\n",
    "### 1. **Add More Data Sources**\n",
    "```python\n",
    "# Add your own documentation\n",
    "your_docs = read_repo_data('your-org', 'your-repo')\n",
    "# Index and search through them\n",
    "```\n",
    "\n",
    "### 2. **Improve Search Quality**\n",
    "- Add relevance scoring\n",
    "- Implement query expansion\n",
    "- Add metadata filtering\n",
    "\n",
    "### 3. **Build Web Interface**\n",
    "- Use Streamlit, Gradio, or FastAPI\n",
    "- Add conversation history\n",
    "- Implement user authentication\n",
    "\n",
    "### 4. **Production Deployment**\n",
    "- Containerize with Docker\n",
    "- Deploy to cloud (AWS, GCP, Azure)\n",
    "- Add monitoring and logging\n",
    "\n",
    "### 5. **Advanced Features**\n",
    "- Multi-turn conversations\n",
    "- Source citations\n",
    "- Confidence scoring\n",
    "- Fallback responses\n",
    "\n",
    "## 🏗️ Architecture Overview\n",
    "\n",
    "```\n",
    "User Query → Search System → Context Retrieval → LLM Generation → Response\n",
    "     ↓           ↓              ↓                    ↓           ↓\n",
    "  \"Question\" → Hybrid Search → Relevant Docs → Gemini AI → Answer\n",
    "```\n",
    "\n",
    "## 💡 Key Benefits\n",
    "\n",
    "- **Accurate**: Answers based on your actual documentation\n",
    "- **Up-to-date**: No training required, just index new docs\n",
    "- **Cost-effective**: No expensive model fine-tuning\n",
    "- **Transparent**: Can show sources and reasoning\n",
    "- **Extensible**: Easy to add new data sources\n",
    "\n",
    "Your AI agent is ready to help users with accurate, context-aware responses! 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
